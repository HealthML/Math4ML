{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 - Probabilistic PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.datasets import fetch_openml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will present **Probabilistic PCA (PPCA)**.  \n",
    "\n",
    "### Key Aspects of Probabilistic PCA  \n",
    "- Unlike standard PCA, **Probabilistic PCA** is formulated as a probabilistic model, allowing us to **sample new data points** from the learned distribution.  \n",
    "- We will use the **closed-form solution** for PPCA to compute the principal components.  \n",
    "- An alternative approach to solving PPCA is using the **Expectation-Maximization (EM) algorithm**, which iteratively estimates the latent variables and model parameters.  \n",
    "\n",
    "PPCA is particularly useful when working with noisy or missing data, as it naturally integrates probabilistic modeling into dimensionality reduction.  \n",
    "\n",
    "## Math behind PPCA\n",
    "\n",
    "We introduce an explicit laten variale $z$ corresponding to the principal-component subspace. \n",
    "The prior distribution of $z$ is given by:\n",
    "p(z) = N(z|0, I).\n",
    "\n",
    "We can define the conditional distrubution of the observed variable $x$, conditioned on $z$ as:\n",
    "p(x|z) = N(x|Wz + \\mu, \\sigma^2I).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPCA():\n",
    "    '''\n",
    "    X - dataset\n",
    "    x - data point\n",
    "    z - laten variable\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,X, M):\n",
    "        self.D = X.shape[1] # dimension of oryginal data points   \n",
    "        self.M = M # dimension of reduced data point\n",
    "        self.X = X #dataset\n",
    "        self.calculate_parameters()\n",
    "    def calculate_parameters(self):\n",
    "        '''\n",
    "        Determine parameteres of the model (mean, variance and W matrix). \n",
    "        Have to be overriden in child classes\n",
    "        '''\n",
    "        raise NotImplementedError \n",
    "    def sample_x(self):\n",
    "        '''\n",
    "        Sample from p(x) distribution\n",
    "        '''\n",
    "        mean = self.mean\n",
    "        C = np.dot(self.W_ML, self.W_ML.T) + self.sigma * np.eye(self.D)\n",
    "        distribution = stats.multivariate_normal(mean, C)\n",
    "        return distribution.rvs()\n",
    "    def sample_z(self):\n",
    "        '''\n",
    "        Sample from p(z) distribution\n",
    "        '''\n",
    "        distribution = stats.multivariate_normal(np.zeros(shape = self.M), np.eye(self.M))\n",
    "        return distribution.rvs()\n",
    "    def sample_x_given_z(self, z):\n",
    "        '''\n",
    "        Sample from p(x|z) distribution'\n",
    "        '''\n",
    "        distribution = stats.multivariate_normal(np.dot(self.W_ML, z) + self.mean, self.sigma * np.eye(self.D))\n",
    "        return distribution.rvs()\n",
    "    def sample_z_given_x(self, x):\n",
    "        '''\n",
    "        Sample from p(z|x) distribution\n",
    "        '''\n",
    "        M_matrix = np.dot(self.W_ML.T, self.W_ML) + self.sigma * np.eye(self.M)\n",
    "        M_matrix_inv = np.linalg.inv(M_matrix)\n",
    "        mean = np.linalg.multi_dot([M_matrix_inv, self.W_ML.T, (x - self.mean)])\n",
    "        variance = self.sigma * M_matrix_inv                                    \n",
    "        distribution = stats.multivariate_normal(mean, variance)\n",
    "        return distribution.rvs()\n",
    "                                       \n",
    "\n",
    "# ## Closed-form solution (CF)\n",
    "\n",
    "class PPCA_CF(PPCA):\n",
    "    '''\n",
    "    X - dataset\n",
    "    x - data point\n",
    "    z - laten variable\n",
    "    \n",
    "    '''        \n",
    "    def calculate_parameters(self):\n",
    "        '''\n",
    "        Determine parameteres of the model by optimizing likelihood function. \n",
    "        It involves caltulating mean, variance and W matrix.\n",
    "        '''\n",
    "        self.mean = self.X.mean(axis = 0)\n",
    "        \n",
    "        covariance = np.cov(self.X, rowvar = False)\n",
    "        eig_val, eig_vec = np.linalg.eig(covariance)\n",
    "        idx = np.argsort(eig_val)[::-1]\n",
    "        eig_val = np.real(eig_val[idx])\n",
    "        eig_vec = np.real(eig_vec[:, idx])\n",
    "        \n",
    "        self.sigma = 1/(self.D - self.M) * np.sum(eig_val[self.M+1:])\n",
    "        \n",
    "        U_M = eig_vec[:, :self.M]\n",
    "        L_M = np.diag(eig_val[:self.M])\n",
    "        self.W_ML = np.dot(U_M, np.sqrt(L_M - self.sigma*np.eye(self.M)))\n",
    "                                        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# Fetch MNIST data\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "# Extract data and labels\n",
    "x_train, y_train = mnist['data'], mnist['target']\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_train = x_train.reshape(70000, -1)\n",
    "x_train = x_train[((y_train == '8') + (y_train == '1')),:]\n",
    "y_train = y_train[((y_train == '8') + (y_train == '1'))]\n",
    "\n",
    "model = PPCA_CF(x_train, 2)\n",
    "\n",
    "# sample from p(x)\n",
    "plt.figure(figsize =(10,10))\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    plt.imshow(model.sample_x().reshape(28,28))\n",
    "plt.suptitle('Sampling from p(x)', fontsize=20)\n",
    "\n",
    "# Show the original image\n",
    "plt.figure()\n",
    "idx = np.random.randint(0, x_train[0].shape[0])\n",
    "plt.imshow(x_train[idx,:].reshape(28,28))\n",
    "plt.suptitle('Original image', fontsize=20)\n",
    "\n",
    "# Show the reconstructions\n",
    "\n",
    "z = model.sample_z_given_x(x_train[idx,:]) # get latent variable p(z|x)\n",
    "\n",
    "plt.figure(figsize =(10,10))\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    image = model.sample_x_given_z(z)\n",
    "    plt.imshow(image.reshape(28,28))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Image reconstruction p(x|z)', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
