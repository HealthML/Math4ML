{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 - Probabilistic PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.datasets import fetch_openml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will present **Probabilistic PCA (PPCA)**.  \n",
    "\n",
    "### Key Aspects of Probabilistic PCA  \n",
    "- Unlike standard PCA, **Probabilistic PCA** is formulated as a probabilistic model, allowing us to **sample new data points** from the learned distribution.  \n",
    "- We will use the **closed-form solution** for PPCA to compute the principal components.  \n",
    "- An alternative approach to solving PPCA is using the **Expectation-Maximization (EM) algorithm**, which iteratively estimates the latent variables and model parameters.  \n",
    "\n",
    "PPCA is particularly useful when working with noisy or missing data, as it naturally integrates probabilistic modeling into dimensionality reduction.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Math behind PPCA\n",
    "\n",
    "We introduce an explicit laten variale $z$ corresponding to the principal-component subspace. \n",
    "The prior distribution of $z$ is given by:\n",
    "\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z} \\mid \\mathbf{0}, \\mathbf{I})\n",
    "$$\n",
    "\n",
    "We can define the conditional distrubution of the observed variable $\\mathbf{x}$, conditioned on the laten variable $\\mathbf{z}$ as:\n",
    "$$\n",
    "p(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mathbf{x} \\mid \\mathbf{W}\\mathbf{z} + \\mathbf{\\mu}, \\sigma^2\\mathbf{I}).\n",
    "$$\n",
    "\n",
    "Therefore we can express $\\mathbf{x}$ as a linear transformation of the latent variable $z$ plus a Gaussian noise term $\\epsilon$:\n",
    "\n",
    "$$ \n",
    "\\mathbf{x} = \\mathbf{W}\\mathbf{z} + \\mathbf{\\mu} + \\mathbf{\\epsilon}\n",
    "$$\n",
    "\n",
    "The marginal distribution $p(\\mathbf{x})$ of the observed variable could be obtained from the sum and product rules of probability:\n",
    "$$\n",
    "p(\\mathbf{x}) = \\int p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})d\\mathbf{z} \n",
    "\n",
    "$$\n",
    "\n",
    "This distribution is Gaussian given by:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x} \\mid \\mathbf{\\mu}, \\mathbf{C})\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\mathbf{C} = \\mathbf{W}\\mathbf{W}^T + \\sigma^2\\mathbf{I}\n",
    "$$\n",
    "\n",
    "The last equation is the posterior distribution of the latent variable $z$ given the observed variable $x$:\n",
    "$$\n",
    "p(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N}(\\mathbf{z} \\mid \\mathbf{M}^{-1}\\mathbf{W}^T(\\mathbf{x}-\\mathbf{\\mu}), \\sigma^{-2}\\mathbf{M})\n",
    "$$\n",
    "\n",
    "where:\n",
    "$$\\mathbf{M} = \\mathbf{W}^T\\mathbf{W} + \\sigma^2\\mathbf{I}$$\n",
    "\n",
    "To sum up, the PPCA model could be defined by following distributions:\n",
    "- laten variable distribution $p(\\mathbf{z}) $\n",
    "- distribution of the observed variable conditioned on the latent variable $p(\\mathbf{x}|\\mathbf{z})$\n",
    "- predictive distribution $p(\\mathbf{x})$\n",
    "- posterior distribution $p(\\mathbf{z}|\\mathbf{x})$\n",
    "\n",
    "Note that all the distribution are multivariate Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PPCA():\n",
    "    '''\n",
    "    X - dataset\n",
    "    x - data point\n",
    "    z - laten variable\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,X, M):\n",
    "        self.D = X.shape[1] # dimension of oryginal data points   \n",
    "        self.M = M # dimension of reduced data point\n",
    "        self.X = X #dataset\n",
    "        self.calculate_parameters()\n",
    "    def calculate_parameters(self):\n",
    "        '''\n",
    "        Determine parameteres of the model (mean, variance and W matrix). \n",
    "        Have to be overriden in child classes\n",
    "        '''\n",
    "        raise NotImplementedError \n",
    "    def sample_x(self):\n",
    "        '''\n",
    "        Sample from p(x) distribution\n",
    "        '''\n",
    "        mean = self.mean\n",
    "        C = np.dot(self.W_ML, self.W_ML.T) + self.sigma * np.eye(self.D)\n",
    "        distribution = stats.multivariate_normal(mean, C)\n",
    "        return distribution.rvs()\n",
    "    def sample_z(self):\n",
    "        '''\n",
    "        Sample from p(z) distribution\n",
    "        '''\n",
    "        distribution = stats.multivariate_normal(np.zeros(shape = self.M), np.eye(self.M))\n",
    "        return distribution.rvs()\n",
    "    def sample_x_given_z(self, z):\n",
    "        '''\n",
    "        Sample from p(x|z) distribution'\n",
    "        '''\n",
    "        distribution = stats.multivariate_normal(np.dot(self.W_ML, z) + self.mean, self.sigma * np.eye(self.D))\n",
    "        return distribution.rvs()\n",
    "    def sample_z_given_x(self, x):\n",
    "        '''\n",
    "        Sample from p(z|x) distribution\n",
    "        '''\n",
    "        M_matrix = np.dot(self.W_ML.T, self.W_ML) + self.sigma * np.eye(self.M)\n",
    "        M_matrix_inv = np.linalg.inv(M_matrix)\n",
    "        mean = np.linalg.multi_dot([M_matrix_inv, self.W_ML.T, (x - self.mean)])\n",
    "        variance = self.sigma * M_matrix_inv                                    \n",
    "        distribution = stats.multivariate_normal(mean, variance)\n",
    "        return distribution.rvs()\n",
    "                                       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed form solution\n",
    "The closed-form solution for PPCA is derived from the maximum likelihood estimation of the model parameters. The likelihood is represented by:\n",
    "$$\n",
    "p(\\mathbf{X} \\mid \\mathbf{\\mu}, \\mathbf{W}, \\sigma^2) \n",
    "$$\n",
    "where $\\mathbf{X}$ is the observed data matrix. \n",
    "We need to find the values of $\\mathbf{\\mu}$, $\\mathbf{W}$, and $\\sigma^2$ that maximize the likelihood function.\n",
    "\n",
    "The solution for $\\mathbf{\\mu} = \\mathbf{\\bar{x}}$ where $\\mathbf{\\bar{x}}$ is the mean of the data.\n",
    "\n",
    "The solution for $\\mathbf{W}$ is given by:\n",
    "$$\n",
    "\\mathbf{W_{ML}} = \\mathbf{U}_M (\\mathbf{L}_M - \\sigma^2\\mathbf{I})^{1/2}\n",
    "$$\n",
    "where $\\mathbf{U}_M$ is the matrix of the eigenvectors of the data covariance matrix, and $\\mathbf{L}_M$ is the diagonal matrix of the corresponding eigenvalues. We assume the arangement of the eigenvectors in order of decreasing values of the corresponding eigenvalues.\n",
    "\n",
    "The solution for $\\sigma^2$ is given by:\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{D-M} \\sum_{i=M+1}^{D} \\lambda_i\n",
    "\n",
    "where $D$ is the number of dimensions of the data, $M$ is the number of principal components, and $\\lambda_i$ are the eigenvalues of the data covariance matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## Closed-form solution (CF)\n",
    "\n",
    "class PPCA_CF(PPCA):\n",
    "    '''\n",
    "    X - dataset\n",
    "    x - data point\n",
    "    z - laten variable\n",
    "    \n",
    "    '''        \n",
    "    def calculate_parameters(self):\n",
    "        '''\n",
    "        Determine parameteres of the model by optimizing likelihood function. \n",
    "        It involves caltulating mean, variance and W matrix.\n",
    "        '''\n",
    "        self.mean = self.X.mean(axis = 0)\n",
    "        \n",
    "        covariance = np.cov(self.X, rowvar = False)\n",
    "        eig_val, eig_vec = np.linalg.eig(covariance)\n",
    "        idx = np.argsort(eig_val)[::-1]\n",
    "        eig_val = np.real(eig_val[idx])\n",
    "        eig_vec = np.real(eig_vec[:, idx])\n",
    "        \n",
    "        self.sigma = 1/(self.D - self.M) * np.sum(eig_val[self.M+1:])\n",
    "        \n",
    "        U_M = eig_vec[:, :self.M]\n",
    "        L_M = np.diag(eig_val[:self.M])\n",
    "        self.W_ML = np.dot(U_M, np.sqrt(L_M - self.sigma*np.eye(self.M)))\n",
    "                                        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPCA on MNIST dataset\n",
    "\n",
    "We will implement PPCA on the MNIST dataset. First we train the model. Then we will sample from $p(\\mathbf{x})$ and visualize the generated samples.\n",
    "\n",
    "Then we will use one image to sample from the posterior distribution $p(\\mathbf{z}|\\mathbf{x})$, then using the sampled latent variable $\\mathbf{z}$ we generate and show images using conditional distribution $p(\\mathbf{x}|\\mathbf{z})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fetch MNIST data\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "# Extract data and labels\n",
    "x_train, y_train = mnist['data'], mnist['target']\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_train = x_train.reshape(70000, -1)\n",
    "x_train = x_train[((y_train == '8') + (y_train == '1')),:]\n",
    "y_train = y_train[((y_train == '8') + (y_train == '1'))]\n",
    "\n",
    "model = PPCA_CF(x_train, 2)\n",
    "\n",
    "# sample from p(x)\n",
    "plt.figure(figsize =(10,10))\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    plt.imshow(model.sample_x().reshape(28,28))\n",
    "plt.suptitle('Sampling from p(x)', fontsize=20)\n",
    "\n",
    "# Show the original image\n",
    "plt.figure()\n",
    "idx = np.random.randint(0, x_train[0].shape[0])\n",
    "plt.imshow(x_train[idx,:].reshape(28,28))\n",
    "plt.suptitle('Original image', fontsize=20)\n",
    "\n",
    "# Show the reconstructions\n",
    "\n",
    "z = model.sample_z_given_x(x_train[idx,:]) # get latent variable p(z|x)\n",
    "\n",
    "plt.figure(figsize =(10,10))\n",
    "for i in range(1,10):\n",
    "    plt.subplot(3,3,i)\n",
    "    image = model.sample_x_given_z(z)\n",
    "    plt.imshow(image.reshape(28,28))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Image reconstruction p(x|z)', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
