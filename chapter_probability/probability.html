
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Probability &#8212; Mathematics for Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_probability/probability';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Machine Learning for Human Phenotype Classification" href="../logistic_regression_uci_breast_cancer/phenotype_classification_LogisticRegression.html" />
    <link rel="prev" title="Calculus and Optimization" href="../chapter_calculus/calculus.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/hpi-logo-colored.svg" class="logo__image only-light" alt="Mathematics for Machine Learning - Home"/>
    <img src="../_static/hpi-logo-colored.svg" class="logo__image only-dark pst-js-only" alt="Mathematics for Machine Learning - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Mathematics for Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/intro.html">Notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear_algebra/linear_algebra.html">Linear Algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_calculus/calculus.html">Calculus and Optimization</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../logistic_regression_uci_breast_cancer/phenotype_classification_LogisticRegression.html">Machine Learning for Human Phenotype Classification</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fchapter_probability/probability.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_probability/probability.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Probability</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basics">Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability">Conditional probability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule">Chain rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-rule">Bayes’ rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variables">Random variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cumulative-distribution-function">The cumulative distribution function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-random-variables">Discrete random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-random-variables">Continuous random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-kinds-of-random-variables">Other kinds of random variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distributions">Joint distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-of-random-variables">Independence of random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-distributions">Marginal distributions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#great-expectations">Great Expectations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-expected-value">Properties of expected value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-variance">Properties of variance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-deviation">Standard deviation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation">Correlation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-vectors">Random vectors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-parameters">Estimation of Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum likelihood estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-estimation">Maximum a posteriori estimation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-distribution">The Gaussian distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-geometry-of-multivariate-gaussians">The geometry of multivariate Gaussians</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="probability">
<h1>Probability<a class="headerlink" href="#probability" title="Link to this heading">#</a></h1>
<p>Probability theory provides powerful tools for modeling and dealing with
uncertainty.</p>
<section id="basics">
<h2>Basics<a class="headerlink" href="#basics" title="Link to this heading">#</a></h2>
<p>Suppose we have some sort of randomized experiment (e.g. a coin toss,
die roll) that has a fixed set of possible outcomes. This set is called
the <strong>sample space</strong> and denoted <span class="math notranslate nohighlight">\(\Omega\)</span>.</p>
<p>We would like to define probabilities for some <strong>events</strong>, which are
subsets of <span class="math notranslate nohighlight">\(\Omega\)</span>. The set of events is denoted <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>.<a class="footnote-reference brackets" href="#id9" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> The
<strong>complement</strong> of the event <span class="math notranslate nohighlight">\(A\)</span> is another event,
<span class="math notranslate nohighlight">\(A^\text{c} = \Omega \setminus A\)</span>.</p>
<p>Then we can define a <strong>probability measure</strong>
<span class="math notranslate nohighlight">\(\mathbb{P} : \mathcal{F} \to [0,1]\)</span> which must satisfy</p>
<p>(i) <span class="math notranslate nohighlight">\(\mathbb{P}(\Omega) = 1\)</span></p>
<p>(ii) <strong>Countable additivity</strong>: for any countable collection of disjoint
sets <span class="math notranslate nohighlight">\(\{A_i\} \subseteq \mathcal{F}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\bigg(\bigcup_i A_i\bigg) = \sum_i \mathbb{P}(A_i)\]</div>
<p>The triple <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, \mathbb{P})\)</span> is called a <strong>probability
space</strong>.<a class="footnote-reference brackets" href="#id10" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a></p>
<p>If <span class="math notranslate nohighlight">\(\mathbb{P}(A) = 1\)</span>, we say that <span class="math notranslate nohighlight">\(A\)</span> occurs <strong>almost surely</strong> (often
abbreviated a.s.).<a class="footnote-reference brackets" href="#id11" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>, and conversely <span class="math notranslate nohighlight">\(A\)</span> occurs <strong>almost never</strong> if
<span class="math notranslate nohighlight">\(\mathbb{P}(A) = 0\)</span>.</p>
<p>From these axioms, a number of useful rules can be derived.</p>
<p><em>Proposition.</em>
Let <span class="math notranslate nohighlight">\(A\)</span> be an event. Then</p>
<p>(i) <span class="math notranslate nohighlight">\(\mathbb{P}(A^\text{c}) = 1 - \mathbb{P}(A)\)</span>.</p>
<p>(ii) If <span class="math notranslate nohighlight">\(B\)</span> is an event and <span class="math notranslate nohighlight">\(B \subseteq A\)</span>, then
<span class="math notranslate nohighlight">\(\mathbb{P}(B) \leq \mathbb{P}(A)\)</span>.</p>
<p>(iii) <span class="math notranslate nohighlight">\(0 = \mathbb{P}(\varnothing) \leq \mathbb{P}(A) \leq \mathbb{P}(\Omega) = 1\)</span></p>
<p><em>Proof.</em> (i) Using the countable additivity of <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(A) + \mathbb{P}(A^\text{c}) = \mathbb{P}(A \mathbin{\dot{\cup}} A^\text{c}) = \mathbb{P}(\Omega) = 1\]</div>
<p>To show (ii), suppose <span class="math notranslate nohighlight">\(B \in \mathcal{F}\)</span> and <span class="math notranslate nohighlight">\(B \subseteq A\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(A) = \mathbb{P}(B \mathbin{\dot{\cup}} (A \setminus B)) = \mathbb{P}(B) + \mathbb{P}(A \setminus B) \geq \mathbb{P}(B)\]</div>
<p>as claimed.</p>
<p>For (iii): the middle inequality follows from (ii) since
<span class="math notranslate nohighlight">\(\varnothing \subseteq A \subseteq \Omega\)</span>. We also have</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(\varnothing) = \mathbb{P}(\varnothing \mathbin{\dot{\cup}} \varnothing) = \mathbb{P}(\varnothing) + \mathbb{P}(\varnothing)\]</div>
<p>by countable additivity, which shows <span class="math notranslate nohighlight">\(\mathbb{P}(\varnothing) = 0\)</span>. ◻</p>
<p><em>Proposition.</em>
If <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are events, then
<span class="math notranslate nohighlight">\(\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)\)</span>.</p>
<p><em>Proof.</em> The key is to break the events up into their various
overlapping and non-overlapping parts.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}(A \cup B) &amp;= \mathbb{P}((A \cap B) \mathbin{\dot{\cup}} (A \setminus B) \mathbin{\dot{\cup}} (B \setminus A)) \\
&amp;= \mathbb{P}(A \cap B) + \mathbb{P}(A \setminus B) + \mathbb{P}(B \setminus A) \\
&amp;= \mathbb{P}(A \cap B) + \mathbb{P}(A) - \mathbb{P}(A \cap B) + \mathbb{P}(B) - \mathbb{P}(A \cap B) \\
&amp;= \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)
\end{aligned}\end{split}\]</div>
<p>◻</p>
<p><em>Proposition.</em>
If <span class="math notranslate nohighlight">\(\{A_i\} \subseteq \mathcal{F}\)</span> is a countable set of events,
disjoint or not, then</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\bigg(\bigcup_i A_i\bigg) \leq \sum_i \mathbb{P}(A_i)\]</div>
<p>This inequality is sometimes referred to as <strong>Boole’s inequality</strong> or
the <strong>union bound</strong>.</p>
<p><em>Proof.</em> Define <span class="math notranslate nohighlight">\(B_1 = A_1\)</span> and
<span class="math notranslate nohighlight">\(B_i = A_i \setminus (\bigcup_{j &lt; i} A_j)\)</span> for <span class="math notranslate nohighlight">\(i &gt; 1\)</span>, noting that
<span class="math notranslate nohighlight">\(\bigcup_{j \leq i} B_j = \bigcup_{j \leq i} A_j\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and the
<span class="math notranslate nohighlight">\(B_i\)</span> are disjoint. Then</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}\bigg(\bigcup_i A_i\bigg) = \mathbb{P}\bigg(\bigcup_i B_i\bigg) = \sum_i \mathbb{P}(B_i) \leq \sum_i \mathbb{P}(A_i)\]</div>
<p>where the last inequality follows by monotonicity since
<span class="math notranslate nohighlight">\(B_i \subseteq A_i\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>. ◻</p>
<section id="conditional-probability">
<h3>Conditional probability<a class="headerlink" href="#conditional-probability" title="Link to this heading">#</a></h3>
<p>The <strong>conditional probability</strong> of event <span class="math notranslate nohighlight">\(A\)</span> given that event <span class="math notranslate nohighlight">\(B\)</span> has
occurred is written <span class="math notranslate nohighlight">\(\mathbb{P}(A | B)\)</span> and defined as</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}\]</div>
<p>assuming <span class="math notranslate nohighlight">\(\mathbb{P}(B) &gt; 0\)</span>.<a class="footnote-reference brackets" href="#id12" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a></p>
</section>
<section id="chain-rule">
<h3>Chain rule<a class="headerlink" href="#chain-rule" title="Link to this heading">#</a></h3>
<p>Another very useful tool, the <strong>chain rule</strong>, follows immediately from
this definition:</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(A \cap B) = \mathbb{P}(A | B)\mathbb{P}(B) = \mathbb{P}(B | A)\mathbb{P}(A)\]</div>
</section>
<section id="bayes-rule">
<h3>Bayes’ rule<a class="headerlink" href="#bayes-rule" title="Link to this heading">#</a></h3>
<p>Taking the equality from above one step further, we arrive at the simple
but crucial <strong>Bayes’ rule</strong>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(A | B) = \frac{\mathbb{P}(B | A)\mathbb{P}(A)}{\mathbb{P}(B)}\]</div>
<p>It is sometimes beneficial to omit the normalizing constant and write</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(A | B) \propto \mathbb{P}(A)\mathbb{P}(B | A)\]</div>
<p>Under this
formulation, <span class="math notranslate nohighlight">\(\mathbb{P}(A)\)</span> is often referred to as the <strong>prior</strong>,
<span class="math notranslate nohighlight">\(\mathbb{P}(A | B)\)</span> as the <strong>posterior</strong>, and <span class="math notranslate nohighlight">\(\mathbb{P}(B | A)\)</span> as the
<strong>likelihood</strong>.</p>
<p>In the context of machine learning, we can use Bayes’ rule to update our
“beliefs” (e.g. values of our model parameters) given some data that
we’ve observed.</p>
</section>
</section>
<section id="random-variables">
<h2>Random variables<a class="headerlink" href="#random-variables" title="Link to this heading">#</a></h2>
<p>A <strong>random variable</strong> is some uncertain quantity with an associated
probability distribution over the values it can assume.</p>
<p>Formally, a random variable on a probability space
<span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, \mathbb{P})\)</span> is a function<a class="footnote-reference brackets" href="#id13" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a>
<span class="math notranslate nohighlight">\(X: \Omega \to \mathbb{R}\)</span>.<a class="footnote-reference brackets" href="#id14" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a></p>
<p>We denote the range of <span class="math notranslate nohighlight">\(X\)</span> by
<span class="math notranslate nohighlight">\(X(\Omega) = \{X(\omega) : \omega \in \Omega\}\)</span>. To give a concrete
example (taken from [&#64;pitman]), suppose <span class="math notranslate nohighlight">\(X\)</span> is the number of heads in
two tosses of a fair coin. The sample space is</p>
<div class="math notranslate nohighlight">
\[\Omega = \{hh, tt, ht, th\}\]</div>
<p>and <span class="math notranslate nohighlight">\(X\)</span> is determined completely by the
outcome <span class="math notranslate nohighlight">\(\omega\)</span>, i.e. <span class="math notranslate nohighlight">\(X = X(\omega)\)</span>. For example, the event <span class="math notranslate nohighlight">\(X = 1\)</span>
is the set of outcomes <span class="math notranslate nohighlight">\(\{ht, th\}\)</span>.</p>
<p>It is common to talk about the values of a random variable without
directly referencing its sample space. The two are related by the
following definition: the event that the value of <span class="math notranslate nohighlight">\(X\)</span> lies in some set
<span class="math notranslate nohighlight">\(S \subseteq \mathbb{R}\)</span> is</p>
<div class="math notranslate nohighlight">
\[X \in S = \{\omega \in \Omega : X(\omega) \in S\}\]</div>
<p>Note that special
cases of this definition include <span class="math notranslate nohighlight">\(X\)</span> being equal to, less than, or
greater than some specified value. For example</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(X = x) = \mathbb{P}(\{\omega \in \Omega : X(\omega) = x\})\]</div>
<p>A word on notation: we write <span class="math notranslate nohighlight">\(p(X)\)</span> to denote the entire probability
distribution of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(p(x)\)</span> for the evaluation of the function <span class="math notranslate nohighlight">\(p\)</span> at
a particular value <span class="math notranslate nohighlight">\(x \in X(\Omega)\)</span>. Hopefully this (reasonably
standard) abuse of notation is not too distracting. If <span class="math notranslate nohighlight">\(p\)</span> is
parameterized by some parameters <span class="math notranslate nohighlight">\(\theta\)</span>, we write
<span class="math notranslate nohighlight">\(p(X; \mathbf{\theta})\)</span> or <span class="math notranslate nohighlight">\(p(x; \mathbf{\theta})\)</span>, unless we are in a
Bayesian setting where the parameters are considered a random variable,
in which case we condition on the parameters.</p>
<section id="the-cumulative-distribution-function">
<h3>The cumulative distribution function<a class="headerlink" href="#the-cumulative-distribution-function" title="Link to this heading">#</a></h3>
<p>The <strong>cumulative distribution function</strong> (c.d.f.) gives the probability
that a random variable is at most a certain value:</p>
<div class="math notranslate nohighlight">
\[F(x) = \mathbb{P}(X \leq x)\]</div>
<p>The c.d.f. can be used to give the
probability that a variable lies within a certain range:</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(a &lt; X \leq b) = F(b) - F(a)\]</div>
</section>
<section id="discrete-random-variables">
<h3>Discrete random variables<a class="headerlink" href="#discrete-random-variables" title="Link to this heading">#</a></h3>
<p>A <strong>discrete random variable</strong> is a random variable that has a countable
range and assumes each value in this range with positive probability.
Discrete random variables are completely specified by their
<strong>probability mass function</strong> (p.m.f.) <span class="math notranslate nohighlight">\(p : X(\Omega) \to [0,1]\)</span> which
satisfies</p>
<div class="math notranslate nohighlight">
\[\sum_{x \in X(\Omega)} p(x) = 1\]</div>
<p>For a discrete <span class="math notranslate nohighlight">\(X\)</span>, the
probability of a particular value is given exactly by its p.m.f.:</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(X = x) = p(x)\]</div>
</section>
<section id="continuous-random-variables">
<h3>Continuous random variables<a class="headerlink" href="#continuous-random-variables" title="Link to this heading">#</a></h3>
<p>A <strong>continuous random variable</strong> is a random variable that has an
uncountable range and assumes each value in this range with probability
zero. Most of the continuous random variables that one would encounter
in practice are <strong>absolutely continuous random variables</strong><a class="footnote-reference brackets" href="#id15" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a>, which
means that there exists a function <span class="math notranslate nohighlight">\(p : \mathbb{R} \to [0,\infty)\)</span> that
satisfies</p>
<div class="math notranslate nohighlight">
\[F(x) \equiv \int_{-\infty}^x p(z)\operatorname{d}{z}\]</div>
<p>The function <span class="math notranslate nohighlight">\(p\)</span>
is called a <strong>probability density function</strong> (abbreviated p.d.f.) and
must satisfy</p>
<div class="math notranslate nohighlight">
\[\int_{-\infty}^\infty p(x)\operatorname{d}{x} = 1\]</div>
<p>The values of this
function are not themselves probabilities, since they could exceed 1.
However, they do have a couple of reasonable interpretations. One is as
relative probabilities; even though the probability of each particular
value being picked is technically zero, some points are still in a sense
more likely than others.</p>
<p>One can also think of the density as determining the probability that
the variable will lie in a small range about a given value. This is
because, for small <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(x-\epsilon \leq X \leq x+\epsilon) = \int_{x-\epsilon}^{x+\epsilon} p(z)\operatorname{d}{z} \approx 2\epsilon p(x)\]</div>
<p>using a midpoint approximation to the integral.</p>
<p>Here are some useful identities that follow from the definitions above:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}(a \leq X \leq b) &amp;= \int_a^b p(x)\operatorname{d}{x} \\
p(x) &amp;= F'(x)
\end{aligned}\end{split}\]</div>
</section>
<section id="other-kinds-of-random-variables">
<h3>Other kinds of random variables<a class="headerlink" href="#other-kinds-of-random-variables" title="Link to this heading">#</a></h3>
<p>There are random variables that are neither discrete nor continuous. For
example, consider a random variable determined as follows: flip a fair
coin, then the value is zero if it comes up heads, otherwise draw a
number uniformly at random from <span class="math notranslate nohighlight">\([1,2]\)</span>. Such a random variable can take
on uncountably many values, but only finitely many of these with
positive probability. We will not discuss such random variables because
they are rather pathological and require measure theory to analyze.</p>
</section>
</section>
<section id="joint-distributions">
<h2>Joint distributions<a class="headerlink" href="#joint-distributions" title="Link to this heading">#</a></h2>
<p>Often we have several random variables and we would like to get a
distribution over some combination of them. A <strong>joint distribution</strong> is
exactly this. For some random variables <span class="math notranslate nohighlight">\(X_1, \dots, X_n\)</span>, the joint
distribution is written <span class="math notranslate nohighlight">\(p(X_1, \dots, X_n)\)</span> and gives probabilities
over entire assignments to all the <span class="math notranslate nohighlight">\(X_i\)</span> simultaneously.</p>
<section id="independence-of-random-variables">
<h3>Independence of random variables<a class="headerlink" href="#independence-of-random-variables" title="Link to this heading">#</a></h3>
<p>We say that two variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are <strong>independent</strong> if their joint
distribution factors into their respective distributions, i.e.</p>
<div class="math notranslate nohighlight">
\[p(X, Y) = p(X)p(Y)\]</div>
<p>We can also define independence for more than two
random variables, although it is more complicated. Let
<span class="math notranslate nohighlight">\(\{X_i\}_{i \in I}\)</span> be a collection of random variables indexed by <span class="math notranslate nohighlight">\(I\)</span>,
which may be infinite. Then <span class="math notranslate nohighlight">\(\{X_i\}\)</span> are independent if for every
finite subset of indices <span class="math notranslate nohighlight">\(i_1, \dots, i_k \in I\)</span> we have</p>
<div class="math notranslate nohighlight">
\[p(X_{i_1}, \dots, X_{i_k}) = \prod_{j=1}^k p(X_{i_j})\]</div>
<p>For example,
in the case of three random variables, <span class="math notranslate nohighlight">\(X, Y, Z\)</span>, we require that
<span class="math notranslate nohighlight">\(p(X,Y,Z) = p(X)p(Y)p(Z)\)</span> as well as <span class="math notranslate nohighlight">\(p(X,Y) = p(X)p(Y)\)</span>,
<span class="math notranslate nohighlight">\(p(X,Z) = p(X)p(Z)\)</span>, and <span class="math notranslate nohighlight">\(p(Y,Z) = p(Y)p(Z)\)</span>.</p>
<p>It is often convenient (though perhaps questionable) to assume that a
bunch of random variables are <strong>independent and identically
distributed</strong> (i.i.d.) so that their joint distribution can be factored
entirely:</p>
<div class="math notranslate nohighlight">
\[p(X_1, \dots, X_n) = \prod_{i=1}^n p(X_i)\]</div>
<p>where
<span class="math notranslate nohighlight">\(X_1, \dots, X_n\)</span> all share the same p.m.f./p.d.f.</p>
</section>
<section id="marginal-distributions">
<h3>Marginal distributions<a class="headerlink" href="#marginal-distributions" title="Link to this heading">#</a></h3>
<p>If we have a joint distribution over some set of random variables, it is
possible to obtain a distribution for a subset of them by “summing out”
(or “integrating out” in the continuous case) the variables we don’t
care about:</p>
<div class="math notranslate nohighlight">
\[p(X) = \sum_{y} p(X, y)\]</div>
</section>
</section>
<section id="great-expectations">
<h2>Great Expectations<a class="headerlink" href="#great-expectations" title="Link to this heading">#</a></h2>
<p>If we have some random variable <span class="math notranslate nohighlight">\(X\)</span>, we might be interested in knowing
what is the “average” value of <span class="math notranslate nohighlight">\(X\)</span>. This concept is captured by the
<strong>expected value</strong> (or <strong>mean</strong>) <span class="math notranslate nohighlight">\(\mathbb{E}[X]\)</span>, which is defined as</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[X] = \sum_{x \in X(\Omega)} xp(x)\]</div>
<p>for discrete <span class="math notranslate nohighlight">\(X\)</span> and as</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[X] = \int_{-\infty}^\infty xp(x)\operatorname{d}{x}\]</div>
<p>for continuous
<span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>In words, we are taking a weighted sum of the values that <span class="math notranslate nohighlight">\(X\)</span> can take
on, where the weights are the probabilities of those respective values.
The expected value has a physical interpretation as the “center of mass”
of the distribution.</p>
<section id="properties-of-expected-value">
<h3>Properties of expected value<a class="headerlink" href="#properties-of-expected-value" title="Link to this heading">#</a></h3>
<p>A very useful property of expectation is that of linearity:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left[\sum_{i=1}^n \alpha_i X_i + \beta\right] = \sum_{i=1}^n \alpha_i \mathbb{E}[X_i] + \beta\]</div>
<p>Note that this holds even if the <span class="math notranslate nohighlight">\(X_i\)</span> are not independent!</p>
<p>But if they are independent, the product rule also holds:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left[\prod_{i=1}^n X_i\right] = \prod_{i=1}^n \mathbb{E}[X_i]\]</div>
</section>
</section>
<section id="variance">
<h2>Variance<a class="headerlink" href="#variance" title="Link to this heading">#</a></h2>
<p>Expectation provides a measure of the “center” of a distribution, but
frequently we are also interested in what the “spread” is about that
center. We define the variance <span class="math notranslate nohighlight">\(\operatorname{Var}(X)\)</span> of a random
variable <span class="math notranslate nohighlight">\(X\)</span> by</p>
<div class="math notranslate nohighlight">
\[\operatorname{Var}(X) = \mathbb{E}\left[\left(X - \mathbb{E}[X]\right)^2\right]\]</div>
<p>In words, this is the average squared deviation of the values of <span class="math notranslate nohighlight">\(X\)</span>
from the mean of <span class="math notranslate nohighlight">\(X\)</span>. Using a little algebra and the linearity of
expectation, it is straightforward to show that</p>
<div class="math notranslate nohighlight">
\[\operatorname{Var}(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2\]</div>
<section id="properties-of-variance">
<h3>Properties of variance<a class="headerlink" href="#properties-of-variance" title="Link to this heading">#</a></h3>
<p>Variance is not linear (because of the squaring in the definition), but
one can show the following:</p>
<div class="math notranslate nohighlight">
\[\operatorname{Var}(\alpha X + \beta) = \alpha^2 \operatorname{Var}(X)\]</div>
<p>Basically, multiplicative constants become squared when they are pulled
out, and additive constants disappear (since the variance contributed by
a constant is zero).</p>
<p>Furthermore, if <span class="math notranslate nohighlight">\(X_1, \dots, X_n\)</span> are uncorrelated<a class="footnote-reference brackets" href="#id16" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a>, then</p>
<div class="math notranslate nohighlight">
\[\operatorname{Var}(X_1 + \dots + X_n) = \operatorname{Var}(X_1) + \dots + \operatorname{Var}(X_n)\]</div>
</section>
<section id="standard-deviation">
<h3>Standard deviation<a class="headerlink" href="#standard-deviation" title="Link to this heading">#</a></h3>
<p>Variance is a useful notion, but it suffers from that fact the units of
variance are not the same as the units of the random variable (again
because of the squaring). To overcome this problem we can use <strong>standard
deviation</strong>, which is defined as <span class="math notranslate nohighlight">\(\sqrt{\operatorname{Var}(X)}\)</span>. The
standard deviation of <span class="math notranslate nohighlight">\(X\)</span> has the same units as <span class="math notranslate nohighlight">\(X\)</span>.</p>
</section>
</section>
<section id="covariance">
<h2>Covariance<a class="headerlink" href="#covariance" title="Link to this heading">#</a></h2>
<p>Covariance is a measure of the linear relationship between two random
variables. We denote the covariance between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> as
<span class="math notranslate nohighlight">\(\operatorname{Cov}(X, Y)\)</span>, and it is defined to be</p>
<div class="math notranslate nohighlight">
\[\operatorname{Cov}(X, Y) = \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]\]</div>
<p>Note that the outer expectation must be taken over the joint
distribution of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>Again, the linearity of expectation allows us to rewrite this as</p>
<div class="math notranslate nohighlight">
\[\operatorname{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]\]</div>
<p>Comparing these formulas to the ones for variance, it is not hard to see
that <span class="math notranslate nohighlight">\(\operatorname{Var}(X) = \operatorname{Cov}(X, X)\)</span>.</p>
<p>A useful property of covariance is that of <strong>bilinearity</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\operatorname{Cov}(\alpha X + \beta Y, Z) &amp;= \alpha\operatorname{Cov}(X, Z) + \beta\operatorname{Cov}(Y, Z) \\
\operatorname{Cov}(X, \alpha Y + \beta Z) &amp;= \alpha\operatorname{Cov}(X, Y) + \beta\operatorname{Cov}(X, Z)
\end{aligned}\end{split}\]</div>
<section id="correlation">
<h3>Correlation<a class="headerlink" href="#correlation" title="Link to this heading">#</a></h3>
<p>Normalizing the covariance gives the <strong>correlation</strong>:</p>
<div class="math notranslate nohighlight">
\[\rho(X, Y) = \frac{\operatorname{Cov}(X, Y)}{\sqrt{\operatorname{Var}(X)\operatorname{Var}(Y)}}\]</div>
<p>Correlation also measures the linear relationship between two variables,
but unlike covariance always lies between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>Two variables are said to be <strong>uncorrelated</strong> if
<span class="math notranslate nohighlight">\(\operatorname{Cov}(X, Y) = 0\)</span> because <span class="math notranslate nohighlight">\(\operatorname{Cov}(X, Y) = 0\)</span>
implies that <span class="math notranslate nohighlight">\(\rho(X, Y) = 0\)</span>. If two variables are independent, then
they are uncorrelated, but the converse does not hold in general.</p>
</section>
</section>
<section id="random-vectors">
<h2>Random vectors<a class="headerlink" href="#random-vectors" title="Link to this heading">#</a></h2>
<p>So far we have been talking about <strong>univariate distributions</strong>, that is,
distributions of single variables. But we can also talk about
<strong>multivariate distributions</strong> which give distributions of <strong>random
vectors</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{X} = \begin{bmatrix}X_1 \\ \vdots \\ X_n\end{bmatrix}\end{split}\]</div>
<p>The
summarizing quantities we have discussed for single variables have
natural generalizations to the multivariate case.</p>
<p>Expectation of a random vector is simply the expectation applied to each
component:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbb{E}[\mathbf{X}] = \begin{bmatrix}\mathbb{E}[X_1] \\ \vdots \\ \mathbb{E}[X_n]\end{bmatrix}\end{split}\]</div>
<p>The variance is generalized by the <strong>covariance matrix</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{\Sigma} = \mathbb{E}[(\mathbf{X} - \mathbb{E}[\mathbf{X}])(\mathbf{X} - \mathbb{E}[\mathbf{X}])^{\!\top\!}] = \begin{bmatrix}
\operatorname{Var}(X_1) &amp; \operatorname{Cov}(X_1, X_2) &amp; \dots &amp; \operatorname{Cov}(X_1, X_n) \\
\operatorname{Cov}(X_2, X_1) &amp; \operatorname{Var}(X_2) &amp; \dots &amp; \operatorname{Cov}(X_2, X_n) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\operatorname{Cov}(X_n, X_1) &amp; \operatorname{Cov}(X_n, X_2) &amp; \dots &amp; \operatorname{Var}(X_n)
\end{bmatrix}\end{split}\]</div>
<p>That is, <span class="math notranslate nohighlight">\(\Sigma_{ij} = \operatorname{Cov}(X_i, X_j)\)</span>.
Since covariance is symmetric in its arguments, the covariance matrix is
also symmetric. It’s also positive semi-definite: for any <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}^{\!\top\!}\mathbf{\Sigma}\mathbf{x} = \mathbf{x}^{\!\top\!}\mathbb{E}[(\mathbf{X} - \mathbb{E}[\mathbf{X}])(\mathbf{X} - \mathbb{E}[\mathbf{X}])^{\!\top\!}]\mathbf{x} = \mathbb{E}[\mathbf{x}^{\!\top\!}(\mathbf{X} - \mathbb{E}[\mathbf{X}])(\mathbf{X} - \mathbb{E}[\mathbf{X}])^{\!\top\!}\mathbf{x}] = \mathbb{E}[((\mathbf{X} - \mathbb{E}[\mathbf{X}])^{\!\top\!}\mathbf{x})^2] \geq 0\]</div>
<p>The inverse of the covariance matrix, <span class="math notranslate nohighlight">\(\mathbf{\Sigma}^{-1}\)</span>, is
sometimes called the <strong>precision matrix</strong>.</p>
</section>
<section id="estimation-of-parameters">
<h2>Estimation of Parameters<a class="headerlink" href="#estimation-of-parameters" title="Link to this heading">#</a></h2>
<p>Now we get into some basic topics from statistics. We make some
assumptions about our problem by prescribing a <strong>parametric</strong> model
(e.g. a distribution that describes how the data were generated), then
we fit the parameters of the model to the data. How do we choose the
values of the parameters?</p>
<section id="maximum-likelihood-estimation">
<h3>Maximum likelihood estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Link to this heading">#</a></h3>
<p>A common way to fit parameters is <strong>maximum likelihood estimation</strong>
(MLE). The basic principle of MLE is to choose values that “explain” the
data best by maximizing the probability/density of the data we’ve seen
as a function of the parameters. Suppose we have random variables
<span class="math notranslate nohighlight">\(X_1, \dots, X_n\)</span> and corresponding observations <span class="math notranslate nohighlight">\(x_1, \dots, x_n\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\hat{\mathbf{\theta}}_\text{mle} = \operatorname{argmax}_\mathbf{\theta} \mathcal{L}(\mathbf{\theta})\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is the <strong>likelihood function</strong></p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\mathbf{\theta}) = p(x_1, \dots, x_n; \mathbf{\theta})\]</div>
<p>Often, we assume that <span class="math notranslate nohighlight">\(X_1, \dots, X_n\)</span> are i.i.d. Then we can write</p>
<div class="math notranslate nohighlight">
\[p(x_1, \dots, x_n; \theta) = \prod_{i=1}^n p(x_i; \mathbf{\theta})\]</div>
<p>At this point, it is usually convenient to take logs, giving rise to the
<strong>log-likelihood</strong></p>
<div class="math notranslate nohighlight">
\[\log\mathcal{L}(\mathbf{\theta}) = \sum_{i=1}^n \log p(x_i; \mathbf{\theta})\]</div>
<p>This is a valid operation because the probabilities/densities are
assumed to be positive, and since log is a monotonically increasing
function, it preserves ordering. In other words, any maximizer of
<span class="math notranslate nohighlight">\(\log\mathcal{L}\)</span> will also maximize <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>.</p>
<p>For some distributions, it is possible to analytically solve for the
maximum likelihood estimator. If <span class="math notranslate nohighlight">\(\log\mathcal{L}\)</span> is differentiable,
setting the derivatives to zero and trying to solve for
<span class="math notranslate nohighlight">\(\mathbf{\theta}\)</span> is a good place to start.</p>
</section>
<section id="maximum-a-posteriori-estimation">
<h3>Maximum a posteriori estimation<a class="headerlink" href="#maximum-a-posteriori-estimation" title="Link to this heading">#</a></h3>
<p>A more Bayesian way to fit parameters is through <strong>maximum a posteriori
estimation</strong> (MAP). In this technique we assume that the parameters are
a random variable, and we specify a prior distribution
<span class="math notranslate nohighlight">\(p(\mathbf{\theta})\)</span>. Then we can employ Bayes’ rule to compute the
posterior distribution of the parameters given the observed data:</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{\theta} | x_1, \dots, x_n) \propto p(\mathbf{\theta})p(x_1, \dots, x_n | \mathbf{\theta})\]</div>
<p>Computing the normalizing constant is often intractable, because it
involves integrating over the parameter space, which may be very
high-dimensional. Fortunately, if we just want the MAP estimate, we
don’t care about the normalizing constant! It does not affect which
values of <span class="math notranslate nohighlight">\(\mathbf{\theta}\)</span> maximize the posterior. So we have</p>
<div class="math notranslate nohighlight">
\[\hat{\mathbf{\theta}}_\text{map} = \operatorname{argmax}_\mathbf{\theta} p(\mathbf{\theta})p(x_1, \dots, x_n | \mathbf{\theta})\]</div>
<p>Again, if we assume the observations are i.i.d., then we can express
this in the equivalent, and possibly friendlier, form</p>
<div class="math notranslate nohighlight">
\[\hat{\mathbf{\theta}}_\text{map} = \operatorname{argmax}_\mathbf{\theta} \left(\log p(\mathbf{\theta}) + \sum_{i=1}^n \log p(x_i | \mathbf{\theta})\right)\]</div>
<p>A particularly nice case is when the prior is chosen carefully such that
the posterior comes from the same family as the prior. In this case the
prior is called a <strong>conjugate prior</strong>. For example, if the likelihood is
binomial and the prior is beta, the posterior is also beta. There are
many conjugate priors; the reader may find this <a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions">table of conjugate
priors</a>
useful.</p>
</section>
</section>
<section id="the-gaussian-distribution">
<h2>The Gaussian distribution<a class="headerlink" href="#the-gaussian-distribution" title="Link to this heading">#</a></h2>
<p>There are many distributions, but one of particular importance is the
<strong>Gaussian distribution</strong>, also known as the <strong>normal distribution</strong>. It
is a continuous distribution, parameterized by its mean
<span class="math notranslate nohighlight">\(\boldsymbol\mu \in \mathbb{R}^d\)</span> and positive-definite covariance matrix
<span class="math notranslate nohighlight">\(\mathbf{\Sigma} \in \mathbb{R}^{d \times d}\)</span>, with density</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{x}; \boldsymbol\mu, \mathbf{\Sigma}) = \frac{1}{\sqrt{(2\pi)^d \det(\mathbf{\Sigma})}}\exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol\mu)^{\!\top\!}\mathbf{\Sigma}^{-1}(\mathbf{x} - \boldsymbol\mu)\right)\]</div>
<p>Note that in the special case <span class="math notranslate nohighlight">\(d = 1\)</span>, the density is written in the
more recognizable form</p>
<div class="math notranslate nohighlight">
\[p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</div>
<p>We write <span class="math notranslate nohighlight">\(\mathbf{X} \sim \mathcal{N}(\boldsymbol\mu, \mathbf{\Sigma})\)</span> to
denote that <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is normally distributed with mean <span class="math notranslate nohighlight">\(\boldsymbol\mu\)</span> and
variance <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>.</p>
<section id="the-geometry-of-multivariate-gaussians">
<h3>The geometry of multivariate Gaussians<a class="headerlink" href="#the-geometry-of-multivariate-gaussians" title="Link to this heading">#</a></h3>
<p>The geometry of the multivariate Gaussian density is intimately related
to the geometry of positive definite quadratic forms, so make sure the
material in that section is well-understood before tackling this
section.</p>
<p>First observe that the p.d.f. of the multivariate Gaussian can be
rewritten as</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{x}; \boldsymbol\mu, \mathbf{\Sigma}) = g(\tilde{\mathbf{x}}^{\!\top\!}\mathbf{\Sigma}^{-1}\tilde{\mathbf{x}})\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}} = \mathbf{x} - \boldsymbol\mu\)</span> and
<span class="math notranslate nohighlight">\(g(z) = [(2\pi)^d \det(\mathbf{\Sigma})]^{-\frac{1}{2}}\exp\left(-\frac{z}{2}\right)\)</span>.
Writing the density in this way, we see that after shifting by the mean
<span class="math notranslate nohighlight">\(\boldsymbol\mu\)</span>, the density is really just a simple function of its precision
matrix’s quadratic form.</p>
<p>Here is a key observation: this function <span class="math notranslate nohighlight">\(g\)</span> is <strong>strictly monotonically
decreasing</strong> in its argument. That is, <span class="math notranslate nohighlight">\(g(a) &gt; g(b)\)</span> whenever <span class="math notranslate nohighlight">\(a &lt; b\)</span>.
Therefore, small values of
<span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}^{\!\top\!}\mathbf{\Sigma}^{-1}\tilde{\mathbf{x}}\)</span>
(which generally correspond to points where <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}\)</span> is
closer to <span class="math notranslate nohighlight">\(\mathbf{0}\)</span>, i.e. <span class="math notranslate nohighlight">\(\mathbf{x} \approx \boldsymbol\mu\)</span>) have
relatively high probability densities, and vice-versa. Furthermore,
because <span class="math notranslate nohighlight">\(g\)</span> is <em>strictly</em> monotonic, it is injective, so the
<span class="math notranslate nohighlight">\(c\)</span>-isocontours of <span class="math notranslate nohighlight">\(p(\mathbf{x}; \boldsymbol\mu, \mathbf{\Sigma})\)</span> are the
<span class="math notranslate nohighlight">\(g^{-1}(c)\)</span>-isocontours of the function
<span class="math notranslate nohighlight">\(\mathbf{x} \mapsto \tilde{\mathbf{x}}^{\!\top\!}\mathbf{\Sigma}^{-1}\tilde{\mathbf{x}}\)</span>.
That is, for any <span class="math notranslate nohighlight">\(c\)</span>,</p>
<div class="math notranslate nohighlight">
\[\{\mathbf{x} \in \mathbb{R}^d : p(\mathbf{x}; \boldsymbol\mu, \mathbf{\Sigma}) = c\} = \{\mathbf{x} \in \mathbb{R}^d : \tilde{\mathbf{x}}^{\!\top\!}\mathbf{\Sigma}^{-1}\tilde{\mathbf{x}} = g^{-1}(c)\}\]</div>
<p>In words, these functions have the same isocontours but different
isovalues.</p>
<p>Recall the executive summary of the geometry of positive definite
quadratic forms: the isocontours of
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}\)</span> are
ellipsoids such that the axes point in the directions of the
eigenvectors of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, and the lengths of these axes are
proportional to the inverse square roots of the corresponding
eigenvalues. Therefore in this case, the isocontours of the density are
ellipsoids (centered at <span class="math notranslate nohighlight">\(\boldsymbol\mu\)</span>) with axis lengths proportional to the
inverse square roots of the eigenvalues of <span class="math notranslate nohighlight">\(\mathbf{\Sigma}^{-1}\)</span>, or
equivalently, the square roots of the eigenvalues of <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>.</p>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id9" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">7</a><span class="fn-bracket">]</span></span>
<p><span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is required to be a <span class="math notranslate nohighlight">\(\sigma\)</span>-algebra for technical
reasons; see [&#64;rigorousprob].</p>
</aside>
<aside class="footnote brackets" id="id10" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">8</a><span class="fn-bracket">]</span></span>
<p>Note that a probability space is simply a measure space in which
the measure of the whole space equals 1.</p>
</aside>
<aside class="footnote brackets" id="id11" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">9</a><span class="fn-bracket">]</span></span>
<p>This is a probabilist’s version of the measure-theoretic term
<em>almost everywhere</em>.</p>
</aside>
<aside class="footnote brackets" id="id12" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">10</a><span class="fn-bracket">]</span></span>
<p>In some cases it is possible to define conditional probability on
events of probability zero, but this is significantly more technical
so we omit it.</p>
</aside>
<aside class="footnote brackets" id="id13" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">11</a><span class="fn-bracket">]</span></span>
<p>The function must be measurable.</p>
</aside>
<aside class="footnote brackets" id="id14" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">12</a><span class="fn-bracket">]</span></span>
<p>More generally, the codomain can be any measurable space, but
<span class="math notranslate nohighlight">\(\mathbb{R}\)</span> is the most common case by far and sufficient for our
purposes.</p>
</aside>
<aside class="footnote brackets" id="id15" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">13</a><span class="fn-bracket">]</span></span>
<p>Random variables that are continuous but not absolutely
continuous are called <strong>singular random variables</strong>. We will not
discuss them, assuming rather that all continuous random variables
admit a density function.</p>
</aside>
<aside class="footnote brackets" id="id16" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">14</a><span class="fn-bracket">]</span></span>
<p>We haven’t defined this yet; see the Correlation section below</p>
</aside>
</aside>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_probability"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_calculus/calculus.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Calculus and Optimization</p>
      </div>
    </a>
    <a class="right-next"
       href="../logistic_regression_uci_breast_cancer/phenotype_classification_LogisticRegression.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Machine Learning for Human Phenotype Classification</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basics">Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability">Conditional probability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule">Chain rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-rule">Bayes’ rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variables">Random variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cumulative-distribution-function">The cumulative distribution function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-random-variables">Discrete random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-random-variables">Continuous random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-kinds-of-random-variables">Other kinds of random variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distributions">Joint distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-of-random-variables">Independence of random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-distributions">Marginal distributions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#great-expectations">Great Expectations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-expected-value">Properties of expected value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-variance">Properties of variance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-deviation">Standard deviation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation">Correlation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-vectors">Random vectors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-parameters">Estimation of Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum likelihood estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-estimation">Maximum a posteriori estimation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-distribution">The Gaussian distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-geometry-of-multivariate-gaussians">The geometry of multivariate Gaussians</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christoph Lippert
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>