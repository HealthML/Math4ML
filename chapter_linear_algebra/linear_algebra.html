
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Linear Algebra &#8212; Mathematics for Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_linear_algebra/linear_algebra';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Calculus and Optimization" href="../chapter_calculus/calculus.html" />
    <link rel="prev" title="Notation" href="../chapter_preface/intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/hpi-logo-colored.svg" class="logo__image only-light" alt="Mathematics for Machine Learning - Home"/>
    <img src="../_static/hpi-logo-colored.svg" class="logo__image only-dark pst-js-only" alt="Mathematics for Machine Learning - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Mathematics for Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/intro.html">Notation</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Linear Algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_calculus/calculus.html">Calculus and Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_probability/probability.html">Probability</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fchapter_linear_algebra/linear_algebra.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_linear_algebra/linear_algebra.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Algebra</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-spaces">Vector spaces</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#euclidean-space">Euclidean space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subspaces">Subspaces</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metric-spaces">Metric spaces</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normed-spaces">Normed spaces</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inner-product-spaces">Inner product spaces</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pythagorean-theorem">Pythagorean Theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cauchy-schwarz-inequality">Cauchy-Schwarz inequality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transposition">Transposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenthings">Eigenthings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#trace">Trace</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determinant">Determinant</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#orthogonal-matrices">Orthogonal matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#symmetric-matrices">Symmetric matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rayleigh-quotients">Rayleigh quotients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positive-semi-definite-matrices">Positive (semi-)definite matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-geometry-of-positive-definite-quadratic-forms">The geometry of positive definite quadratic forms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#singular-value-decomposition">Singular value decomposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-useful-matrix-identities">Some useful matrix identities</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-vector-product-as-linear-combination-of-matrix-columns">Matrix-vector product as linear combination of matrix columns</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sum-of-outer-products-as-matrix-matrix-product">Sum of outer products as matrix-matrix product</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-forms">Quadratic forms</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-algebra">
<h1>Linear Algebra<a class="headerlink" href="#linear-algebra" title="Link to this heading">#</a></h1>
<p>In this section we present important classes of spaces in which our data
will live and our operations will take place: vector spaces, metric
spaces, normed spaces, and inner product spaces. Generally speaking,
these are defined in such a way as to capture one or more important
properties of Euclidean space but in a more general way.</p>
<section id="vector-spaces">
<h2>Vector spaces<a class="headerlink" href="#vector-spaces" title="Link to this heading">#</a></h2>
<p><strong>Vector spaces</strong> are the basic setting in which linear algebra happens.
A vector space <span class="math notranslate nohighlight">\(V\)</span> is a set (the elements of which are called
<strong>vectors</strong>) on which two operations are defined: vectors can be added
together, and vectors can be multiplied by real numbers[^1] called
<strong>scalars</strong>. <span class="math notranslate nohighlight">\(V\)</span> must satisfy</p>
<p>(i) There exists an additive identity (written <span class="math notranslate nohighlight">\(\mathbf{0}\)</span>) in <span class="math notranslate nohighlight">\(V\)</span> such
that <span class="math notranslate nohighlight">\(\mathbf{x}+\mathbf{0} = \mathbf{x}\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x} \in V\)</span></p>
<p>(ii) For each <span class="math notranslate nohighlight">\(\mathbf{x} \in V\)</span>, there exists an additive inverse
(written <span class="math notranslate nohighlight">\(\mathbf{-x}\)</span>) such that
<span class="math notranslate nohighlight">\(\mathbf{x}+(\mathbf{-x}) = \mathbf{0}\)</span></p>
<p>(iii) There exists a multiplicative identity (written <span class="math notranslate nohighlight">\(1\)</span>) in
<span class="math notranslate nohighlight">\(\mathbb{R}\)</span> such that <span class="math notranslate nohighlight">\(1\mathbf{x} = \mathbf{x}\)</span> for all
<span class="math notranslate nohighlight">\(\mathbf{x} \in V\)</span></p>
<p>(iv) Commutativity: <span class="math notranslate nohighlight">\(\mathbf{x}+\mathbf{y} = \mathbf{y}+\mathbf{x}\)</span> for
all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in V\)</span></p>
<p>(v) Associativity:
<span class="math notranslate nohighlight">\((\mathbf{x}+\mathbf{y})+\mathbf{z} = \mathbf{x}+(\mathbf{y}+\mathbf{z})\)</span>
and <span class="math notranslate nohighlight">\(\alpha(\beta\mathbf{x}) = (\alpha\beta)\mathbf{x}\)</span> for all
<span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y}, \mathbf{z} \in V\)</span> and
<span class="math notranslate nohighlight">\(\alpha, \beta \in \mathbb{R}\)</span></p>
<p>(vi) Distributivity:
<span class="math notranslate nohighlight">\(\alpha(\mathbf{x}+\mathbf{y}) = \alpha\mathbf{x} + \alpha\mathbf{y}\)</span>
and <span class="math notranslate nohighlight">\((\alpha+\beta)\mathbf{x} = \alpha\mathbf{x} + \beta\mathbf{x}\)</span>
for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in V\)</span> and
<span class="math notranslate nohighlight">\(\alpha, \beta \in \mathbb{R}\)</span></p>
<section id="euclidean-space">
<h3>Euclidean space<a class="headerlink" href="#euclidean-space" title="Link to this heading">#</a></h3>
<p>The quintessential vector space is <strong>Euclidean space</strong>, which we denote
<span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. The vectors in this space consist of <span class="math notranslate nohighlight">\(n\)</span>-tuples of real
numbers: $<span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, x_2, \dots, x_n)\)</span><span class="math notranslate nohighlight">\( For our purposes, it
will be useful to think of them as \)</span>n \times 1<span class="math notranslate nohighlight">\( matrices, or **column
vectors**:
\)</span><span class="math notranslate nohighlight">\(\mathbf{x} = \begin{bmatrix}x_1 \\ x_2 \\ \vdots \\ x_n\end{bmatrix}\)</span><span class="math notranslate nohighlight">\(
Addition and scalar multiplication are defined component-wise on vectors
in \)</span>\mathbb{R}^n<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(\mathbf{x} + \mathbf{y} = \begin{bmatrix}x_1 + y_1 \\ \vdots \\ x_n + y_n\end{bmatrix}, \hspace{0.5cm} \alpha\mathbf{x} = \begin{bmatrix}\alpha x_1 \\ \vdots \\ \alpha x_n\end{bmatrix}\)</span><span class="math notranslate nohighlight">\(
Euclidean space is used to mathematically represent physical space, with
notions such as distance, length, and angles. Although it becomes hard
to visualize for \)</span>n &gt; 3<span class="math notranslate nohighlight">\(, these concepts generalize mathematically in
obvious ways. Even when you're working in more general settings than
\)</span>\mathbb{R}^n$, it is often useful to visualize vector addition and
scalar multiplication in terms of 2D vectors in the plane or 3D vectors
in space.</p>
</section>
<section id="subspaces">
<h3>Subspaces<a class="headerlink" href="#subspaces" title="Link to this heading">#</a></h3>
<p>Vector spaces can contain other vector spaces. If <span class="math notranslate nohighlight">\(V\)</span> is a vector space,
then <span class="math notranslate nohighlight">\(S \subseteq V\)</span> is said to be a <strong>subspace</strong> of <span class="math notranslate nohighlight">\(V\)</span> if</p>
<p>(i) <span class="math notranslate nohighlight">\(\mathbf{0} \in S\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\(S\)</span> is closed under addition: <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in S\)</span>
implies <span class="math notranslate nohighlight">\(\mathbf{x}+\mathbf{y} \in S\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\(S\)</span> is closed under scalar multiplication:
<span class="math notranslate nohighlight">\(\mathbf{x} \in S, \alpha \in \mathbb{R}\)</span> implies
<span class="math notranslate nohighlight">\(\alpha\mathbf{x} \in S\)</span></p>
<p>Note that <span class="math notranslate nohighlight">\(V\)</span> is always a subspace of <span class="math notranslate nohighlight">\(V\)</span>, as is the trivial vector
space which contains only <span class="math notranslate nohighlight">\(\mathbf{0}\)</span>.</p>
<p>As a concrete example, a line passing through the origin is a subspace
of Euclidean space.</p>
<p>Some of the most important subspaces are those induced by linear maps.
If <span class="math notranslate nohighlight">\(T : V \to W\)</span> is a linear map, we define the <strong>nullspace</strong> (or <strong>kernel</strong>) of <span class="math notranslate nohighlight">\(T\)</span>
as</p>
<p><span class="math notranslate nohighlight">\(\operatorname{null}(T) = \{\mathbf{x} \in V \mid T\mathbf{x} = \mathbf{0}\}.\)</span></p>
<p>and
the <strong>range</strong> (or the <strong>columnspace</strong> if we are considering the matrix
form) of <span class="math notranslate nohighlight">\(T\)</span> as</p>
<p><span class="math notranslate nohighlight">\(\operatorname{range}(T) = \{\mathbf{y} \in W \mid \exists \mathbf{x} \in V : T\mathbf{x} = \mathbf{y}\}.\)</span></p>
<p>It is a good exercise to verify that the nullspace and range of a linear
map are always subspaces of its domain and codomain, respectively.</p>
</section>
</section>
<section id="metric-spaces">
<h2>Metric spaces<a class="headerlink" href="#metric-spaces" title="Link to this heading">#</a></h2>
<p>Metrics generalize the notion of distance from Euclidean space (although
metric spaces need not be vector spaces).</p>
<p>A <strong>metric</strong> on a set <span class="math notranslate nohighlight">\(S\)</span> is a function <span class="math notranslate nohighlight">\(d : S \times S \to \mathbb{R}\)</span>
that satisfies</p>
<p>(i) <span class="math notranslate nohighlight">\(d(x,y) \geq 0\)</span>, with equality if and only if <span class="math notranslate nohighlight">\(x = y\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\(d(x,y) = d(y,x)\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\(d(x,z) \leq d(x,y) + d(y,z)\)</span> (the so-called <strong>triangle
inequality</strong>)</p>
<p>for all <span class="math notranslate nohighlight">\(x, y, z \in S\)</span>.</p>
<p>A key motivation for metrics is that they allow limits to be defined for
mathematical objects other than real numbers. We say that a sequence
<span class="math notranslate nohighlight">\(\{x_n\} \subseteq S\)</span> converges to the limit <span class="math notranslate nohighlight">\(x\)</span> if for any
<span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>, there exists <span class="math notranslate nohighlight">\(N \in \mathbb{N}\)</span> such that
<span class="math notranslate nohighlight">\(d(x_n, x) &lt; \epsilon\)</span> for all <span class="math notranslate nohighlight">\(n \geq N\)</span>. Note that the definition for
limits of sequences of real numbers, which you have likely seen in a
calculus class, is a special case of this definition when using the
metric <span class="math notranslate nohighlight">\(d(x, y) = |x-y|\)</span>.</p>
</section>
<section id="normed-spaces">
<h2>Normed spaces<a class="headerlink" href="#normed-spaces" title="Link to this heading">#</a></h2>
<p>Norms generalize the notion of length from Euclidean space.</p>
<p>A <strong>norm</strong> on a real vector space <span class="math notranslate nohighlight">\(V\)</span> is a function
<span class="math notranslate nohighlight">\(\|\cdot\| : V \to \mathbb{R}\)</span> that satisfies</p>
<p>(i) <span class="math notranslate nohighlight">\(\|\mathbf{x}\| \geq 0\)</span>, with equality if and only if
<span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{0}\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\(\|\alpha\mathbf{x}\| = |\alpha|\|\mathbf{x}\|\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\(\|\mathbf{x}+\mathbf{y}\| \leq \|\mathbf{x}\| + \|\mathbf{y}\|\)</span>
(the <strong>triangle inequality</strong> again)</p>
<p>for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in V\)</span> and all <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span>.
A vector space endowed with a norm is called a <strong>normed vector space</strong>,
or simply a <strong>normed space</strong>.</p>
<p>Note that any norm on <span class="math notranslate nohighlight">\(V\)</span> induces a distance metric on <span class="math notranslate nohighlight">\(V\)</span>:</p>
<p><span class="math notranslate nohighlight">\(d(\mathbf{x}, \mathbf{y}) = \|\mathbf{x}-\mathbf{y}\|\)</span></p>
<p>One can verify that the axioms for metrics are satisfied under this definition and
follow directly from the axioms for norms. Therefore any normed space is
also a metric space.[^3]</p>
<p>We will typically only be concerned with a few specific norms on
<span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>:
$<span class="math notranslate nohighlight">\(\begin{aligned}
\|\mathbf{x}\|_1 &amp;= \sum_{i=1}^n |x_i| \\
\|\mathbf{x}\|_2 &amp;= \sqrt{\sum_{i=1}^n x_i^2} \\
\|\mathbf{x}\|_p &amp;= \left(\sum_{i=1}^n |x_i|^p\right)^\frac{1}{p} \hspace{0.5cm}\hspace{0.5cm} (p \geq 1) \\
\|\mathbf{x}\|_\infty &amp;= \max_{1 \leq i \leq n} |x_i|
\end{aligned}\)</span><span class="math notranslate nohighlight">\(
Note that the 1- and 2-norms are special cases of the
\)</span>p<span class="math notranslate nohighlight">\(-norm, and the \)</span>\infty<span class="math notranslate nohighlight">\(-norm is the limit of the \)</span>p<span class="math notranslate nohighlight">\(-norm as \)</span>p<span class="math notranslate nohighlight">\(
tends to infinity. We require \)</span>p \geq 1<span class="math notranslate nohighlight">\( for the general definition of
the \)</span>p<span class="math notranslate nohighlight">\(-norm because the triangle inequality fails to hold if \)</span>p &lt; 1$.
(Try to find a counterexample!)</p>
<p>Here’s a fun fact: for any given finite-dimensional vector space <span class="math notranslate nohighlight">\(V\)</span>,
all norms on <span class="math notranslate nohighlight">\(V\)</span> are equivalent in the sense that for two norms
<span class="math notranslate nohighlight">\(\|\cdot\|_A, \|\cdot\|_B\)</span>, there exist constants <span class="math notranslate nohighlight">\(\alpha, \beta &gt; 0\)</span>
such that
$<span class="math notranslate nohighlight">\(\alpha\|\mathbf{x}\|_A \leq \|\mathbf{x}\|_B \leq \beta\|\mathbf{x}\|_A\)</span><span class="math notranslate nohighlight">\(
for all \)</span>\mathbf{x} \in V$. Therefore convergence in one norm implies
convergence in any other norm. This rule may not apply in
infinite-dimensional vector spaces such as function spaces, though.</p>
</section>
<section id="inner-product-spaces">
<h2>Inner product spaces<a class="headerlink" href="#inner-product-spaces" title="Link to this heading">#</a></h2>
<p>An <strong>inner product</strong> on a real vector space <span class="math notranslate nohighlight">\(V\)</span> is a function
<span class="math notranslate nohighlight">\(\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}\)</span> satisfying</p>
<p>(i) <span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{x} \rangle \geq 0\)</span>, with equality if
and only if <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{0}\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\(\langle \alpha\mathbf{x} + \beta\mathbf{y}, \mathbf{z} \rangle = \alpha\langle \mathbf{x}, \mathbf{z} \rangle + \beta\langle \mathbf{y}, \mathbf{z} \rangle\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{y} \rangle = \langle \mathbf{y}, \mathbf{x} \rangle\)</span></p>
<p>for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y}, \mathbf{z} \in V\)</span> and all
<span class="math notranslate nohighlight">\(\alpha,\beta \in \mathbb{R}\)</span>. A vector space endowed with an inner
product is called an <strong>inner product space</strong>.</p>
<p>Note that any inner product on <span class="math notranslate nohighlight">\(V\)</span> induces a norm on <span class="math notranslate nohighlight">\(V\)</span>:
$<span class="math notranslate nohighlight">\(\|\mathbf{x}\| = \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle}\)</span>$
One can verify that the axioms for norms are satisfied under this definition
and follow (almost) directly from the axioms for inner products.
Therefore any inner product space is also a normed space (and hence also
a metric space).[^4]</p>
<p>Two vectors <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> are said to be <strong>orthogonal</strong>
if <span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{y} \rangle = 0\)</span>; we write
<span class="math notranslate nohighlight">\(\mathbf{x} \perp \mathbf{y}\)</span> for shorthand. Orthogonality generalizes
the notion of perpendicularity from Euclidean space. If two orthogonal
vectors <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> additionally have unit length
(i.e. <span class="math notranslate nohighlight">\(\|\mathbf{x}\| = \|\mathbf{y}\| = 1\)</span>), then they are described as
<strong>orthonormal</strong>.</p>
<p>The standard inner product on <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> is given by
$<span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{y} \rangle = \sum_{i=1}^n x_iy_i = \mathbf{x}^{\!\top\!}\mathbf{y}\)</span><span class="math notranslate nohighlight">\(
The matrix notation on the righthand side (see the Transposition section
if it's unfamiliar) arises because this inner product is a special case
of matrix multiplication where we regard the resulting \)</span>1 \times 1<span class="math notranslate nohighlight">\(
matrix as a scalar. The inner product on \)</span>\mathbb{R}^n<span class="math notranslate nohighlight">\( is also often
written \)</span>\mathbf{x}\cdot\mathbf{y}<span class="math notranslate nohighlight">\( (hence the alternate name **dot
product**). The reader can verify that the two-norm \)</span>|\cdot|_2<span class="math notranslate nohighlight">\( on
\)</span>\mathbb{R}^n$ is induced by this inner product.</p>
<section id="pythagorean-theorem">
<h3>Pythagorean Theorem<a class="headerlink" href="#pythagorean-theorem" title="Link to this heading">#</a></h3>
<p>The well-known Pythagorean theorem generalizes naturally to arbitrary
inner product spaces.</p>
<p><em>Theorem.</em>
If <span class="math notranslate nohighlight">\(\mathbf{x} \perp \mathbf{y}\)</span>, then
$<span class="math notranslate nohighlight">\(\|\mathbf{x}+\mathbf{y}\|^2 = \|\mathbf{x}\|^2 + \|\mathbf{y}\|^2.\)</span>$</p>
<p><em>Proof.</em> Suppose <span class="math notranslate nohighlight">\(\mathbf{x} \perp \mathbf{y}\)</span>, i.e.
<span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{y} \rangle = 0\)</span>. Then
$<span class="math notranslate nohighlight">\(\|\mathbf{x}+\mathbf{y}\|^2 = \langle \mathbf{x}+\mathbf{y}, \mathbf{x}+\mathbf{y} \rangle = \langle \mathbf{x}, \mathbf{x} \rangle + \langle \mathbf{y}, \mathbf{x} \rangle + \langle \mathbf{x}, \mathbf{y} \rangle + \langle \mathbf{y}, \mathbf{y} \rangle = \|\mathbf{x}\|^2 + \|\mathbf{y}\|^2\)</span>$
as claimed. ◻</p>
</section>
<section id="cauchy-schwarz-inequality">
<h3>Cauchy-Schwarz inequality<a class="headerlink" href="#cauchy-schwarz-inequality" title="Link to this heading">#</a></h3>
<p>This inequality is sometimes useful in proving bounds:
$<span class="math notranslate nohighlight">\(|\langle \mathbf{x}, \mathbf{y} \rangle| \leq \|\mathbf{x}\| \cdot \|\mathbf{y}\|\)</span><span class="math notranslate nohighlight">\(
for all \)</span>\mathbf{x}, \mathbf{y} \in V<span class="math notranslate nohighlight">\(. Equality holds exactly when
\)</span>\mathbf{x}<span class="math notranslate nohighlight">\( and \)</span>\mathbf{y}$ are scalar multiples of each other (or
equivalently, when they are linearly dependent).</p>
</section>
</section>
<section id="transposition">
<h2>Transposition<a class="headerlink" href="#transposition" title="Link to this heading">#</a></h2>
<p>If <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span>, its <strong>transpose</strong>
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!} \in \mathbb{R}^{n \times m}\)</span> is given by
<span class="math notranslate nohighlight">\((\mathbf{A}^{\!\top\!})_{ij} = A_{ji}\)</span> for each <span class="math notranslate nohighlight">\((i, j)\)</span>. In other
words, the columns of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> become the rows of
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\)</span>, and the rows of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> become the columns
of <span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\)</span>.</p>
<p>The transpose has several nice algebraic properties that can be easily
verified from the definition:</p>
<p>(i) <span class="math notranslate nohighlight">\((\mathbf{A}^{\!\top\!})^{\!\top\!} = \mathbf{A}\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\((\mathbf{A}+\mathbf{B})^{\!\top\!} = \mathbf{A}^{\!\top\!} + \mathbf{B}^{\!\top\!}\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\((\alpha \mathbf{A})^{\!\top\!} = \alpha \mathbf{A}^{\!\top\!}\)</span></p>
<p>(iv) <span class="math notranslate nohighlight">\((\mathbf{A}\mathbf{B})^{\!\top\!} = \mathbf{B}^{\!\top\!} \mathbf{A}^{\!\top\!}\)</span></p>
</section>
<section id="eigenthings">
<h2>Eigenthings<a class="headerlink" href="#eigenthings" title="Link to this heading">#</a></h2>
<p>For a square matrix <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{n \times n}\)</span>, there may
be vectors which, when <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is applied to them, are simply
scaled by some constant. We say that a nonzero vector
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span> is an <strong>eigenvector</strong> of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>
corresponding to <strong>eigenvalue</strong> <span class="math notranslate nohighlight">\(\lambda\)</span> if
$<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x} = \lambda\mathbf{x}\)</span><span class="math notranslate nohighlight">\(
The zero vector is excluded
from this definition because
\)</span>\mathbf{A}\mathbf{0} = \mathbf{0} = \lambda\mathbf{0}<span class="math notranslate nohighlight">\( for every
\)</span>\lambda$.</p>
<p>We now give some useful results about how eigenvalues change after
various manipulations.</p>
<p><em>Proposition.</em>
Let <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> be an eigenvector of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> with corresponding
eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>. Then</p>
<p>(i) For any <span class="math notranslate nohighlight">\(\gamma \in \mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is an eigenvector of
<span class="math notranslate nohighlight">\(\mathbf{A} + \gamma\mathbf{I}\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda + \gamma\)</span>.</p>
<p>(ii) If <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is invertible, then <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is an eigenvector
of <span class="math notranslate nohighlight">\(\mathbf{A}^{-1}\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda^{-1}\)</span>.</p>
<p>(iii) <span class="math notranslate nohighlight">\(\mathbf{A}^k\mathbf{x} = \lambda^k\mathbf{x}\)</span> for any
<span class="math notranslate nohighlight">\(k \in \mathbb{Z}\)</span> (where <span class="math notranslate nohighlight">\(\mathbf{A}^0 = \mathbf{I}\)</span> by
definition).</p>
<p><em>Proof.</em> (i) follows readily:
$<span class="math notranslate nohighlight">\((\mathbf{A} + \gamma\mathbf{I})\mathbf{x} = \mathbf{A}\mathbf{x} + \gamma\mathbf{I}\mathbf{x} = \lambda\mathbf{x} + \gamma\mathbf{x} = (\lambda + \gamma)\mathbf{x}\)</span>$</p>
<p>(ii) Suppose <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is invertible. Then
$<span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{A}^{-1}\mathbf{A}\mathbf{x} = \mathbf{A}^{-1}(\lambda\mathbf{x}) = \lambda\mathbf{A}^{-1}\mathbf{x}\)</span><span class="math notranslate nohighlight">\(
Dividing by \)</span>\lambda<span class="math notranslate nohighlight">\(, which is valid because the invertibility of
\)</span>\mathbf{A}<span class="math notranslate nohighlight">\( implies \)</span>\lambda \neq 0<span class="math notranslate nohighlight">\(, gives
\)</span>\lambda^{-1}\mathbf{x} = \mathbf{A}^{-1}\mathbf{x}$.</p>
<p>(iii) The case <span class="math notranslate nohighlight">\(k \geq 0\)</span> follows immediately by induction on <span class="math notranslate nohighlight">\(k\)</span>.
Then the general case <span class="math notranslate nohighlight">\(k \in \mathbb{Z}\)</span> follows by combining the
<span class="math notranslate nohighlight">\(k \geq 0\)</span> case with (ii). ◻</p>
</section>
<section id="trace">
<h2>Trace<a class="headerlink" href="#trace" title="Link to this heading">#</a></h2>
<p>The <strong>trace</strong> of a square matrix is the sum of its diagonal entries:
$<span class="math notranslate nohighlight">\(\operatorname{tr}(\mathbf{A}) = \sum_{i=1}^n A_{ii}\)</span>$
The trace has several nice
algebraic properties:</p>
<p>(i) <span class="math notranslate nohighlight">\(\operatorname{tr}(\mathbf{A}+\mathbf{B}) = \operatorname{tr}(\mathbf{A}) + \operatorname{tr}(\mathbf{B})\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\(\operatorname{tr}(\alpha\mathbf{A}) = \alpha\operatorname{tr}(\mathbf{A})\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\(\operatorname{tr}(\mathbf{A}^{\!\top\!}) = \operatorname{tr}(\mathbf{A})\)</span></p>
<p>(iv) <span class="math notranslate nohighlight">\(\operatorname{tr}(\mathbf{A}\mathbf{B}\mathbf{C}\mathbf{D}) = \operatorname{tr}(\mathbf{B}\mathbf{C}\mathbf{D}\mathbf{A}) = \operatorname{tr}(\mathbf{C}\mathbf{D}\mathbf{A}\mathbf{B}) = \operatorname{tr}(\mathbf{D}\mathbf{A}\mathbf{B}\mathbf{C})\)</span></p>
<p>The first three properties follow readily from the definition. The last
is known as <strong>invariance under cyclic permutations</strong>. Note that the
matrices cannot be reordered arbitrarily, for example
<span class="math notranslate nohighlight">\(\operatorname{tr}(\mathbf{A}\mathbf{B}\mathbf{C}\mathbf{D}) \neq \operatorname{tr}(\mathbf{B}\mathbf{A}\mathbf{C}\mathbf{D})\)</span>
in general. Also, there is nothing special about the product of four
matrices – analogous rules hold for more or fewer matrices.</p>
<p>Interestingly, the trace of a matrix is equal to the sum of its
eigenvalues (repeated according to multiplicity):
$<span class="math notranslate nohighlight">\(\operatorname{tr}(\mathbf{A}) = \sum_i \lambda_i(\mathbf{A})\)</span>$</p>
</section>
<section id="determinant">
<h2>Determinant<a class="headerlink" href="#determinant" title="Link to this heading">#</a></h2>
<p>The <strong>determinant</strong> of a square matrix can be defined in several
different confusing ways, none of which are particularly important for
our purposes; go look at an introductory linear algebra text (or
Wikipedia) if you need a definition. But it’s good to know the
properties:</p>
<p>(i) <span class="math notranslate nohighlight">\(\det(\mathbf{I}) = 1\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\(\det(\mathbf{A}^{\!\top\!}) = \det(\mathbf{A})\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\(\det(\mathbf{A}\mathbf{B}) = \det(\mathbf{A})\det(\mathbf{B})\)</span></p>
<p>(iv) <span class="math notranslate nohighlight">\(\det(\mathbf{A}^{-1}) = \det(\mathbf{A})^{-1}\)</span></p>
<p>(v) <span class="math notranslate nohighlight">\(\det(\alpha\mathbf{A}) = \alpha^n \det(\mathbf{A})\)</span></p>
<p>Interestingly, the determinant of a matrix is equal to the product of
its eigenvalues (repeated according to multiplicity):
$<span class="math notranslate nohighlight">\(\det(\mathbf{A}) = \prod_i \lambda_i(\mathbf{A})\)</span>$</p>
</section>
<section id="orthogonal-matrices">
<h2>Orthogonal matrices<a class="headerlink" href="#orthogonal-matrices" title="Link to this heading">#</a></h2>
<p>A matrix <span class="math notranslate nohighlight">\(\mathbf{Q} \in \mathbb{R}^{n \times n}\)</span> is said to be
<strong>orthogonal</strong> if its columns are pairwise orthonormal. This definition
implies that
$<span class="math notranslate nohighlight">\(\mathbf{Q}^{\!\top\!} \mathbf{Q} = \mathbf{Q}\mathbf{Q}^{\!\top\!} = \mathbf{I}\)</span><span class="math notranslate nohighlight">\(
or equivalently, \)</span>\mathbf{Q}^{!\top!} = \mathbf{Q}^{-1}<span class="math notranslate nohighlight">\(. A nice thing
about orthogonal matrices is that they preserve inner products:
\)</span><span class="math notranslate nohighlight">\((\mathbf{Q}\mathbf{x})^{\!\top\!}(\mathbf{Q}\mathbf{y}) = \mathbf{x}^{\!\top\!} \mathbf{Q}^{\!\top\!} \mathbf{Q}\mathbf{y} = \mathbf{x}^{\!\top\!} \mathbf{I}\mathbf{y} = \mathbf{x}^{\!\top\!}\mathbf{y}\)</span><span class="math notranslate nohighlight">\(
A direct result of this fact is that they also preserve 2-norms:
\)</span><span class="math notranslate nohighlight">\(\|\mathbf{Q}\mathbf{x}\|_2 = \sqrt{(\mathbf{Q}\mathbf{x})^{\!\top\!}(\mathbf{Q}\mathbf{x})} = \sqrt{\mathbf{x}^{\!\top\!}\mathbf{x}} = \|\mathbf{x}\|_2\)</span>$
Therefore multiplication by an orthogonal matrix can be considered as a
transformation that preserves length, but may rotate or reflect the
vector about the origin.</p>
</section>
<section id="symmetric-matrices">
<h2>Symmetric matrices<a class="headerlink" href="#symmetric-matrices" title="Link to this heading">#</a></h2>
<p>A matrix <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{n \times n}\)</span> is said to be
<strong>symmetric</strong> if it is equal to its own transpose
(<span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{A}^{\!\top\!}\)</span>), meaning that <span class="math notranslate nohighlight">\(A_{ij} = A_{ji}\)</span>
for all <span class="math notranslate nohighlight">\((i,j)\)</span>. This definition seems harmless enough but turns out to
have some strong implications. We summarize the most important of these
as</p>
<p><em>Theorem.</em>
(Spectral Theorem) If <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{n \times n}\)</span> is
symmetric, then there exists an orthonormal basis for <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>
consisting of eigenvectors of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>.</p>
<p>The practical application of this theorem is a particular factorization
of symmetric matrices, referred to as the <strong>eigendecomposition</strong> or
<strong>spectral decomposition</strong>. Denote the orthonormal basis of eigenvectors
<span class="math notranslate nohighlight">\(\mathbf{q}_1, \dots, \mathbf{q}_n\)</span> and their eigenvalues
<span class="math notranslate nohighlight">\(\lambda_1, \dots, \lambda_n\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> be an orthogonal matrix
with <span class="math notranslate nohighlight">\(\mathbf{q}_1, \dots, \mathbf{q}_n\)</span> as its columns, and
<span class="math notranslate nohighlight">\(\mathbf{\Lambda} = \operatorname{diag}(\lambda_1, \dots, \lambda_n)\)</span>. Since by
definition <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{q}_i = \lambda_i\mathbf{q}_i\)</span> for every
<span class="math notranslate nohighlight">\(i\)</span>, the following relationship holds:
$<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{Q} = \mathbf{Q}\mathbf{\Lambda}\)</span><span class="math notranslate nohighlight">\(
Right-multiplying
by \)</span>\mathbf{Q}^{!\top!}<span class="math notranslate nohighlight">\(, we arrive at the decomposition
\)</span><span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{\!\top\!}\)</span>$</p>
<section id="rayleigh-quotients">
<h3>Rayleigh quotients<a class="headerlink" href="#rayleigh-quotients" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{n \times n}\)</span> be a symmetric matrix. The
expression <span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}\)</span> is called a
<strong>quadratic form</strong>.</p>
<p>There turns out to be an interesting connection between the quadratic
form of a symmetric matrix and its eigenvalues. This connection is
provided by the <strong>Rayleigh quotient</strong>
$<span class="math notranslate nohighlight">\(R_\mathbf{A}(\mathbf{x}) = \frac{\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}}{\mathbf{x}^{\!\top\!}\mathbf{x}}\)</span>$
The Rayleigh quotient has a couple of important properties which the
reader can (and should!) easily verify from the definition:</p>
<p>(i) <strong>Scale invariance</strong>: for any vector <span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{0}\)</span>
and any scalar <span class="math notranslate nohighlight">\(\alpha \neq 0\)</span>,
<span class="math notranslate nohighlight">\(R_\mathbf{A}(\mathbf{x}) = R_\mathbf{A}(\alpha\mathbf{x})\)</span>.</p>
<p>(ii) If <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> with eigenvalue
<span class="math notranslate nohighlight">\(\lambda\)</span>, then <span class="math notranslate nohighlight">\(R_\mathbf{A}(\mathbf{x}) = \lambda\)</span>.</p>
<p>We can further show that the Rayleigh quotient is bounded by the largest
and smallest eigenvalues of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. But first we will show a
useful special case of the final result.</p>
<p><em>Proposition.</em>
For any <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> such that <span class="math notranslate nohighlight">\(\|\mathbf{x}\|_2 = 1\)</span>,
$<span class="math notranslate nohighlight">\(\lambda_{\min}(\mathbf{A}) \leq \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} \leq \lambda_{\max}(\mathbf{A})\)</span><span class="math notranslate nohighlight">\(
with equality if and only if \)</span>\mathbf{x}$ is a corresponding
eigenvector.</p>
<p><em>Proof.</em> We show only the <span class="math notranslate nohighlight">\(\max\)</span> case because the argument for the
<span class="math notranslate nohighlight">\(\min\)</span> case is entirely analogous.</p>
<p>Since <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is symmetric, we can decompose it as
<span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{\!\top\!}\)</span>. Then use
the change of variable <span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{Q}^{\!\top\!}\mathbf{x}\)</span>,
noting that the relationship between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is
one-to-one and that <span class="math notranslate nohighlight">\(\|\mathbf{y}\|_2 = 1\)</span> since <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> is
orthogonal. Hence
$<span class="math notranslate nohighlight">\(\max_{\|\mathbf{x}\|_2 = 1} \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} = \max_{\|\mathbf{y}\|_2 = 1} \mathbf{y}^{\!\top\!}\mathbf{\Lambda}\mathbf{y} = \max_{y_1^2+\dots+y_n^2=1} \sum_{i=1}^n \lambda_i y_i^2\)</span><span class="math notranslate nohighlight">\(
Written this way, it is clear that \)</span>\mathbf{y}<span class="math notranslate nohighlight">\( maximizes this
expression exactly if and only if it satisfies
\)</span>\sum_{i \in I} y_i^2 = 1<span class="math notranslate nohighlight">\( where
\)</span>I = {i : \lambda_i = \max_{j=1,\dots,n} \lambda_j = \lambda_{\max}(\mathbf{A})}<span class="math notranslate nohighlight">\(
and \)</span>y_j = 0<span class="math notranslate nohighlight">\( for \)</span>j \not\in I<span class="math notranslate nohighlight">\(. That is, \)</span>I<span class="math notranslate nohighlight">\( contains the index or
indices of the largest eigenvalue. In this case, the maximal value of
the expression is
\)</span><span class="math notranslate nohighlight">\(\sum_{i=1}^n \lambda_i y_i^2 = \sum_{i \in I} \lambda_i y_i^2 = \lambda_{\max}(\mathbf{A}) \sum_{i \in I} y_i^2 = \lambda_{\max}(\mathbf{A})\)</span><span class="math notranslate nohighlight">\(
Then writing \)</span>\mathbf{q}_1, \dots, \mathbf{q}_n<span class="math notranslate nohighlight">\( for the columns of
\)</span>\mathbf{Q}<span class="math notranslate nohighlight">\(, we have
\)</span><span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{Q}\mathbf{Q}^{\!\top\!}\mathbf{x} = \mathbf{Q}\mathbf{y} = \sum_{i=1}^n y_i\mathbf{q}_i = \sum_{i \in I} y_i\mathbf{q}_i\)</span>$
where we have used the matrix-vector product identity.</p>
<p>Recall that <span class="math notranslate nohighlight">\(\mathbf{q}_1, \dots, \mathbf{q}_n\)</span> are eigenvectors of
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and form an orthonormal basis for <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. Therefore
by construction, the set <span class="math notranslate nohighlight">\(\{\mathbf{q}_i : i \in I\}\)</span> forms an
orthonormal basis for the eigenspace of <span class="math notranslate nohighlight">\(\lambda_{\max}(\mathbf{A})\)</span>.
Hence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, which is a linear combination of these, lies in that
eigenspace and thus is an eigenvector of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> corresponding to
<span class="math notranslate nohighlight">\(\lambda_{\max}(\mathbf{A})\)</span>.</p>
<p>We have shown that
<span class="math notranslate nohighlight">\(\max_{\|\mathbf{x}\|_2 = 1} \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} = \lambda_{\max}(\mathbf{A})\)</span>,
from which we have the general inequality
<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} \leq \lambda_{\max}(\mathbf{A})\)</span>
for all unit-length <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. ◻</p>
<p>By the scale invariance of the Rayleigh quotient, we immediately have as
a corollary (since
<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} = R_{\mathbf{A}}(\mathbf{x})\)</span>
for unit <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>)</p>
<p><em>Theorem.</em>
(Min-max theorem) For all <span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{0}\)</span>,
$<span class="math notranslate nohighlight">\(\lambda_{\min}(\mathbf{A}) \leq R_\mathbf{A}(\mathbf{x}) \leq \lambda_{\max}(\mathbf{A})\)</span><span class="math notranslate nohighlight">\(
with equality if and only if \)</span>\mathbf{x}$ is a corresponding
eigenvector.</p>
</section>
</section>
<section id="positive-semi-definite-matrices">
<h2>Positive (semi-)definite matrices<a class="headerlink" href="#positive-semi-definite-matrices" title="Link to this heading">#</a></h2>
<p>A symmetric matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is <strong>positive semi-definite</strong> if for all
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} \geq 0\)</span>. Sometimes people
write <span class="math notranslate nohighlight">\(\mathbf{A} \succeq 0\)</span> to indicate that <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is positive
semi-definite.</p>
<p>A symmetric matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is <strong>positive definite</strong> if for all
nonzero <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} &gt; 0\)</span>. Sometimes people write
<span class="math notranslate nohighlight">\(\mathbf{A} \succ 0\)</span> to indicate that <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is positive definite.
Note that positive definiteness is a strictly stronger property than
positive semi-definiteness, in the sense that every positive definite
matrix is positive semi-definite but not vice-versa.</p>
<p>These properties are related to eigenvalues in the following way.</p>
<p><em>Proposition.</em>
A symmetric matrix is positive semi-definite if and only if all of its
eigenvalues are nonnegative, and positive definite if and only if all of
its eigenvalues are positive.</p>
<p><em>Proof.</em> Suppose <span class="math notranslate nohighlight">\(A\)</span> is positive semi-definite, and let <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> be
an eigenvector of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>. Then
$<span class="math notranslate nohighlight">\(0 \leq \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} = \mathbf{x}^{\!\top\!}(\lambda\mathbf{x}) = \lambda\mathbf{x}^{\!\top\!}\mathbf{x} = \lambda\|\mathbf{x}\|_2^2\)</span><span class="math notranslate nohighlight">\(
Since \)</span>\mathbf{x} \neq \mathbf{0}<span class="math notranslate nohighlight">\( (by the assumption that it is an
eigenvector), we have \)</span>|\mathbf{x}|_2^2 &gt; 0<span class="math notranslate nohighlight">\(, so we can divide both
sides by \)</span>|\mathbf{x}|_2^2<span class="math notranslate nohighlight">\( to arrive at \)</span>\lambda \geq 0<span class="math notranslate nohighlight">\(. If
\)</span>\mathbf{A}<span class="math notranslate nohighlight">\( is positive definite, the inequality above holds strictly,
so \)</span>\lambda &gt; 0$. This proves one direction.</p>
<p>To simplify the proof of the other direction, we will use the machinery
of Rayleigh quotients. Suppose that <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is symmetric and all
its eigenvalues are nonnegative. Then for all
<span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{0}\)</span>,
$<span class="math notranslate nohighlight">\(0 \leq \lambda_{\min}(\mathbf{A}) \leq R_\mathbf{A}(\mathbf{x})\)</span><span class="math notranslate nohighlight">\(
Since \)</span>\mathbf{x}^{!\top!}\mathbf{A}\mathbf{x}<span class="math notranslate nohighlight">\( matches
\)</span>R_\mathbf{A}(\mathbf{x})<span class="math notranslate nohighlight">\( in sign, we conclude that \)</span>\mathbf{A}<span class="math notranslate nohighlight">\( is
positive semi-definite. If the eigenvalues of \)</span>\mathbf{A}<span class="math notranslate nohighlight">\( are all
strictly positive, then \)</span>0 &lt; \lambda_{\min}(\mathbf{A})<span class="math notranslate nohighlight">\(, whence it
follows that \)</span>\mathbf{A}$ is positive definite. ◻</p>
<p>As an example of how these matrices arise, consider</p>
<p><em>Proposition.</em>
Suppose <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span>. Then
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\mathbf{A}\)</span> is positive semi-definite. If
<span class="math notranslate nohighlight">\(\operatorname{null}(\mathbf{A}) = \{\mathbf{0}\}\)</span>, then
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\mathbf{A}\)</span> is positive definite.</p>
<p><em>Proof.</em> For any <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span>,
$<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!} (\mathbf{A}^{\!\top\!}\mathbf{A})\mathbf{x} = (\mathbf{A}\mathbf{x})^{\!\top\!}(\mathbf{A}\mathbf{x}) = \|\mathbf{A}\mathbf{x}\|_2^2 \geq 0\)</span><span class="math notranslate nohighlight">\(
so \)</span>\mathbf{A}^{!\top!}\mathbf{A}$ is positive semi-definite.</p>
<p>Note that <span class="math notranslate nohighlight">\(\|\mathbf{A}\mathbf{x}\|_2^2 = 0\)</span> implies
<span class="math notranslate nohighlight">\(\|\mathbf{A}\mathbf{x}\|_2 = 0\)</span>, which in turn implies
<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x} = \mathbf{0}\)</span> (recall that this is a property of
norms). If <span class="math notranslate nohighlight">\(\operatorname{null}(\mathbf{A}) = \{\mathbf{0}\}\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x} = \mathbf{0}\)</span> implies <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{0}\)</span>,
so
<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!} (\mathbf{A}^{\!\top\!}\mathbf{A})\mathbf{x} = 0\)</span>
if and only if <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{0}\)</span>, and thus
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\mathbf{A}\)</span> is positive definite. ◻</p>
<p>Positive definite matrices are invertible (since their eigenvalues are
nonzero), whereas positive semi-definite matrices might not be. However,
if you already have a positive semi-definite matrix, it is possible to
perturb its diagonal slightly to produce a positive definite matrix.</p>
<p><em>Proposition.</em>
If <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is positive semi-definite and <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>, then
<span class="math notranslate nohighlight">\(\mathbf{A} + \epsilon\mathbf{I}\)</span> is positive definite.</p>
<p><em>Proof.</em> Assuming <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is positive semi-definite and
<span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>, we have for any <span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{0}\)</span> that
$<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}(\mathbf{A}+\epsilon\mathbf{I})\mathbf{x} = \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} + \epsilon\mathbf{x}^{\!\top\!}\mathbf{I}\mathbf{x} = \underbrace{\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}}_{\geq 0} + \underbrace{\epsilon\|\mathbf{x}\|_2^2}_{&gt; 0} &gt; 0\)</span>$
as claimed. ◻</p>
<p>An obvious but frequently useful consequence of the two propositions we
have just shown is that
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\mathbf{A} + \epsilon\mathbf{I}\)</span> is positive
definite (and in particular, invertible) for <em>any</em> matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>
and any <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>.</p>
<section id="the-geometry-of-positive-definite-quadratic-forms">
<h3>The geometry of positive definite quadratic forms<a class="headerlink" href="#the-geometry-of-positive-definite-quadratic-forms" title="Link to this heading">#</a></h3>
<p>A useful way to understand quadratic forms is by the geometry of their
level sets. A <strong>level set</strong> or <strong>isocontour</strong> of a function is the set
of all inputs such that the function applied to those inputs yields a
given output. Mathematically, the <span class="math notranslate nohighlight">\(c\)</span>-isocontour of <span class="math notranslate nohighlight">\(f\)</span> is
<span class="math notranslate nohighlight">\(\{\mathbf{x} \in \operatorname{dom} f : f(\mathbf{x}) = c\}\)</span>.</p>
<p>Let us consider the special case
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}\)</span> where
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is a positive definite matrix. Since <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is
positive definite, it has a unique matrix square root
<span class="math notranslate nohighlight">\(\mathbf{A}^{\frac{1}{2}} = \mathbf{Q}\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{Q}^{\!\top\!}\)</span>,
where <span class="math notranslate nohighlight">\(\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{\!\top\!}\)</span> is the
eigendecomposition of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{\Lambda}^{\frac{1}{2}} = \operatorname{diag}(\sqrt{\lambda_1}, \dots \sqrt{\lambda_n})\)</span>.
It is easy to see that this matrix <span class="math notranslate nohighlight">\(\mathbf{A}^{\frac{1}{2}}\)</span> is
positive definite (consider its eigenvalues) and satisfies
<span class="math notranslate nohighlight">\(\mathbf{A}^{\frac{1}{2}}\mathbf{A}^{\frac{1}{2}} = \mathbf{A}\)</span>. Fixing
a value <span class="math notranslate nohighlight">\(c \geq 0\)</span>, the <span class="math notranslate nohighlight">\(c\)</span>-isocontour of <span class="math notranslate nohighlight">\(f\)</span> is the set of
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span> such that
$<span class="math notranslate nohighlight">\(c = \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} = \mathbf{x}^{\!\top\!}\mathbf{A}^{\frac{1}{2}}\mathbf{A}^{\frac{1}{2}}\mathbf{x} = \|\mathbf{A}^{\frac{1}{2}}\mathbf{x}\|_2^2\)</span><span class="math notranslate nohighlight">\(
where we have used the symmetry of \)</span>\mathbf{A}^{\frac{1}{2}}<span class="math notranslate nohighlight">\(. Making
the change of variable
\)</span>\mathbf{z} = \mathbf{A}^{\frac{1}{2}}\mathbf{x}<span class="math notranslate nohighlight">\(, we have the condition
\)</span>|\mathbf{z}|_2 = \sqrt{c}<span class="math notranslate nohighlight">\(. That is, the values \)</span>\mathbf{z}<span class="math notranslate nohighlight">\( lie on a
sphere of radius \)</span>\sqrt{c}<span class="math notranslate nohighlight">\(. These can be parameterized as
\)</span>\mathbf{z} = \sqrt{c}\hat{\mathbf{z}}<span class="math notranslate nohighlight">\( where \)</span>\hat{\mathbf{z}}<span class="math notranslate nohighlight">\( has
\)</span>|\hat{\mathbf{z}}|_2 = 1<span class="math notranslate nohighlight">\(. Then since
\)</span>\mathbf{A}^{-\frac{1}{2}} = \mathbf{Q}\mathbf{\Lambda}^{-\frac{1}{2}}\mathbf{Q}^{!\top!}<span class="math notranslate nohighlight">\(,
we have
\)</span><span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{A}^{-\frac{1}{2}}\mathbf{z} = \mathbf{Q}\mathbf{\Lambda}^{-\frac{1}{2}}\mathbf{Q}^{\!\top\!}\sqrt{c}\hat{\mathbf{z}} = \sqrt{c}\mathbf{Q}\mathbf{\Lambda}^{-\frac{1}{2}}\tilde{\mathbf{z}}\)</span><span class="math notranslate nohighlight">\(
where \)</span>\tilde{\mathbf{z}} = \mathbf{Q}^{!\top!}\hat{\mathbf{z}}<span class="math notranslate nohighlight">\( also
satisfies \)</span>|\tilde{\mathbf{z}}|_2 = 1<span class="math notranslate nohighlight">\( since \)</span>\mathbf{Q}<span class="math notranslate nohighlight">\( is
orthogonal. Using this parameterization, we see that the solution set
\)</span>{\mathbf{x} \in \mathbb{R}^n : f(\mathbf{x}) = c}<span class="math notranslate nohighlight">\( is the image of
the unit sphere
\)</span>{\tilde{\mathbf{z}} \in \mathbb{R}^n : |\tilde{\mathbf{z}}|_2 = 1}<span class="math notranslate nohighlight">\(
under the invertible linear map
\)</span>\mathbf{x} = \sqrt{c}\mathbf{Q}\mathbf{\Lambda}^{-\frac{1}{2}}\tilde{\mathbf{z}}$.</p>
<p>What we have gained with all these manipulations is a clear algebraic
understanding of the <span class="math notranslate nohighlight">\(c\)</span>-isocontour of <span class="math notranslate nohighlight">\(f\)</span> in terms of a sequence of
linear transformations applied to a well-understood set. We begin with
the unit sphere, then scale every axis <span class="math notranslate nohighlight">\(i\)</span> by
<span class="math notranslate nohighlight">\(\lambda_i^{-\frac{1}{2}}\)</span>, resulting in an axis-aligned ellipsoid.
Observe that the axis lengths of the ellipsoid are proportional to the
inverse square roots of the eigenvalues of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. Hence larger
eigenvalues correspond to shorter axis lengths, and vice-versa.</p>
<p>Then this axis-aligned ellipsoid undergoes a rigid transformation (i.e.
one that preserves length and angles, such as a rotation/reflection)
given by <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>. The result of this transformation is that the
axes of the ellipse are no longer along the coordinate axes in general,
but rather along the directions given by the corresponding eigenvectors.
To see this, consider the unit vector <span class="math notranslate nohighlight">\(\mathbf{e}_i \in \mathbb{R}^n\)</span>
that has <span class="math notranslate nohighlight">\([\mathbf{e}_i]_j = \delta_{ij}\)</span>. In the pre-transformed space,
this vector points along the axis with length proportional to
<span class="math notranslate nohighlight">\(\lambda_i^{-\frac{1}{2}}\)</span>. But after applying the rigid transformation
<span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>, the resulting vector points in the direction of the
corresponding eigenvector <span class="math notranslate nohighlight">\(\mathbf{q}_i\)</span>, since
$<span class="math notranslate nohighlight">\(\mathbf{Q}\mathbf{e}_i = \sum_{j=1}^n [\mathbf{e}_i]_j\mathbf{q}_j = \mathbf{q}_i\)</span>$
where we have used the matrix-vector product identity from earlier.</p>
<p>In summary: the isocontours of
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}\)</span> are
ellipsoids such that the axes point in the directions of the
eigenvectors of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, and the radii of these axes are
proportional to the inverse square roots of the corresponding
eigenvalues.</p>
</section>
</section>
<section id="singular-value-decomposition">
<h2>Singular value decomposition<a class="headerlink" href="#singular-value-decomposition" title="Link to this heading">#</a></h2>
<p>Singular value decomposition (SVD) is a widely applicable tool in linear
algebra. Its strength stems partially from the fact that <em>every matrix</em>
<span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> has an SVD (even non-square
matrices)! The decomposition goes as follows:
$<span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!}\)</span><span class="math notranslate nohighlight">\(
where
\)</span>\mathbf{U} \in \mathbb{R}^{m \times m}<span class="math notranslate nohighlight">\( and
\)</span>\mathbf{V} \in \mathbb{R}^{n \times n}<span class="math notranslate nohighlight">\( are orthogonal matrices and
\)</span>\mathbf{\Sigma} \in \mathbb{R}^{m \times n}<span class="math notranslate nohighlight">\( is a diagonal matrix with
the **singular values** of \)</span>\mathbf{A}<span class="math notranslate nohighlight">\( (denoted \)</span>\sigma_i$) on its
diagonal.</p>
<p>By convention, the singular values are given in non-increasing order,
i.e.
$<span class="math notranslate nohighlight">\(\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_{\min(m,n)} \geq 0\)</span><span class="math notranslate nohighlight">\(
Only the first \)</span>r<span class="math notranslate nohighlight">\( singular values are nonzero, where \)</span>r<span class="math notranslate nohighlight">\( is the rank of
\)</span>\mathbf{A}$.</p>
<p>Observe that the SVD factors provide eigendecompositions for
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^{\!\top\!}\)</span>:
$<span class="math notranslate nohighlight">\(\begin{aligned}
\mathbf{A}^{\!\top\!}\mathbf{A} &amp;= (\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!})^{\!\top\!}\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!} = \mathbf{V}\mathbf{\Sigma}^{\!\top\!}\mathbf{U}^{\!\top\!}\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!} = \mathbf{V}\mathbf{\Sigma}^{\!\top\!}\mathbf{\Sigma}\mathbf{V}^{\!\top\!} \\
\mathbf{A}\mathbf{A}^{\!\top\!} &amp;= \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!}(\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!})^{\!\top\!} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!}\mathbf{V}\mathbf{\Sigma}^{\!\top\!}\mathbf{U}^{\!\top\!} = \mathbf{U}\mathbf{\Sigma}\mathbf{\Sigma}^{\!\top\!}\mathbf{U}^{\!\top\!}
\end{aligned}\)</span><span class="math notranslate nohighlight">\(
It follows immediately that the columns of \)</span>\mathbf{V}<span class="math notranslate nohighlight">\(
(the **right-singular vectors** of \)</span>\mathbf{A}<span class="math notranslate nohighlight">\() are eigenvectors of
\)</span>\mathbf{A}^{!\top!}\mathbf{A}<span class="math notranslate nohighlight">\(, and the columns of \)</span>\mathbf{U}<span class="math notranslate nohighlight">\( (the
**left-singular vectors** of \)</span>\mathbf{A}<span class="math notranslate nohighlight">\() are eigenvectors of
\)</span>\mathbf{A}\mathbf{A}^{!\top!}$.</p>
<p>The matrices <span class="math notranslate nohighlight">\(\mathbf{\Sigma}^{\!\top\!}\mathbf{\Sigma}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{\Sigma}\mathbf{\Sigma}^{\!\top\!}\)</span> are not necessarily the same
size, but both are diagonal with the squared singular values
<span class="math notranslate nohighlight">\(\sigma_i^2\)</span> on the diagonal (plus possibly some zeros). Thus the
singular values of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> are the square roots of the eigenvalues
of <span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\mathbf{A}\)</span> (or equivalently, of
<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^{\!\top\!}\)</span>)[^5].</p>
</section>
<section id="some-useful-matrix-identities">
<h2>Some useful matrix identities<a class="headerlink" href="#some-useful-matrix-identities" title="Link to this heading">#</a></h2>
<section id="matrix-vector-product-as-linear-combination-of-matrix-columns">
<h3>Matrix-vector product as linear combination of matrix columns<a class="headerlink" href="#matrix-vector-product-as-linear-combination-of-matrix-columns" title="Link to this heading">#</a></h3>
<p><em>Proposition.</em>
Let <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span> be a vector and
<span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> a matrix with columns
<span class="math notranslate nohighlight">\(\mathbf{a}_1, \dots, \mathbf{a}_n\)</span>. Then
$<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x} = \sum_{i=1}^n x_i\mathbf{a}_i\)</span>$</p>
<p>This identity is extremely useful in understanding linear operators in
terms of their matrices’ columns. The proof is very simple (consider
each element of <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x}\)</span> individually and expand by
definitions) but it is a good exercise to convince yourself.</p>
</section>
<section id="sum-of-outer-products-as-matrix-matrix-product">
<h3>Sum of outer products as matrix-matrix product<a class="headerlink" href="#sum-of-outer-products-as-matrix-matrix-product" title="Link to this heading">#</a></h3>
<p>An <strong>outer product</strong> is an expression of the form
<span class="math notranslate nohighlight">\(\mathbf{a}\mathbf{b}^{\!\top\!}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{a} \in \mathbb{R}^m\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span>. By inspection it is not hard to see
that such an expression yields an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix such that
$<span class="math notranslate nohighlight">\([\mathbf{a}\mathbf{b}^{\!\top\!}]_{ij} = a_ib_j\)</span>$
It is not
immediately obvious, but the sum of outer products is actually
equivalent to an appropriate matrix-matrix product! We formalize this
statement as</p>
<p><em>Proposition.</em>
Let <span class="math notranslate nohighlight">\(\mathbf{a}_1, \dots, \mathbf{a}_k \in \mathbb{R}^m\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{b}_1, \dots, \mathbf{b}_k \in \mathbb{R}^n\)</span>. Then
$<span class="math notranslate nohighlight">\(\sum_{\ell=1}^k \mathbf{a}_\ell\mathbf{b}_\ell^{\!\top\!} = \mathbf{A}\mathbf{B}^{\!\top\!}\)</span><span class="math notranslate nohighlight">\(
where
\)</span><span class="math notranslate nohighlight">\(\mathbf{A} = \begin{bmatrix}\mathbf{a}_1 &amp; \cdots &amp; \mathbf{a}_k\end{bmatrix}, \hspace{0.5cm} \mathbf{B} = \begin{bmatrix}\mathbf{b}_1 &amp; \cdots &amp; \mathbf{b}_k\end{bmatrix}\)</span>$</p>
<p><em>Proof.</em> For each <span class="math notranslate nohighlight">\((i,j)\)</span>, we have
$<span class="math notranslate nohighlight">\(\left[\sum_{\ell=1}^k \mathbf{a}_\ell\mathbf{b}_\ell^{\!\top\!}\right]_{ij} = \sum_{\ell=1}^k [\mathbf{a}_\ell\mathbf{b}_\ell^{\!\top\!}]_{ij} = \sum_{\ell=1}^k [\mathbf{a}_\ell]_i[\mathbf{b}_\ell]_j = \sum_{\ell=1}^k A_{i\ell}B_{j\ell}\)</span><span class="math notranslate nohighlight">\(
This last expression should be recognized as an inner product between
the \)</span>i<span class="math notranslate nohighlight">\(th row of \)</span>\mathbf{A}<span class="math notranslate nohighlight">\( and the \)</span>j<span class="math notranslate nohighlight">\(th row of \)</span>\mathbf{B}<span class="math notranslate nohighlight">\(, or
equivalently the \)</span>j<span class="math notranslate nohighlight">\(th column of \)</span>\mathbf{B}^{!\top!}<span class="math notranslate nohighlight">\(. Hence by the
definition of matrix multiplication, it is equal to
\)</span>[\mathbf{A}\mathbf{B}^{!\top!}]_{ij}$. ◻</p>
</section>
<section id="quadratic-forms">
<h3>Quadratic forms<a class="headerlink" href="#quadratic-forms" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{n \times n}\)</span> be a symmetric matrix, and
recall that the expression <span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}\)</span>
is called a quadratic form of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. It is in some cases helpful
to rewrite the quadratic form in terms of the individual elements that
make up <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>:
$<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} = \sum_{i=1}^n\sum_{j=1}^n A_{ij}x_ix_j\)</span>$
This identity is valid for any square matrix (need not be symmetric),
although quadratic forms are usually only discussed in the context of
symmetric matrices.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_linear_algebra"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_preface/intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Notation</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter_calculus/calculus.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Calculus and Optimization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-spaces">Vector spaces</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#euclidean-space">Euclidean space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subspaces">Subspaces</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metric-spaces">Metric spaces</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normed-spaces">Normed spaces</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inner-product-spaces">Inner product spaces</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pythagorean-theorem">Pythagorean Theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cauchy-schwarz-inequality">Cauchy-Schwarz inequality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transposition">Transposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenthings">Eigenthings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#trace">Trace</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determinant">Determinant</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#orthogonal-matrices">Orthogonal matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#symmetric-matrices">Symmetric matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rayleigh-quotients">Rayleigh quotients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positive-semi-definite-matrices">Positive (semi-)definite matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-geometry-of-positive-definite-quadratic-forms">The geometry of positive definite quadratic forms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#singular-value-decomposition">Singular value decomposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-useful-matrix-identities">Some useful matrix identities</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-vector-product-as-linear-combination-of-matrix-columns">Matrix-vector product as linear combination of matrix columns</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sum-of-outer-products-as-matrix-matrix-product">Sum of outer products as matrix-matrix product</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-forms">Quadratic forms</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christoph Lippert
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>