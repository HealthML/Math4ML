
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Linear Algebra &#8212; Mathematics for Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_linear_algebra/linear_algebra';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Calculus and Optimization" href="../chapter_calculus/calculus.html" />
    <link rel="prev" title="Notation" href="../chapter_preface/intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/hpi-logo-colored.svg" class="logo__image only-light" alt="Mathematics for Machine Learning - Home"/>
    <img src="../_static/hpi-logo-colored.svg" class="logo__image only-dark pst-js-only" alt="Mathematics for Machine Learning - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Mathematics for Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/intro.html">Notation</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Linear Algebra</a></li>


<li class="toctree-l1"><a class="reference internal" href="../chapter_calculus/calculus.html">Calculus and Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_probability/probability.html">Probability</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fchapter_linear_algebra/linear_algebra.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_linear_algebra/linear_algebra.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Algebra</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Linear Algebra</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-spaces">Vector spaces</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#euclidean-space">Euclidean space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subspaces">Subspaces</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metric-spaces">Metric spaces</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normed-spaces">Normed spaces</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inner-product-spaces">Inner product spaces</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pythagorean-theorem">Pythagorean Theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cauchy-schwarz-inequality">Cauchy-Schwarz inequality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transposition">Transposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenthings">Eigenthings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#trace">Trace</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determinant">Determinant</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#orthogonal-matrices">Orthogonal matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#symmetric-matrices">Symmetric matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rayleigh-quotients">Rayleigh quotients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positive-semi-definite-matrices">Positive (semi-)definite matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-geometry-of-positive-definite-quadratic-forms">The geometry of positive definite quadratic forms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#singular-value-decomposition">Singular value decomposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-useful-matrix-identities">Some useful matrix identities</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-vector-product-as-linear-combination-of-matrix-columns">Matrix-vector product as linear combination of matrix columns</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sum-of-outer-products-as-matrix-matrix-product">Sum of outer products as matrix-matrix product</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-forms">Quadratic forms</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#calculus-and-optimization">Calculus and Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extrema">Extrema</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients">Gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-jacobian">The Jacobian</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hessian">The Hessian</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-calculus">Matrix calculus</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chain-rule">The chain rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#taylor-s-theorem">Taylor’s theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditions-for-local-minima">Conditions for local minima</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity">Convexity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-sets">Convex sets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basics-of-convex-functions">Basics of convex functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#consequences-of-convexity">Consequences of convexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#showing-that-a-function-is-convex">Showing that a function is convex</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#orthogonal-projections">Orthogonal projections</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#probability">Probability</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basics">Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability">Conditional probability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule">Chain rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-rule">Bayes’ rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variables">Random variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cumulative-distribution-function">The cumulative distribution function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-random-variables">Discrete random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-random-variables">Continuous random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-kinds-of-random-variables">Other kinds of random variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distributions">Joint distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-of-random-variables">Independence of random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-distributions">Marginal distributions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#great-expectations">Great Expectations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-expected-value">Properties of expected value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-variance">Properties of variance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-deviation">Standard deviation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation">Correlation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-vectors">Random vectors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-parameters">Estimation of Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum likelihood estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-estimation">Maximum a posteriori estimation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-distribution">The Gaussian distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-geometry-of-multivariate-gaussians">The geometry of multivariate Gaussians</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-algebra">
<h1>Linear Algebra<a class="headerlink" href="#linear-algebra" title="Link to this heading">#</a></h1>
<p>In this section we present important classes of spaces in which our data
will live and our operations will take place: vector spaces, metric
spaces, normed spaces, and inner product spaces. Generally speaking,
these are defined in such a way as to capture one or more important
properties of Euclidean space but in a more general way.</p>
<section id="vector-spaces">
<h2>Vector spaces<a class="headerlink" href="#vector-spaces" title="Link to this heading">#</a></h2>
<p><strong>Vector spaces</strong> are the basic setting in which linear algebra happens.
A vector space <span class="math notranslate nohighlight">\(V\)</span> is a set (the elements of which are called
<strong>vectors</strong>) on which two operations are defined: vectors can be added
together, and vectors can be multiplied by real numbers<a class="footnote-reference brackets" href="#id15" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> called
<strong>scalars</strong>. <span class="math notranslate nohighlight">\(V\)</span> must satisfy</p>
<p>(i) There exists an additive identity (written <span class="math notranslate nohighlight">\(\mathbf{0}\)</span>) in <span class="math notranslate nohighlight">\(V\)</span> such
that <span class="math notranslate nohighlight">\(\mathbf{x}+\mathbf{0} = \mathbf{x}\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x} \in V\)</span></p>
<p>(ii) For each <span class="math notranslate nohighlight">\(\mathbf{x} \in V\)</span>, there exists an additive inverse
(written <span class="math notranslate nohighlight">\(\mathbf{-x}\)</span>) such that
<span class="math notranslate nohighlight">\(\mathbf{x}+(\mathbf{-x}) = \mathbf{0}\)</span></p>
<p>(iii) There exists a multiplicative identity (written <span class="math notranslate nohighlight">\(1\)</span>) in
<span class="math notranslate nohighlight">\(\mathbb{R}\)</span> such that <span class="math notranslate nohighlight">\(1\mathbf{x} = \mathbf{x}\)</span> for all
<span class="math notranslate nohighlight">\(\mathbf{x} \in V\)</span></p>
<p>(iv) Commutativity: <span class="math notranslate nohighlight">\(\mathbf{x}+\mathbf{y} = \mathbf{y}+\mathbf{x}\)</span> for
all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in V\)</span></p>
<p>(v) Associativity:
<span class="math notranslate nohighlight">\((\mathbf{x}+\mathbf{y})+\mathbf{z} = \mathbf{x}+(\mathbf{y}+\mathbf{z})\)</span>
and <span class="math notranslate nohighlight">\(\alpha(\beta\mathbf{x}) = (\alpha\beta)\mathbf{x}\)</span> for all
<span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y}, \mathbf{z} \in V\)</span> and
<span class="math notranslate nohighlight">\(\alpha, \beta \in \mathbb{R}\)</span></p>
<p>(vi) Distributivity:
<span class="math notranslate nohighlight">\(\alpha(\mathbf{x}+\mathbf{y}) = \alpha\mathbf{x} + \alpha\mathbf{y}\)</span>
and <span class="math notranslate nohighlight">\((\alpha+\beta)\mathbf{x} = \alpha\mathbf{x} + \beta\mathbf{x}\)</span>
for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in V\)</span> and
<span class="math notranslate nohighlight">\(\alpha, \beta \in \mathbb{R}\)</span></p>
<section id="euclidean-space">
<h3>Euclidean space<a class="headerlink" href="#euclidean-space" title="Link to this heading">#</a></h3>
<p>The quintessential vector space is <strong>Euclidean space</strong>, which we denote
<span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. The vectors in this space consist of <span class="math notranslate nohighlight">\(n\)</span>-tuples of real
numbers: $<span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, x_2, \dots, x_n)\)</span><span class="math notranslate nohighlight">\( For our purposes, it
will be useful to think of them as \)</span>n \times 1<span class="math notranslate nohighlight">\( matrices, or **column
vectors**:
\)</span><span class="math notranslate nohighlight">\(\mathbf{x} = \begin{bmatrix}x_1 \\ x_2 \\ \vdots \\ x_n\end{bmatrix}\)</span><span class="math notranslate nohighlight">\(
Addition and scalar multiplication are defined component-wise on vectors
in \)</span>\mathbb{R}^n<span class="math notranslate nohighlight">\(:
\)</span><span class="math notranslate nohighlight">\(\mathbf{x} + \mathbf{y} = \begin{bmatrix}x_1 + y_1 \\ \vdots \\ x_n + y_n\end{bmatrix}, \hspace{0.5cm} \alpha\mathbf{x} = \begin{bmatrix}\alpha x_1 \\ \vdots \\ \alpha x_n\end{bmatrix}\)</span><span class="math notranslate nohighlight">\(
Euclidean space is used to mathematically represent physical space, with
notions such as distance, length, and angles. Although it becomes hard
to visualize for \)</span>n &gt; 3<span class="math notranslate nohighlight">\(, these concepts generalize mathematically in
obvious ways. Even when you're working in more general settings than
\)</span>\mathbb{R}^n$, it is often useful to visualize vector addition and
scalar multiplication in terms of 2D vectors in the plane or 3D vectors
in space.</p>
</section>
<section id="subspaces">
<h3>Subspaces<a class="headerlink" href="#subspaces" title="Link to this heading">#</a></h3>
<p>Vector spaces can contain other vector spaces. If <span class="math notranslate nohighlight">\(V\)</span> is a vector space,
then <span class="math notranslate nohighlight">\(S \subseteq V\)</span> is said to be a <strong>subspace</strong> of <span class="math notranslate nohighlight">\(V\)</span> if</p>
<p>(i) <span class="math notranslate nohighlight">\(\mathbf{0} \in S\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\(S\)</span> is closed under addition: <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in S\)</span>
implies <span class="math notranslate nohighlight">\(\mathbf{x}+\mathbf{y} \in S\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\(S\)</span> is closed under scalar multiplication:
<span class="math notranslate nohighlight">\(\mathbf{x} \in S, \alpha \in \mathbb{R}\)</span> implies
<span class="math notranslate nohighlight">\(\alpha\mathbf{x} \in S\)</span></p>
<p>Note that <span class="math notranslate nohighlight">\(V\)</span> is always a subspace of <span class="math notranslate nohighlight">\(V\)</span>, as is the trivial vector
space which contains only <span class="math notranslate nohighlight">\(\mathbf{0}\)</span>.</p>
<p>As a concrete example, a line passing through the origin is a subspace
of Euclidean space.</p>
<p>Some of the most important subspaces are those induced by linear maps.
If <span class="math notranslate nohighlight">\(T : V \to W\)</span> is a linear map, we define the <strong>nullspace</strong><a class="footnote-reference brackets" href="#id16" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> of <span class="math notranslate nohighlight">\(T\)</span>
as $<span class="math notranslate nohighlight">\(\Null(T) = \{\mathbf{x} \in V \mid T\mathbf{x} = \mathbf{0}\}\)</span><span class="math notranslate nohighlight">\( and
the **range** (or the **columnspace** if we are considering the matrix
form) of \)</span>T<span class="math notranslate nohighlight">\( as
\)</span><span class="math notranslate nohighlight">\(\range(T) = \{\mathbf{y} \in W \mid \text{\)</span>\exists \mathbf{x} \in V<span class="math notranslate nohighlight">\( such that \)</span>T\mathbf{x} = \mathbf{y}<span class="math notranslate nohighlight">\(}\}\)</span>$
It is a good exercise to verify that the nullspace and range of a linear
map are always subspaces of its domain and codomain, respectively.</p>
</section>
</section>
<section id="metric-spaces">
<h2>Metric spaces<a class="headerlink" href="#metric-spaces" title="Link to this heading">#</a></h2>
<p>Metrics generalize the notion of distance from Euclidean space (although
metric spaces need not be vector spaces).</p>
<p>A <strong>metric</strong> on a set <span class="math notranslate nohighlight">\(S\)</span> is a function <span class="math notranslate nohighlight">\(d : S \times S \to \mathbb{R}\)</span>
that satisfies</p>
<p>(i) <span class="math notranslate nohighlight">\(d(x,y) \geq 0\)</span>, with equality if and only if <span class="math notranslate nohighlight">\(x = y\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\(d(x,y) = d(y,x)\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\(d(x,z) \leq d(x,y) + d(y,z)\)</span> (the so-called <strong>triangle
inequality</strong>)</p>
<p>for all <span class="math notranslate nohighlight">\(x, y, z \in S\)</span>.</p>
<p>A key motivation for metrics is that they allow limits to be defined for
mathematical objects other than real numbers. We say that a sequence
<span class="math notranslate nohighlight">\(\{x_n\} \subseteq S\)</span> converges to the limit <span class="math notranslate nohighlight">\(x\)</span> if for any
<span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>, there exists <span class="math notranslate nohighlight">\(N \in \mathbb{N}\)</span> such that
<span class="math notranslate nohighlight">\(d(x_n, x) &lt; \epsilon\)</span> for all <span class="math notranslate nohighlight">\(n \geq N\)</span>. Note that the definition for
limits of sequences of real numbers, which you have likely seen in a
calculus class, is a special case of this definition when using the
metric <span class="math notranslate nohighlight">\(d(x, y) = |x-y|\)</span>.</p>
</section>
<section id="normed-spaces">
<h2>Normed spaces<a class="headerlink" href="#normed-spaces" title="Link to this heading">#</a></h2>
<p>Norms generalize the notion of length from Euclidean space.</p>
<p>A <strong>norm</strong> on a real vector space <span class="math notranslate nohighlight">\(V\)</span> is a function
<span class="math notranslate nohighlight">\(\|\cdot\| : V \to \mathbb{R}\)</span> that satisfies</p>
<p>(i) <span class="math notranslate nohighlight">\(\|\mathbf{x}\| \geq 0\)</span>, with equality if and only if
<span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{0}\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\(\|\alpha\mathbf{x}\| = |\alpha|\|\mathbf{x}\|\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\(\|\mathbf{x}+\mathbf{y}\| \leq \|\mathbf{x}\| + \|\mathbf{y}\|\)</span>
(the <strong>triangle inequality</strong> again)</p>
<p>for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in V\)</span> and all <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span>.
A vector space endowed with a norm is called a <strong>normed vector space</strong>,
or simply a <strong>normed space</strong>.</p>
<p>Note that any norm on <span class="math notranslate nohighlight">\(V\)</span> induces a distance metric on <span class="math notranslate nohighlight">\(V\)</span>:
$<span class="math notranslate nohighlight">\(d(\mathbf{x}, \mathbf{y}) = \|\mathbf{x}-\mathbf{y}\|\)</span>$ One can verify
that the axioms for metrics are satisfied under this definition and
follow directly from the axioms for norms. Therefore any normed space is
also a metric space.<a class="footnote-reference brackets" href="#id17" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a></p>
<p>We will typically only be concerned with a few specific norms on
<span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>: $<span class="math notranslate nohighlight">\(\begin{aligned}
\|\mathbf{x}\|_1 &amp;= \sum_{i=1}^n |x_i| \\
\|\mathbf{x}\|_2 &amp;= \sqrt{\sum_{i=1}^n x_i^2} \\
\|\mathbf{x}\|_p &amp;= \left(\sum_{i=1}^n |x_i|^p\right)^\frac{1}{p} \hspace{0.5cm}\hspace{0.5cm} (p \geq 1) \\
\|\mathbf{x}\|_\infty &amp;= \max_{1 \leq i \leq n} |x_i|
\end{aligned}\)</span><span class="math notranslate nohighlight">\( Note that the 1- and 2-norms are special cases of the
\)</span>p<span class="math notranslate nohighlight">\(-norm, and the \)</span>\infty<span class="math notranslate nohighlight">\(-norm is the limit of the \)</span>p<span class="math notranslate nohighlight">\(-norm as \)</span>p<span class="math notranslate nohighlight">\(
tends to infinity. We require \)</span>p \geq 1<span class="math notranslate nohighlight">\( for the general definition of
the \)</span>p<span class="math notranslate nohighlight">\(-norm because the triangle inequality fails to hold if \)</span>p &lt; 1$.
(Try to find a counterexample!)</p>
<p>Here’s a fun fact: for any given finite-dimensional vector space <span class="math notranslate nohighlight">\(V\)</span>,
all norms on <span class="math notranslate nohighlight">\(V\)</span> are equivalent in the sense that for two norms
<span class="math notranslate nohighlight">\(\|\cdot\|_A, \|\cdot\|_B\)</span>, there exist constants <span class="math notranslate nohighlight">\(\alpha, \beta &gt; 0\)</span>
such that
$<span class="math notranslate nohighlight">\(\alpha\|\mathbf{x}\|_A \leq \|\mathbf{x}\|_B \leq \beta\|\mathbf{x}\|_A\)</span><span class="math notranslate nohighlight">\(
for all \)</span>\mathbf{x} \in V$. Therefore convergence in one norm implies
convergence in any other norm. This rule may not apply in
infinite-dimensional vector spaces such as function spaces, though.</p>
</section>
<section id="inner-product-spaces">
<h2>Inner product spaces<a class="headerlink" href="#inner-product-spaces" title="Link to this heading">#</a></h2>
<p>An <strong>inner product</strong> on a real vector space <span class="math notranslate nohighlight">\(V\)</span> is a function
<span class="math notranslate nohighlight">\(\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}\)</span> satisfying</p>
<p>(i) <span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{x} \rangle \geq 0\)</span>, with equality if
and only if <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{0}\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\(\langle \alpha\mathbf{x} + \beta\mathbf{y}, \mathbf{z} \rangle = \alpha\langle \mathbf{x}, \mathbf{z} \rangle + \beta\langle \mathbf{y}, \mathbf{z} \rangle\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{y} \rangle = \langle \mathbf{y}, \mathbf{x} \rangle\)</span></p>
<p>for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y}, \mathbf{z} \in V\)</span> and all
<span class="math notranslate nohighlight">\(\alpha,\beta \in \mathbb{R}\)</span>. A vector space endowed with an inner
product is called an <strong>inner product space</strong>.</p>
<p>Note that any inner product on <span class="math notranslate nohighlight">\(V\)</span> induces a norm on <span class="math notranslate nohighlight">\(V\)</span>:
$<span class="math notranslate nohighlight">\(\|\mathbf{x}\| = \sqrt{\langle \mathbf{x}, \mathbf{x} \rangle}\)</span>$ One
can verify that the axioms for norms are satisfied under this definition
and follow (almost) directly from the axioms for inner products.
Therefore any inner product space is also a normed space (and hence also
a metric space).<a class="footnote-reference brackets" href="#id18" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a></p>
<p>Two vectors <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> are said to be <strong>orthogonal</strong>
if <span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{y} \rangle = 0\)</span>; we write
<span class="math notranslate nohighlight">\(\mathbf{x} \perp \mathbf{y}\)</span> for shorthand. Orthogonality generalizes
the notion of perpendicularity from Euclidean space. If two orthogonal
vectors <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> additionally have unit length
(i.e. <span class="math notranslate nohighlight">\(\|\mathbf{x}\| = \|\mathbf{y}\| = 1\)</span>), then they are described as
<strong>orthonormal</strong>.</p>
<p>The standard inner product on <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> is given by
$<span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{y} \rangle = \sum_{i=1}^n x_iy_i = \mathbf{x}^{\!\top\!}\mathbf{y}\)</span><span class="math notranslate nohighlight">\(
The matrix notation on the righthand side (see the Transposition section
if it's unfamiliar) arises because this inner product is a special case
of matrix multiplication where we regard the resulting \)</span>1 \times 1<span class="math notranslate nohighlight">\(
matrix as a scalar. The inner product on \)</span>\mathbb{R}^n<span class="math notranslate nohighlight">\( is also often
written \)</span>\mathbf{x}\cdot\mathbf{y}<span class="math notranslate nohighlight">\( (hence the alternate name **dot
product**). The reader can verify that the two-norm \)</span>|\cdot|_2<span class="math notranslate nohighlight">\( on
\)</span>\mathbb{R}^n$ is induced by this inner product.</p>
<section id="pythagorean-theorem">
<h3>Pythagorean Theorem<a class="headerlink" href="#pythagorean-theorem" title="Link to this heading">#</a></h3>
<p>The well-known Pythagorean theorem generalizes naturally to arbitrary
inner product spaces.</p>
<div class="theorem docutils">
<p>If <span class="math notranslate nohighlight">\(\mathbf{x} \perp \mathbf{y}\)</span>, then
$<span class="math notranslate nohighlight">\(\|\mathbf{x}+\mathbf{y}\|^2 = \|\mathbf{x}\|^2 + \|\mathbf{y}\|^2\)</span>$</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Suppose <span class="math notranslate nohighlight">\(\mathbf{x} \perp \mathbf{y}\)</span>, i.e.
<span class="math notranslate nohighlight">\(\langle \mathbf{x}, \mathbf{y} \rangle = 0\)</span>. Then
$<span class="math notranslate nohighlight">\(\|\mathbf{x}+\mathbf{y}\|^2 = \langle \mathbf{x}+\mathbf{y}, \mathbf{x}+\mathbf{y} \rangle = \langle \mathbf{x}, \mathbf{x} \rangle + \langle \mathbf{y}, \mathbf{x} \rangle + \langle \mathbf{x}, \mathbf{y} \rangle + \langle \mathbf{y}, \mathbf{y} \rangle = \|\mathbf{x}\|^2 + \|\mathbf{y}\|^2\)</span>$
as claimed. ◻</p>
</div>
</section>
<section id="cauchy-schwarz-inequality">
<h3>Cauchy-Schwarz inequality<a class="headerlink" href="#cauchy-schwarz-inequality" title="Link to this heading">#</a></h3>
<p>This inequality is sometimes useful in proving bounds:
$<span class="math notranslate nohighlight">\(|\langle \mathbf{x}, \mathbf{y} \rangle| \leq \|\mathbf{x}\| \cdot \|\mathbf{y}\|\)</span><span class="math notranslate nohighlight">\(
for all \)</span>\mathbf{x}, \mathbf{y} \in V<span class="math notranslate nohighlight">\(. Equality holds exactly when
\)</span>\mathbf{x}<span class="math notranslate nohighlight">\( and \)</span>\mathbf{y}$ are scalar multiples of each other (or
equivalently, when they are linearly dependent).</p>
</section>
</section>
<section id="transposition">
<h2>Transposition<a class="headerlink" href="#transposition" title="Link to this heading">#</a></h2>
<p>If <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span>, its <strong>transpose</strong>
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!} \in \mathbb{R}^{n \times m}\)</span> is given by
<span class="math notranslate nohighlight">\((\mathbf{A}^{\!\top\!})_{ij} = A_{ji}\)</span> for each <span class="math notranslate nohighlight">\((i, j)\)</span>. In other
words, the columns of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> become the rows of
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\)</span>, and the rows of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> become the columns
of <span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\)</span>.</p>
<p>The transpose has several nice algebraic properties that can be easily
verified from the definition:</p>
<p>(i) <span class="math notranslate nohighlight">\((\mathbf{A}^{\!\top\!})^{\!\top\!} = \mathbf{A}\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\((\mathbf{A}+\mathbf{B})^{\!\top\!} = \mathbf{A}^{\!\top\!} + \mathbf{B}^{\!\top\!}\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\((\alpha \mathbf{A})^{\!\top\!} = \alpha \mathbf{A}^{\!\top\!}\)</span></p>
<p>(iv) <span class="math notranslate nohighlight">\((\mathbf{A}\mathbf{B})^{\!\top\!} = \mathbf{B}^{\!\top\!} \mathbf{A}^{\!\top\!}\)</span></p>
</section>
<section id="eigenthings">
<h2>Eigenthings<a class="headerlink" href="#eigenthings" title="Link to this heading">#</a></h2>
<p>For a square matrix <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{n \times n}\)</span>, there may
be vectors which, when <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is applied to them, are simply
scaled by some constant. We say that a nonzero vector
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span> is an <strong>eigenvector</strong> of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>
corresponding to <strong>eigenvalue</strong> <span class="math notranslate nohighlight">\(\lambda\)</span> if
$<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x} = \lambda\mathbf{x}\)</span><span class="math notranslate nohighlight">\( The zero vector is excluded
from this definition because
\)</span>\mathbf{A}\mathbf{0} = \mathbf{0} = \lambda\mathbf{0}<span class="math notranslate nohighlight">\( for every
\)</span>\lambda$.</p>
<p>We now give some useful results about how eigenvalues change after
various manipulations.</p>
<div class="proposition docutils">
<p>Let <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> be an eigenvector of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> with corresponding
eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>. Then</p>
<p>(i) For any <span class="math notranslate nohighlight">\(\gamma \in \mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is an eigenvector of
<span class="math notranslate nohighlight">\(\mathbf{A} + \gamma\mathbf{I}\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda + \gamma\)</span>.</p>
<p>(ii) If <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is invertible, then <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is an eigenvector
of <span class="math notranslate nohighlight">\(\mathbf{A}^{-1}\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda^{-1}\)</span>.</p>
<p>(iii) <span class="math notranslate nohighlight">\(\mathbf{A}^k\mathbf{x} = \lambda^k\mathbf{x}\)</span> for any
<span class="math notranslate nohighlight">\(k \in \mathbb{Z}\)</span> (where <span class="math notranslate nohighlight">\(\mathbf{A}^0 = \mathbf{I}\)</span> by
definition).</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> (i) follows readily:
$<span class="math notranslate nohighlight">\((\mathbf{A} + \gamma\mathbf{I})\mathbf{x} = \mathbf{A}\mathbf{x} + \gamma\mathbf{I}\mathbf{x} = \lambda\mathbf{x} + \gamma\mathbf{x} = (\lambda + \gamma)\mathbf{x}\)</span>$</p>
<p>(ii) Suppose <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is invertible. Then
$<span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{A}^{-1}\mathbf{A}\mathbf{x} = \mathbf{A}^{-1}(\lambda\mathbf{x}) = \lambda\mathbf{A}^{-1}\mathbf{x}\)</span><span class="math notranslate nohighlight">\(
Dividing by \)</span>\lambda<span class="math notranslate nohighlight">\(, which is valid because the invertibility of
\)</span>\mathbf{A}<span class="math notranslate nohighlight">\( implies \)</span>\lambda \neq 0<span class="math notranslate nohighlight">\(, gives
\)</span>\lambda^{-1}\mathbf{x} = \mathbf{A}^{-1}\mathbf{x}$.</p>
<p>(iii) The case <span class="math notranslate nohighlight">\(k \geq 0\)</span> follows immediately by induction on <span class="math notranslate nohighlight">\(k\)</span>.
Then the general case <span class="math notranslate nohighlight">\(k \in \mathbb{Z}\)</span> follows by combining the
<span class="math notranslate nohighlight">\(k \geq 0\)</span> case with (ii). ◻</p>
</div>
</section>
<section id="trace">
<h2>Trace<a class="headerlink" href="#trace" title="Link to this heading">#</a></h2>
<p>The <strong>trace</strong> of a square matrix is the sum of its diagonal entries:
$<span class="math notranslate nohighlight">\(\tr(\mathbf{A}) = \sum_{i=1}^n A_{ii}\)</span>$ The trace has several nice
algebraic properties:</p>
<p>(i) <span class="math notranslate nohighlight">\(\tr(\mathbf{A}+\mathbf{B}) = \tr(\mathbf{A}) + \tr(\mathbf{B})\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\(\tr(\alpha\mathbf{A}) = \alpha\tr(\mathbf{A})\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\(\tr(\mathbf{A}^{\!\top\!}) = \tr(\mathbf{A})\)</span></p>
<p>(iv) <span class="math notranslate nohighlight">\(\tr(\mathbf{A}\mathbf{B}\mathbf{C}\mathbf{D}) = \tr(\mathbf{B}\mathbf{C}\mathbf{D}\mathbf{A}) = \tr(\mathbf{C}\mathbf{D}\mathbf{A}\mathbf{B}) = \tr(\mathbf{D}\mathbf{A}\mathbf{B}\mathbf{C})\)</span></p>
<p>The first three properties follow readily from the definition. The last
is known as <strong>invariance under cyclic permutations</strong>. Note that the
matrices cannot be reordered arbitrarily, for example
<span class="math notranslate nohighlight">\(\tr(\mathbf{A}\mathbf{B}\mathbf{C}\mathbf{D}) \neq \tr(\mathbf{B}\mathbf{A}\mathbf{C}\mathbf{D})\)</span>
in general. Also, there is nothing special about the product of four
matrices – analogous rules hold for more or fewer matrices.</p>
<p>Interestingly, the trace of a matrix is equal to the sum of its
eigenvalues (repeated according to multiplicity):
$<span class="math notranslate nohighlight">\(\tr(\mathbf{A}) = \sum_i \lambda_i(\mathbf{A})\)</span>$</p>
</section>
<section id="determinant">
<h2>Determinant<a class="headerlink" href="#determinant" title="Link to this heading">#</a></h2>
<p>The <strong>determinant</strong> of a square matrix can be defined in several
different confusing ways, none of which are particularly important for
our purposes; go look at an introductory linear algebra text (or
Wikipedia) if you need a definition. But it’s good to know the
properties:</p>
<p>(i) <span class="math notranslate nohighlight">\(\det(\mathbf{I}) = 1\)</span></p>
<p>(ii) <span class="math notranslate nohighlight">\(\det(\mathbf{A}^{\!\top\!}) = \det(\mathbf{A})\)</span></p>
<p>(iii) <span class="math notranslate nohighlight">\(\det(\mathbf{A}\mathbf{B}) = \det(\mathbf{A})\det(\mathbf{B})\)</span></p>
<p>(iv) <span class="math notranslate nohighlight">\(\det(\mathbf{A}^{-1}) = \det(\mathbf{A})^{-1}\)</span></p>
<p>(v) <span class="math notranslate nohighlight">\(\det(\alpha\mathbf{A}) = \alpha^n \det(\mathbf{A})\)</span></p>
<p>Interestingly, the determinant of a matrix is equal to the product of
its eigenvalues (repeated according to multiplicity):
$<span class="math notranslate nohighlight">\(\det(\mathbf{A}) = \prod_i \lambda_i(\mathbf{A})\)</span>$</p>
</section>
<section id="orthogonal-matrices">
<h2>Orthogonal matrices<a class="headerlink" href="#orthogonal-matrices" title="Link to this heading">#</a></h2>
<p>A matrix <span class="math notranslate nohighlight">\(\mathbf{Q} \in \mathbb{R}^{n \times n}\)</span> is said to be
<strong>orthogonal</strong> if its columns are pairwise orthonormal. This definition
implies that
$<span class="math notranslate nohighlight">\(\mathbf{Q}^{\!\top\!} \mathbf{Q} = \mathbf{Q}\mathbf{Q}^{\!\top\!} = \mathbf{I}\)</span><span class="math notranslate nohighlight">\(
or equivalently, \)</span>\mathbf{Q}^{!\top!} = \mathbf{Q}^{-1}<span class="math notranslate nohighlight">\(. A nice thing
about orthogonal matrices is that they preserve inner products:
\)</span><span class="math notranslate nohighlight">\((\mathbf{Q}\mathbf{x})^{\!\top\!}(\mathbf{Q}\mathbf{y}) = \mathbf{x}^{\!\top\!} \mathbf{Q}^{\!\top\!} \mathbf{Q}\mathbf{y} = \mathbf{x}^{\!\top\!} \mathbf{I}\mathbf{y} = \mathbf{x}^{\!\top\!}\mathbf{y}\)</span><span class="math notranslate nohighlight">\(
A direct result of this fact is that they also preserve 2-norms:
\)</span><span class="math notranslate nohighlight">\(\|\mathbf{Q}\mathbf{x}\|_2 = \sqrt{(\mathbf{Q}\mathbf{x})^{\!\top\!}(\mathbf{Q}\mathbf{x})} = \sqrt{\mathbf{x}^{\!\top\!}\mathbf{x}} = \|\mathbf{x}\|_2\)</span>$
Therefore multiplication by an orthogonal matrix can be considered as a
transformation that preserves length, but may rotate or reflect the
vector about the origin.</p>
</section>
<section id="symmetric-matrices">
<h2>Symmetric matrices<a class="headerlink" href="#symmetric-matrices" title="Link to this heading">#</a></h2>
<p>A matrix <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{n \times n}\)</span> is said to be
<strong>symmetric</strong> if it is equal to its own transpose
(<span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{A}^{\!\top\!}\)</span>), meaning that <span class="math notranslate nohighlight">\(A_{ij} = A_{ji}\)</span>
for all <span class="math notranslate nohighlight">\((i,j)\)</span>. This definition seems harmless enough but turns out to
have some strong implications. We summarize the most important of these
as</p>
<div class="theorem docutils">
<p>(Spectral Theorem) If <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{n \times n}\)</span> is
symmetric, then there exists an orthonormal basis for <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>
consisting of eigenvectors of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>.</p>
</div>
<p>The practical application of this theorem is a particular factorization
of symmetric matrices, referred to as the <strong>eigendecomposition</strong> or
<strong>spectral decomposition</strong>. Denote the orthonormal basis of eigenvectors
<span class="math notranslate nohighlight">\(\mathbf{q}_1, \dots, \mathbf{q}_n\)</span> and their eigenvalues
<span class="math notranslate nohighlight">\(\lambda_1, \dots, \lambda_n\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> be an orthogonal matrix
with <span class="math notranslate nohighlight">\(\mathbf{q}_1, \dots, \mathbf{q}_n\)</span> as its columns, and
<span class="math notranslate nohighlight">\(\mathbf{\Lambda} = \diag(\lambda_1, \dots, \lambda_n)\)</span>. Since by
definition <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{q}_i = \lambda_i\mathbf{q}_i\)</span> for every
<span class="math notranslate nohighlight">\(i\)</span>, the following relationship holds:
$<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{Q} = \mathbf{Q}\mathbf{\Lambda}\)</span><span class="math notranslate nohighlight">\( Right-multiplying
by \)</span>\mathbf{Q}^{!\top!}<span class="math notranslate nohighlight">\(, we arrive at the decomposition
\)</span><span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{\!\top\!}\)</span>$</p>
<section id="rayleigh-quotients">
<h3>Rayleigh quotients<a class="headerlink" href="#rayleigh-quotients" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{n \times n}\)</span> be a symmetric matrix. The
expression <span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}\)</span> is called a
<strong>quadratic form</strong>.</p>
<p>There turns out to be an interesting connection between the quadratic
form of a symmetric matrix and its eigenvalues. This connection is
provided by the <strong>Rayleigh quotient</strong>
$<span class="math notranslate nohighlight">\(R_\mathbf{A}(\mathbf{x}) = \frac{\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}}{\mathbf{x}^{\!\top\!}\mathbf{x}}\)</span>$
The Rayleigh quotient has a couple of important properties which the
reader can (and should!) easily verify from the definition:</p>
<p>(i) <strong>Scale invariance</strong>: for any vector <span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{0}\)</span>
and any scalar <span class="math notranslate nohighlight">\(\alpha \neq 0\)</span>,
<span class="math notranslate nohighlight">\(R_\mathbf{A}(\mathbf{x}) = R_\mathbf{A}(\alpha\mathbf{x})\)</span>.</p>
<p>(ii) If <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is an eigenvector of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> with eigenvalue
<span class="math notranslate nohighlight">\(\lambda\)</span>, then <span class="math notranslate nohighlight">\(R_\mathbf{A}(\mathbf{x}) = \lambda\)</span>.</p>
<p>We can further show that the Rayleigh quotient is bounded by the largest
and smallest eigenvalues of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. But first we will show a
useful special case of the final result.</p>
<div class="proposition docutils">
<p>For any <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> such that <span class="math notranslate nohighlight">\(\|\mathbf{x}\|_2 = 1\)</span>,
$<span class="math notranslate nohighlight">\(\lambda_{\min}(\mathbf{A}) \leq \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} \leq \lambda_{\max}(\mathbf{A})\)</span><span class="math notranslate nohighlight">\(
with equality if and only if \)</span>\mathbf{x}$ is a corresponding
eigenvector.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> We show only the <span class="math notranslate nohighlight">\(\max\)</span> case because the argument for the
<span class="math notranslate nohighlight">\(\min\)</span> case is entirely analogous.</p>
<p>Since <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is symmetric, we can decompose it as
<span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{\!\top\!}\)</span>. Then use
the change of variable <span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{Q}^{\!\top\!}\mathbf{x}\)</span>,
noting that the relationship between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is
one-to-one and that <span class="math notranslate nohighlight">\(\|\mathbf{y}\|_2 = 1\)</span> since <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> is
orthogonal. Hence
$<span class="math notranslate nohighlight">\(\max_{\|\mathbf{x}\|_2 = 1} \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} = \max_{\|\mathbf{y}\|_2 = 1} \mathbf{y}^{\!\top\!}\mathbf{\Lambda}\mathbf{y} = \max_{y_1^2+\dots+y_n^2=1} \sum_{i=1}^n \lambda_i y_i^2\)</span><span class="math notranslate nohighlight">\(
Written this way, it is clear that \)</span>\mathbf{y}<span class="math notranslate nohighlight">\( maximizes this
expression exactly if and only if it satisfies
\)</span>\sum_{i \in I} y_i^2 = 1<span class="math notranslate nohighlight">\( where
\)</span>I = {i : \lambda_i = \max_{j=1,\dots,n} \lambda_j = \lambda_{\max}(\mathbf{A})}<span class="math notranslate nohighlight">\(
and \)</span>y_j = 0<span class="math notranslate nohighlight">\( for \)</span>j \not\in I<span class="math notranslate nohighlight">\(. That is, \)</span>I<span class="math notranslate nohighlight">\( contains the index or
indices of the largest eigenvalue. In this case, the maximal value of
the expression is
\)</span><span class="math notranslate nohighlight">\(\sum_{i=1}^n \lambda_i y_i^2 = \sum_{i \in I} \lambda_i y_i^2 = \lambda_{\max}(\mathbf{A}) \sum_{i \in I} y_i^2 = \lambda_{\max}(\mathbf{A})\)</span><span class="math notranslate nohighlight">\(
Then writing \)</span>\mathbf{q}_1, \dots, \mathbf{q}_n<span class="math notranslate nohighlight">\( for the columns of
\)</span>\mathbf{Q}<span class="math notranslate nohighlight">\(, we have
\)</span><span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{Q}\mathbf{Q}^{\!\top\!}\mathbf{x} = \mathbf{Q}\mathbf{y} = \sum_{i=1}^n y_i\mathbf{q}_i = \sum_{i \in I} y_i\mathbf{q}_i\)</span>$
where we have used the matrix-vector product identity.</p>
<p>Recall that <span class="math notranslate nohighlight">\(\mathbf{q}_1, \dots, \mathbf{q}_n\)</span> are eigenvectors of
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and form an orthonormal basis for <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>. Therefore
by construction, the set <span class="math notranslate nohighlight">\(\{\mathbf{q}_i : i \in I\}\)</span> forms an
orthonormal basis for the eigenspace of <span class="math notranslate nohighlight">\(\lambda_{\max}(\mathbf{A})\)</span>.
Hence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, which is a linear combination of these, lies in that
eigenspace and thus is an eigenvector of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> corresponding to
<span class="math notranslate nohighlight">\(\lambda_{\max}(\mathbf{A})\)</span>.</p>
<p>We have shown that
<span class="math notranslate nohighlight">\(\max_{\|\mathbf{x}\|_2 = 1} \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} = \lambda_{\max}(\mathbf{A})\)</span>,
from which we have the general inequality
<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} \leq \lambda_{\max}(\mathbf{A})\)</span>
for all unit-length <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. ◻</p>
</div>
<p>By the scale invariance of the Rayleigh quotient, we immediately have as
a corollary (since
<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} = R_{\mathbf{A}}(\mathbf{x})\)</span>
for unit <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>)</p>
<div class="theorem docutils">
<p>(Min-max theorem) For all <span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{0}\)</span>,
$<span class="math notranslate nohighlight">\(\lambda_{\min}(\mathbf{A}) \leq R_\mathbf{A}(\mathbf{x}) \leq \lambda_{\max}(\mathbf{A})\)</span><span class="math notranslate nohighlight">\(
with equality if and only if \)</span>\mathbf{x}$ is a corresponding
eigenvector.</p>
</div>
</section>
</section>
<section id="positive-semi-definite-matrices">
<h2>Positive (semi-)definite matrices<a class="headerlink" href="#positive-semi-definite-matrices" title="Link to this heading">#</a></h2>
<p>A symmetric matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is <strong>positive semi-definite</strong> if for all
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} \geq 0\)</span>. Sometimes people
write <span class="math notranslate nohighlight">\(\mathbf{A} \succeq 0\)</span> to indicate that <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is positive
semi-definite.</p>
<p>A symmetric matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is <strong>positive definite</strong> if for all
nonzero <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} &gt; 0\)</span>. Sometimes people write
<span class="math notranslate nohighlight">\(\mathbf{A} \succ 0\)</span> to indicate that <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is positive definite.
Note that positive definiteness is a strictly stronger property than
positive semi-definiteness, in the sense that every positive definite
matrix is positive semi-definite but not vice-versa.</p>
<p>These properties are related to eigenvalues in the following way.</p>
<div class="proposition docutils">
<p>A symmetric matrix is positive semi-definite if and only if all of its
eigenvalues are nonnegative, and positive definite if and only if all of
its eigenvalues are positive.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Suppose <span class="math notranslate nohighlight">\(A\)</span> is positive semi-definite, and let <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> be
an eigenvector of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>. Then
$<span class="math notranslate nohighlight">\(0 \leq \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} = \mathbf{x}^{\!\top\!}(\lambda\mathbf{x}) = \lambda\mathbf{x}^{\!\top\!}\mathbf{x} = \lambda\|\mathbf{x}\|_2^2\)</span><span class="math notranslate nohighlight">\(
Since \)</span>\mathbf{x} \neq \mathbf{0}<span class="math notranslate nohighlight">\( (by the assumption that it is an
eigenvector), we have \)</span>|\mathbf{x}|_2^2 &gt; 0<span class="math notranslate nohighlight">\(, so we can divide both
sides by \)</span>|\mathbf{x}|_2^2<span class="math notranslate nohighlight">\( to arrive at \)</span>\lambda \geq 0<span class="math notranslate nohighlight">\(. If
\)</span>\mathbf{A}<span class="math notranslate nohighlight">\( is positive definite, the inequality above holds strictly,
so \)</span>\lambda &gt; 0$. This proves one direction.</p>
<p>To simplify the proof of the other direction, we will use the machinery
of Rayleigh quotients. Suppose that <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is symmetric and all
its eigenvalues are nonnegative. Then for all
<span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{0}\)</span>,
$<span class="math notranslate nohighlight">\(0 \leq \lambda_{\min}(\mathbf{A}) \leq R_\mathbf{A}(\mathbf{x})\)</span><span class="math notranslate nohighlight">\(
Since \)</span>\mathbf{x}^{!\top!}\mathbf{A}\mathbf{x}<span class="math notranslate nohighlight">\( matches
\)</span>R_\mathbf{A}(\mathbf{x})<span class="math notranslate nohighlight">\( in sign, we conclude that \)</span>\mathbf{A}<span class="math notranslate nohighlight">\( is
positive semi-definite. If the eigenvalues of \)</span>\mathbf{A}<span class="math notranslate nohighlight">\( are all
strictly positive, then \)</span>0 &lt; \lambda_{\min}(\mathbf{A})<span class="math notranslate nohighlight">\(, whence it
follows that \)</span>\mathbf{A}$ is positive definite. ◻</p>
</div>
<p>As an example of how these matrices arise, consider</p>
<div class="proposition docutils">
<p>Suppose <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span>. Then
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\mathbf{A}\)</span> is positive semi-definite. If
<span class="math notranslate nohighlight">\(\Null(\mathbf{A}) = \{\mathbf{0}\}\)</span>, then
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\mathbf{A}\)</span> is positive definite.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> For any <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span>,
$<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!} (\mathbf{A}^{\!\top\!}\mathbf{A})\mathbf{x} = (\mathbf{A}\mathbf{x})^{\!\top\!}(\mathbf{A}\mathbf{x}) = \|\mathbf{A}\mathbf{x}\|_2^2 \geq 0\)</span><span class="math notranslate nohighlight">\(
so \)</span>\mathbf{A}^{!\top!}\mathbf{A}$ is positive semi-definite.</p>
<p>Note that <span class="math notranslate nohighlight">\(\|\mathbf{A}\mathbf{x}\|_2^2 = 0\)</span> implies
<span class="math notranslate nohighlight">\(\|\mathbf{A}\mathbf{x}\|_2 = 0\)</span>, which in turn implies
<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x} = \mathbf{0}\)</span> (recall that this is a property of
norms). If <span class="math notranslate nohighlight">\(\Null(\mathbf{A}) = \{\mathbf{0}\}\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x} = \mathbf{0}\)</span> implies <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{0}\)</span>,
so
<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!} (\mathbf{A}^{\!\top\!}\mathbf{A})\mathbf{x} = 0\)</span>
if and only if <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{0}\)</span>, and thus
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\mathbf{A}\)</span> is positive definite. ◻</p>
</div>
<p>Positive definite matrices are invertible (since their eigenvalues are
nonzero), whereas positive semi-definite matrices might not be. However,
if you already have a positive semi-definite matrix, it is possible to
perturb its diagonal slightly to produce a positive definite matrix.</p>
<div class="proposition docutils">
<p>If <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is positive semi-definite and <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>, then
<span class="math notranslate nohighlight">\(\mathbf{A} + \epsilon\mathbf{I}\)</span> is positive definite.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Assuming <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is positive semi-definite and
<span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>, we have for any <span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{0}\)</span> that
$<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}(\mathbf{A}+\epsilon\mathbf{I})\mathbf{x} = \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} + \epsilon\mathbf{x}^{\!\top\!}\mathbf{I}\mathbf{x} = \underbrace{\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}}_{\geq 0} + \underbrace{\epsilon\|\mathbf{x}\|_2^2}_{&gt; 0} &gt; 0\)</span>$
as claimed. ◻</p>
</div>
<p>An obvious but frequently useful consequence of the two propositions we
have just shown is that
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\mathbf{A} + \epsilon\mathbf{I}\)</span> is positive
definite (and in particular, invertible) for <em>any</em> matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>
and any <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>.</p>
<section id="the-geometry-of-positive-definite-quadratic-forms">
<h3>The geometry of positive definite quadratic forms<a class="headerlink" href="#the-geometry-of-positive-definite-quadratic-forms" title="Link to this heading">#</a></h3>
<p>A useful way to understand quadratic forms is by the geometry of their
level sets. A <strong>level set</strong> or <strong>isocontour</strong> of a function is the set
of all inputs such that the function applied to those inputs yields a
given output. Mathematically, the <span class="math notranslate nohighlight">\(c\)</span>-isocontour of <span class="math notranslate nohighlight">\(f\)</span> is
<span class="math notranslate nohighlight">\(\{\mathbf{x} \in \dom f : f(\mathbf{x}) = c\}\)</span>.</p>
<p>Let us consider the special case
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}\)</span> where
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is a positive definite matrix. Since <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is
positive definite, it has a unique matrix square root
<span class="math notranslate nohighlight">\(\mathbf{A}^{\frac{1}{2}} = \mathbf{Q}\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{Q}^{\!\top\!}\)</span>,
where <span class="math notranslate nohighlight">\(\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{\!\top\!}\)</span> is the
eigendecomposition of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{\Lambda}^{\frac{1}{2}} = \diag(\sqrt{\lambda_1}, \dots \sqrt{\lambda_n})\)</span>.
It is easy to see that this matrix <span class="math notranslate nohighlight">\(\mathbf{A}^{\frac{1}{2}}\)</span> is
positive definite (consider its eigenvalues) and satisfies
<span class="math notranslate nohighlight">\(\mathbf{A}^{\frac{1}{2}}\mathbf{A}^{\frac{1}{2}} = \mathbf{A}\)</span>. Fixing
a value <span class="math notranslate nohighlight">\(c \geq 0\)</span>, the <span class="math notranslate nohighlight">\(c\)</span>-isocontour of <span class="math notranslate nohighlight">\(f\)</span> is the set of
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span> such that
$<span class="math notranslate nohighlight">\(c = \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} = \mathbf{x}^{\!\top\!}\mathbf{A}^{\frac{1}{2}}\mathbf{A}^{\frac{1}{2}}\mathbf{x} = \|\mathbf{A}^{\frac{1}{2}}\mathbf{x}\|_2^2\)</span><span class="math notranslate nohighlight">\(
where we have used the symmetry of \)</span>\mathbf{A}^{\frac{1}{2}}<span class="math notranslate nohighlight">\(. Making
the change of variable
\)</span>\mathbf{z} = \mathbf{A}^{\frac{1}{2}}\mathbf{x}<span class="math notranslate nohighlight">\(, we have the condition
\)</span>|\mathbf{z}|_2 = \sqrt{c}<span class="math notranslate nohighlight">\(. That is, the values \)</span>\mathbf{z}<span class="math notranslate nohighlight">\( lie on a
sphere of radius \)</span>\sqrt{c}<span class="math notranslate nohighlight">\(. These can be parameterized as
\)</span>\mathbf{z} = \sqrt{c}\hat{\mathbf{z}}<span class="math notranslate nohighlight">\( where \)</span>\hat{\mathbf{z}}<span class="math notranslate nohighlight">\( has
\)</span>|\hat{\mathbf{z}}|_2 = 1<span class="math notranslate nohighlight">\(. Then since
\)</span>\mathbf{A}^{-\frac{1}{2}} = \mathbf{Q}\mathbf{\Lambda}^{-\frac{1}{2}}\mathbf{Q}^{!\top!}<span class="math notranslate nohighlight">\(,
we have
\)</span><span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{A}^{-\frac{1}{2}}\mathbf{z} = \mathbf{Q}\mathbf{\Lambda}^{-\frac{1}{2}}\mathbf{Q}^{\!\top\!}\sqrt{c}\hat{\mathbf{z}} = \sqrt{c}\mathbf{Q}\mathbf{\Lambda}^{-\frac{1}{2}}\tilde{\mathbf{z}}\)</span><span class="math notranslate nohighlight">\(
where \)</span>\tilde{\mathbf{z}} = \mathbf{Q}^{!\top!}\hat{\mathbf{z}}<span class="math notranslate nohighlight">\( also
satisfies \)</span>|\tilde{\mathbf{z}}|_2 = 1<span class="math notranslate nohighlight">\( since \)</span>\mathbf{Q}<span class="math notranslate nohighlight">\( is
orthogonal. Using this parameterization, we see that the solution set
\)</span>{\mathbf{x} \in \mathbb{R}^n : f(\mathbf{x}) = c}<span class="math notranslate nohighlight">\( is the image of
the unit sphere
\)</span>{\tilde{\mathbf{z}} \in \mathbb{R}^n : |\tilde{\mathbf{z}}|_2 = 1}<span class="math notranslate nohighlight">\(
under the invertible linear map
\)</span>\mathbf{x} = \sqrt{c}\mathbf{Q}\mathbf{\Lambda}^{-\frac{1}{2}}\tilde{\mathbf{z}}$.</p>
<p>What we have gained with all these manipulations is a clear algebraic
understanding of the <span class="math notranslate nohighlight">\(c\)</span>-isocontour of <span class="math notranslate nohighlight">\(f\)</span> in terms of a sequence of
linear transformations applied to a well-understood set. We begin with
the unit sphere, then scale every axis <span class="math notranslate nohighlight">\(i\)</span> by
<span class="math notranslate nohighlight">\(\lambda_i^{-\frac{1}{2}}\)</span>, resulting in an axis-aligned ellipsoid.
Observe that the axis lengths of the ellipsoid are proportional to the
inverse square roots of the eigenvalues of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. Hence larger
eigenvalues correspond to shorter axis lengths, and vice-versa.</p>
<p>Then this axis-aligned ellipsoid undergoes a rigid transformation (i.e.
one that preserves length and angles, such as a rotation/reflection)
given by <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>. The result of this transformation is that the
axes of the ellipse are no longer along the coordinate axes in general,
but rather along the directions given by the corresponding eigenvectors.
To see this, consider the unit vector <span class="math notranslate nohighlight">\(\mathbf{e}_i \in \mathbb{R}^n\)</span>
that has <span class="math notranslate nohighlight">\([\mathbf{e}_i]_j = \delta_{ij}\)</span>. In the pre-transformed space,
this vector points along the axis with length proportional to
<span class="math notranslate nohighlight">\(\lambda_i^{-\frac{1}{2}}\)</span>. But after applying the rigid transformation
<span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>, the resulting vector points in the direction of the
corresponding eigenvector <span class="math notranslate nohighlight">\(\mathbf{q}_i\)</span>, since
$<span class="math notranslate nohighlight">\(\mathbf{Q}\mathbf{e}_i = \sum_{j=1}^n [\mathbf{e}_i]_j\mathbf{q}_j = \mathbf{q}_i\)</span>$
where we have used the matrix-vector product identity from earlier.</p>
<p>In summary: the isocontours of
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}\)</span> are
ellipsoids such that the axes point in the directions of the
eigenvectors of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, and the radii of these axes are
proportional to the inverse square roots of the corresponding
eigenvalues.</p>
</section>
</section>
<section id="singular-value-decomposition">
<h2>Singular value decomposition<a class="headerlink" href="#singular-value-decomposition" title="Link to this heading">#</a></h2>
<p>Singular value decomposition (SVD) is a widely applicable tool in linear
algebra. Its strength stems partially from the fact that <em>every matrix</em>
<span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> has an SVD (even non-square
matrices)! The decomposition goes as follows:
$<span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!}\)</span><span class="math notranslate nohighlight">\( where
\)</span>\mathbf{U} \in \mathbb{R}^{m \times m}<span class="math notranslate nohighlight">\( and
\)</span>\mathbf{V} \in \mathbb{R}^{n \times n}<span class="math notranslate nohighlight">\( are orthogonal matrices and
\)</span>\mathbf{\Sigma} \in \mathbb{R}^{m \times n}<span class="math notranslate nohighlight">\( is a diagonal matrix with
the **singular values** of \)</span>\mathbf{A}<span class="math notranslate nohighlight">\( (denoted \)</span>\sigma_i$) on its
diagonal.</p>
<p>By convention, the singular values are given in non-increasing order,
i.e.
$<span class="math notranslate nohighlight">\(\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_{\min(m,n)} \geq 0\)</span><span class="math notranslate nohighlight">\(
Only the first \)</span>r<span class="math notranslate nohighlight">\( singular values are nonzero, where \)</span>r<span class="math notranslate nohighlight">\( is the rank of
\)</span>\mathbf{A}$.</p>
<p>Observe that the SVD factors provide eigendecompositions for
<span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^{\!\top\!}\)</span>:
$<span class="math notranslate nohighlight">\(\begin{aligned}
\mathbf{A}^{\!\top\!}\mathbf{A} &amp;= (\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!})^{\!\top\!}\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!} = \mathbf{V}\mathbf{\Sigma}^{\!\top\!}\mathbf{U}^{\!\top\!}\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!} = \mathbf{V}\mathbf{\Sigma}^{\!\top\!}\mathbf{\Sigma}\mathbf{V}^{\!\top\!} \\
\mathbf{A}\mathbf{A}^{\!\top\!} &amp;= \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!}(\mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!})^{\!\top\!} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\!\top\!}\mathbf{V}\mathbf{\Sigma}^{\!\top\!}\mathbf{U}^{\!\top\!} = \mathbf{U}\mathbf{\Sigma}\mathbf{\Sigma}^{\!\top\!}\mathbf{U}^{\!\top\!}
\end{aligned}\)</span><span class="math notranslate nohighlight">\( It follows immediately that the columns of \)</span>\mathbf{V}<span class="math notranslate nohighlight">\(
(the **right-singular vectors** of \)</span>\mathbf{A}<span class="math notranslate nohighlight">\() are eigenvectors of
\)</span>\mathbf{A}^{!\top!}\mathbf{A}<span class="math notranslate nohighlight">\(, and the columns of \)</span>\mathbf{U}<span class="math notranslate nohighlight">\( (the
**left-singular vectors** of \)</span>\mathbf{A}<span class="math notranslate nohighlight">\() are eigenvectors of
\)</span>\mathbf{A}\mathbf{A}^{!\top!}$.</p>
<p>The matrices <span class="math notranslate nohighlight">\(\mathbf{\Sigma}^{\!\top\!}\mathbf{\Sigma}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{\Sigma}\mathbf{\Sigma}^{\!\top\!}\)</span> are not necessarily the same
size, but both are diagonal with the squared singular values
<span class="math notranslate nohighlight">\(\sigma_i^2\)</span> on the diagonal (plus possibly some zeros). Thus the
singular values of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> are the square roots of the eigenvalues
of <span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\mathbf{A}\)</span> (or equivalently, of
<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^{\!\top\!}\)</span>)<a class="footnote-reference brackets" href="#id19" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>.</p>
</section>
<section id="some-useful-matrix-identities">
<h2>Some useful matrix identities<a class="headerlink" href="#some-useful-matrix-identities" title="Link to this heading">#</a></h2>
<section id="matrix-vector-product-as-linear-combination-of-matrix-columns">
<h3>Matrix-vector product as linear combination of matrix columns<a class="headerlink" href="#matrix-vector-product-as-linear-combination-of-matrix-columns" title="Link to this heading">#</a></h3>
<div class="proposition docutils">
<p>Let <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span> be a vector and
<span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> a matrix with columns
<span class="math notranslate nohighlight">\(\mathbf{a}_1, \dots, \mathbf{a}_n\)</span>. Then
$<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x} = \sum_{i=1}^n x_i\mathbf{a}_i\)</span>$</p>
</div>
<p>This identity is extremely useful in understanding linear operators in
terms of their matrices’ columns. The proof is very simple (consider
each element of <span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x}\)</span> individually and expand by
definitions) but it is a good exercise to convince yourself.</p>
</section>
<section id="sum-of-outer-products-as-matrix-matrix-product">
<h3>Sum of outer products as matrix-matrix product<a class="headerlink" href="#sum-of-outer-products-as-matrix-matrix-product" title="Link to this heading">#</a></h3>
<p>An <strong>outer product</strong> is an expression of the form
<span class="math notranslate nohighlight">\(\mathbf{a}\mathbf{b}^{\!\top\!}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{a} \in \mathbb{R}^m\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span>. By inspection it is not hard to see
that such an expression yields an <span class="math notranslate nohighlight">\(m \times n\)</span> matrix such that
$<span class="math notranslate nohighlight">\([\mathbf{a}\mathbf{b}^{\!\top\!}]_{ij} = a_ib_j\)</span>$ It is not
immediately obvious, but the sum of outer products is actually
equivalent to an appropriate matrix-matrix product! We formalize this
statement as</p>
<div class="proposition docutils">
<p>Let <span class="math notranslate nohighlight">\(\mathbf{a}_1, \dots, \mathbf{a}_k \in \mathbb{R}^m\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{b}_1, \dots, \mathbf{b}_k \in \mathbb{R}^n\)</span>. Then
$<span class="math notranslate nohighlight">\(\sum_{\ell=1}^k \mathbf{a}_\ell\mathbf{b}_\ell^{\!\top\!} = \mathbf{A}\mathbf{B}^{\!\top\!}\)</span><span class="math notranslate nohighlight">\(
where
\)</span><span class="math notranslate nohighlight">\(\mathbf{A} = \begin{bmatrix}\mathbf{a}_1 &amp; \cdots &amp; \mathbf{a}_k\end{bmatrix}, \hspace{0.5cm} \mathbf{B} = \begin{bmatrix}\mathbf{b}_1 &amp; \cdots &amp; \mathbf{b}_k\end{bmatrix}\)</span>$</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> For each <span class="math notranslate nohighlight">\((i,j)\)</span>, we have
$<span class="math notranslate nohighlight">\(\left[\sum_{\ell=1}^k \mathbf{a}_\ell\mathbf{b}_\ell^{\!\top\!}\right]_{ij} = \sum_{\ell=1}^k [\mathbf{a}_\ell\mathbf{b}_\ell^{\!\top\!}]_{ij} = \sum_{\ell=1}^k [\mathbf{a}_\ell]_i[\mathbf{b}_\ell]_j = \sum_{\ell=1}^k A_{i\ell}B_{j\ell}\)</span><span class="math notranslate nohighlight">\(
This last expression should be recognized as an inner product between
the \)</span>i<span class="math notranslate nohighlight">\(th row of \)</span>\mathbf{A}<span class="math notranslate nohighlight">\( and the \)</span>j<span class="math notranslate nohighlight">\(th row of \)</span>\mathbf{B}<span class="math notranslate nohighlight">\(, or
equivalently the \)</span>j<span class="math notranslate nohighlight">\(th column of \)</span>\mathbf{B}^{!\top!}<span class="math notranslate nohighlight">\(. Hence by the
definition of matrix multiplication, it is equal to
\)</span>[\mathbf{A}\mathbf{B}^{!\top!}]_{ij}$. ◻</p>
</div>
</section>
<section id="quadratic-forms">
<h3>Quadratic forms<a class="headerlink" href="#quadratic-forms" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{n \times n}\)</span> be a symmetric matrix, and
recall that the expression <span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}\)</span>
is called a quadratic form of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. It is in some cases helpful
to rewrite the quadratic form in terms of the individual elements that
make up <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>:
$<span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x} = \sum_{i=1}^n\sum_{j=1}^n A_{ij}x_ix_j\)</span>$
This identity is valid for any square matrix (need not be symmetric),
although quadratic forms are usually only discussed in the context of
symmetric matrices.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="calculus-and-optimization">
<h1>Calculus and Optimization<a class="headerlink" href="#calculus-and-optimization" title="Link to this heading">#</a></h1>
<p>Much of machine learning is about minimizing a <strong>cost function</strong> (also
called an <strong>objective function</strong> in the optimization community), which
is a scalar function of several variables that typically measures how
poorly our model fits the data we have.</p>
<section id="extrema">
<h2>Extrema<a class="headerlink" href="#extrema" title="Link to this heading">#</a></h2>
<p>Optimization is about finding <strong>extrema</strong>, which depending on the
application could be minima or maxima. When defining extrema, it is
necessary to consider the set of inputs over which we’re optimizing.
This set <span class="math notranslate nohighlight">\(\mathcal{X} \subseteq \mathbb{R}^d\)</span> is called the <strong>feasible
set</strong>. If <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is the entire domain of the function being
optimized (as it often will be for our purposes), we say that the
problem is <strong>unconstrained</strong>. Otherwise the problem is <strong>constrained</strong>
and may be much harder to solve, depending on the nature of the feasible
set.</p>
<p>Suppose <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span>. A point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is said
to be a <strong>local minimum</strong> (resp. <strong>local maximum</strong>) of <span class="math notranslate nohighlight">\(f\)</span> in
<span class="math notranslate nohighlight">\(\mathcal{X}\)</span> if <span class="math notranslate nohighlight">\(f(\mathbf{x}) \leq f(\mathbf{y})\)</span> (resp.
<span class="math notranslate nohighlight">\(f(\mathbf{x}) \geq f(\mathbf{y})\)</span>) for all <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> in some
neighborhood <span class="math notranslate nohighlight">\(N \subseteq \mathcal{X}\)</span> about <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.<a class="footnote-reference brackets" href="#id20" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>
Furthermore, if <span class="math notranslate nohighlight">\(f(\mathbf{x}) \leq f(\mathbf{y})\)</span> for all
<span class="math notranslate nohighlight">\(\mathbf{y} \in \mathcal{X}\)</span>, then <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a <strong>global minimum</strong>
of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> (similarly for global maximum). If the phrase
“in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>” is unclear from context, assume we are optimizing
over the whole domain of the function.</p>
<p>The qualifier <strong>strict</strong> (as in e.g. a strict local minimum) means that
the inequality sign in the definition is actually a <span class="math notranslate nohighlight">\(&gt;\)</span> or <span class="math notranslate nohighlight">\(&lt;\)</span>, with
equality not allowed. This indicates that the extremum is unique within
some neighborhood.</p>
<p>Observe that maximizing a function <span class="math notranslate nohighlight">\(f\)</span> is equivalent to minimizing <span class="math notranslate nohighlight">\(-f\)</span>,
so optimization problems are typically phrased in terms of minimization
without loss of generality. This convention (which we follow here)
eliminates the need to discuss minimization and maximization separately.</p>
</section>
<section id="gradients">
<h2>Gradients<a class="headerlink" href="#gradients" title="Link to this heading">#</a></h2>
<p>The single most important concept from calculus in the context of
machine learning is the <strong>gradient</strong>. Gradients generalize derivatives
to scalar functions of several variables. The gradient of
<span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span>, denoted <span class="math notranslate nohighlight">\(\nabla f\)</span>, is given by
$<span class="math notranslate nohighlight">\(\nabla f = \begin{bmatrix}\pdv{f}{x_1} \\ \vdots \\ \pdv{f}{x_n}\end{bmatrix}
\hspace{0.5cm}\text{i.e.}\hspace{0.5cm}
[\nabla f]_i = \pdv{f}{x_i}\)</span><span class="math notranslate nohighlight">\( Gradients have the following very
important property: \)</span>\nabla f(\mathbf{x})<span class="math notranslate nohighlight">\( points in the direction of
**steepest ascent** from \)</span>\mathbf{x}<span class="math notranslate nohighlight">\(. Similarly,
\)</span>-\nabla f(\mathbf{x})<span class="math notranslate nohighlight">\( points in the direction of **steepest descent**
from \)</span>\mathbf{x}$. We will use this fact frequently when iteratively
minimizing a function via <strong>gradient descent</strong>.</p>
</section>
<section id="the-jacobian">
<h2>The Jacobian<a class="headerlink" href="#the-jacobian" title="Link to this heading">#</a></h2>
<p>The <strong>Jacobian</strong> of <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}^m\)</span> is a matrix of
first-order partial derivatives: $<span class="math notranslate nohighlight">\(\mathbf{J}_f = \begin{bmatrix}
    \pdv{f_1}{x_1} &amp; \hdots &amp; \pdv{f_1}{x_n} \\
    \vdots &amp; \ddots &amp; \vdots \\
    \pdv{f_m}{x_1} &amp; \hdots &amp; \pdv{f_m}{x_n}\end{bmatrix}
\hspace{0.5cm}\text{i.e.}\hspace{0.5cm}
[\mathbf{J}_f]_{ij} = \pdv{f_i}{x_j}\)</span><span class="math notranslate nohighlight">\( Note the special case \)</span>m = 1<span class="math notranslate nohighlight">\(,
where \)</span>\nabla f = \mathbf{J}_f^{!\top!}$.</p>
</section>
<section id="the-hessian">
<h2>The Hessian<a class="headerlink" href="#the-hessian" title="Link to this heading">#</a></h2>
<p>The <strong>Hessian</strong> matrix of <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span> is a matrix
of second-order partial derivatives: $<span class="math notranslate nohighlight">\(\nabla^2 f = \begin{bmatrix}
    \pdv[2]{f}{x_1} &amp; \hdots &amp; \pdv{f}{x_1}{x_d} \\
    \vdots &amp; \ddots &amp; \vdots \\
    \pdv{f}{x_d}{x_1} &amp; \hdots &amp; \pdv[2]{f}{x_d}\end{bmatrix}
\hspace{0.5cm}\text{i.e.}\hspace{0.5cm}
[\nabla^2 f]_{ij} = {\pdv{f}{x_i}{x_j}}\)</span>$ Recall that if the partial
derivatives are continuous, the order of differentiation can be
interchanged (Clairaut’s theorem), so the Hessian matrix will be
symmetric. This will typically be the case for differentiable functions
that we work with.</p>
<p>The Hessian is used in some optimization algorithms such as Newton’s
method. It is expensive to calculate but can drastically reduce the
number of iterations needed to converge to a local minimum by providing
information about the curvature of <span class="math notranslate nohighlight">\(f\)</span>.</p>
</section>
<section id="matrix-calculus">
<h2>Matrix calculus<a class="headerlink" href="#matrix-calculus" title="Link to this heading">#</a></h2>
<p>Since a lot of optimization reduces to finding points where the gradient
vanishes, it is useful to have differentiation rules for matrix and
vector expressions. We give some common rules here. Probably the two
most important for our purposes are $<span class="math notranslate nohighlight">\(\begin{aligned}
\nabla_\mathbf{x} &amp;(\mathbf{a}^{\!\top\!}\mathbf{x}) = \mathbf{a} \\
\nabla_\mathbf{x} &amp;(\mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}) = (\mathbf{A} + \mathbf{A}^{\!\top\!})\mathbf{x}
\end{aligned}\)</span><span class="math notranslate nohighlight">\( Note that this second rule is defined only if
\)</span>\mathbf{A}<span class="math notranslate nohighlight">\( is square. Furthermore, if \)</span>\mathbf{A}<span class="math notranslate nohighlight">\( is symmetric, we
can simplify the result to \)</span>2\mathbf{A}\mathbf{x}$.</p>
<section id="the-chain-rule">
<h3>The chain rule<a class="headerlink" href="#the-chain-rule" title="Link to this heading">#</a></h3>
<p>Most functions that we wish to optimize are not completely arbitrary
functions, but rather are composed of simpler functions which we know
how to handle. The chain rule gives us a way to calculate derivatives
for a composite function in terms of the derivatives of the simpler
functions that make it up.</p>
<p>The chain rule from single-variable calculus should be familiar:
$<span class="math notranslate nohighlight">\((f \circ g)'(x) = f'(g(x))g'(x)\)</span><span class="math notranslate nohighlight">\( where \)</span>\circ$ denotes function
composition. There is a natural generalization of this rule to
multivariate functions.</p>
<div class="proposition docutils">
<p>Suppose <span class="math notranslate nohighlight">\(f : \mathbb{R}^m \to \mathbb{R}^k\)</span> and
<span class="math notranslate nohighlight">\(g : \mathbb{R}^n \to \mathbb{R}^m\)</span>. Then
<span class="math notranslate nohighlight">\(f \circ g : \mathbb{R}^n \to \mathbb{R}^k\)</span> and
$<span class="math notranslate nohighlight">\(\mathbf{J}_{f \circ g}(\mathbf{x}) = \mathbf{J}_f(g(\mathbf{x}))\mathbf{J}_g(\mathbf{x})\)</span>$</p>
</div>
<p>In the special case <span class="math notranslate nohighlight">\(k = 1\)</span> we have the following corollary since
<span class="math notranslate nohighlight">\(\nabla f = \mathbf{J}_f^{\!\top\!}\)</span>.</p>
<div class="corollary docutils">
<p>Suppose <span class="math notranslate nohighlight">\(f : \mathbb{R}^m \to \mathbb{R}\)</span> and
<span class="math notranslate nohighlight">\(g : \mathbb{R}^n \to \mathbb{R}^m\)</span>. Then
<span class="math notranslate nohighlight">\(f \circ g : \mathbb{R}^n \to \mathbb{R}\)</span> and
$<span class="math notranslate nohighlight">\(\nabla (f \circ g)(\mathbf{x}) = \mathbf{J}_g(\mathbf{x})^{\!\top\!} \nabla f(g(\mathbf{x}))\)</span>$</p>
</div>
</section>
</section>
<section id="taylor-s-theorem">
<h2>Taylor’s theorem<a class="headerlink" href="#taylor-s-theorem" title="Link to this heading">#</a></h2>
<p>Taylor’s theorem has natural generalizations to functions of more than
one variable. We give the version presented in [&#64;numopt].</p>
<div class="theorem docutils">
<p>(Taylor’s theorem) Suppose <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span> is
continuously differentiable, and let <span class="math notranslate nohighlight">\(\mathbf{h} \in \mathbb{R}^d\)</span>. Then
there exists <span class="math notranslate nohighlight">\(t \in (0,1)\)</span> such that
$<span class="math notranslate nohighlight">\(f(\mathbf{x} + \mathbf{h}) = f(\mathbf{x}) + \nabla f(\mathbf{x} + t\mathbf{h})^{\!\top\!}\mathbf{h}\)</span><span class="math notranslate nohighlight">\(
Furthermore, if \)</span>f<span class="math notranslate nohighlight">\( is twice continuously differentiable, then
\)</span><span class="math notranslate nohighlight">\(\nabla f(\mathbf{x} + \mathbf{h}) = \nabla f(\mathbf{x}) + \int_0^1 \nabla^2 f(\mathbf{x} + t\mathbf{h})\mathbf{h} \dd{t}\)</span><span class="math notranslate nohighlight">\(
and there exists \)</span>t \in (0,1)<span class="math notranslate nohighlight">\( such that
\)</span><span class="math notranslate nohighlight">\(f(\mathbf{x} + \mathbf{h}) = f(\mathbf{x}) + \nabla f(\mathbf{x})^{\!\top\!}\mathbf{h} + \frac{1}{2}\mathbf{h}^{\!\top\!}\nabla^2f(\mathbf{x}+t\mathbf{h})\mathbf{h}\)</span>$</p>
</div>
<p>This theorem is used in proofs about conditions for local minima of
unconstrained optimization problems. Some of the most important results
are given in the next section.</p>
</section>
<section id="conditions-for-local-minima">
<h2>Conditions for local minima<a class="headerlink" href="#conditions-for-local-minima" title="Link to this heading">#</a></h2>
<div class="proposition docutils">
<p>If <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a local minimum of <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(f\)</span> is continuously
differentiable in a neighborhood of <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>, then
<span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}^*) = \mathbf{0}\)</span>.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Let <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> be a local minimum of <span class="math notranslate nohighlight">\(f\)</span>, and suppose
towards a contradiction that <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}^*) \neq \mathbf{0}\)</span>.
Let <span class="math notranslate nohighlight">\(\mathbf{h} = -\nabla f(\mathbf{x}^*)\)</span>, noting that by the
continuity of <span class="math notranslate nohighlight">\(\nabla f\)</span> we have
$<span class="math notranslate nohighlight">\(\lim_{t \to 0} -\nabla f(\mathbf{x}^* + t\mathbf{h}) = -\nabla f(\mathbf{x}^*) = \mathbf{h}\)</span><span class="math notranslate nohighlight">\(
Hence
\)</span><span class="math notranslate nohighlight">\(\lim_{t \to 0} \mathbf{h}^{\!\top\!}\nabla f(\mathbf{x}^* + t\mathbf{h}) = \mathbf{h}^{\!\top\!}\nabla f(\mathbf{x}^*) = -\|\mathbf{h}\|_2^2 &lt; 0\)</span><span class="math notranslate nohighlight">\(
Thus there exists \)</span>T &gt; 0<span class="math notranslate nohighlight">\( such that
\)</span>\mathbf{h}^{!\top!}\nabla f(\mathbf{x}^* + t\mathbf{h}) &lt; 0<span class="math notranslate nohighlight">\( for all
\)</span>t \in [0,T]<span class="math notranslate nohighlight">\(. Now we apply Taylor's theorem: for any \)</span>t \in (0,T]<span class="math notranslate nohighlight">\(,
there exists \)</span>t’ \in (0,t)<span class="math notranslate nohighlight">\( such that
\)</span><span class="math notranslate nohighlight">\(f(\mathbf{x}^* + t\mathbf{h}) = f(\mathbf{x}^*) + t\mathbf{h}^{\!\top\!} \nabla f(\mathbf{x}^* + t'\mathbf{h}) &lt; f(\mathbf{x}^*)\)</span><span class="math notranslate nohighlight">\(
whence it follows that \)</span>\mathbf{x}^<em><span class="math notranslate nohighlight">\( is not a local minimum, a
contradiction. Hence \)</span>\nabla f(\mathbf{x}^</em>) = \mathbf{0}$. ◻</p>
</div>
<p>The proof shows us why the vanishing gradient is necessary for an
extremum: if <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x})\)</span> is nonzero, there always exists a
sufficiently small step <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span> such that
<span class="math notranslate nohighlight">\(f(\mathbf{x} - \alpha\nabla f(\mathbf{x}))) &lt; f(\mathbf{x})\)</span>. For this
reason, <span class="math notranslate nohighlight">\(-\nabla f(\mathbf{x})\)</span> is called a <strong>descent direction</strong>.</p>
<p>Points where the gradient vanishes are called <strong>stationary points</strong>.
Note that not all stationary points are extrema. Consider
<span class="math notranslate nohighlight">\(f : \mathbb{R}^2 \to \mathbb{R}\)</span> given by <span class="math notranslate nohighlight">\(f(x,y) = x^2 - y^2\)</span>. We have
<span class="math notranslate nohighlight">\(\nabla f(\mathbf{0}) = \mathbf{0}\)</span>, but the point <span class="math notranslate nohighlight">\(\mathbf{0}\)</span> is the
minimum along the line <span class="math notranslate nohighlight">\(y = 0\)</span> and the maximum along the line <span class="math notranslate nohighlight">\(x = 0\)</span>.
Thus it is neither a local minimum nor a local maximum of <span class="math notranslate nohighlight">\(f\)</span>. Points
such as these, where the gradient vanishes but there is no local
extremum, are called <strong>saddle points</strong>.</p>
<p>We have seen that first-order information (i.e. the gradient) is
insufficient to characterize local minima. But we can say more with
second-order information (i.e. the Hessian). First we prove a necessary
second-order condition for local minima.</p>
<div class="proposition docutils">
<p>If <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a local minimum of <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(f\)</span> is twice
continuously differentiable in a neighborhood of <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>, then
<span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}^*)\)</span> is positive semi-definite.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Let <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> be a local minimum of <span class="math notranslate nohighlight">\(f\)</span>, and suppose
towards a contradiction that <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}^*)\)</span> is not positive
semi-definite. Let <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> be such that
<span class="math notranslate nohighlight">\(\mathbf{h}^{\!\top\!}\nabla^2 f(\mathbf{x}^*)\mathbf{h} &lt; 0\)</span>, noting
that by the continuity of <span class="math notranslate nohighlight">\(\nabla^2 f\)</span> we have
$<span class="math notranslate nohighlight">\(\lim_{t \to 0} \nabla^2 f(\mathbf{x}^* + t\mathbf{h}) = \nabla^2 f(\mathbf{x}^*)\)</span><span class="math notranslate nohighlight">\(
Hence
\)</span><span class="math notranslate nohighlight">\(\lim_{t \to 0} \mathbf{h}^{\!\top\!}\nabla^2 f(\mathbf{x}^* + t\mathbf{h})\mathbf{h} = \mathbf{h}^{\!\top\!}\nabla^2 f(\mathbf{x}^*)\mathbf{h} &lt; 0\)</span><span class="math notranslate nohighlight">\(
Thus there exists \)</span>T &gt; 0<span class="math notranslate nohighlight">\( such that
\)</span>\mathbf{h}^{!\top!}\nabla^2 f(\mathbf{x}^* + t\mathbf{h})\mathbf{h} &lt; 0<span class="math notranslate nohighlight">\(
for all \)</span>t \in [0,T]<span class="math notranslate nohighlight">\(. Now we apply Taylor's theorem: for any
\)</span>t \in (0,T]<span class="math notranslate nohighlight">\(, there exists \)</span>t’ \in (0,t)<span class="math notranslate nohighlight">\( such that
\)</span><span class="math notranslate nohighlight">\(f(\mathbf{x}^* + t\mathbf{h}) = f(\mathbf{x}^*) + \underbrace{t\mathbf{h}^{\!\top\!}\nabla f(\mathbf{x}^*)}_0 + \frac{1}{2}t^2\mathbf{h}^{\!\top\!}\nabla^2 f(\mathbf{x}^* + t'\mathbf{h})\mathbf{h} &lt; f(\mathbf{x}^*)\)</span><span class="math notranslate nohighlight">\(
where the middle term vanishes because
\)</span>\nabla f(\mathbf{x}^<em>) = \mathbf{0}<span class="math notranslate nohighlight">\( by the previous result. It follows
that \)</span>\mathbf{x}^</em><span class="math notranslate nohighlight">\( is not a local minimum, a contradiction. Hence
\)</span>\nabla^2 f(\mathbf{x}^*)$ is positive semi-definite. ◻</p>
</div>
<p>Now we give sufficient conditions for local minima.</p>
<div class="proposition docutils">
<p>Suppose <span class="math notranslate nohighlight">\(f\)</span> is twice continuously differentiable with <span class="math notranslate nohighlight">\(\nabla^2 f\)</span>
positive semi-definite in a neighborhood of <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>, and that
<span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}^*) = \mathbf{0}\)</span>. Then <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a local
minimum of <span class="math notranslate nohighlight">\(f\)</span>. Furthermore if <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}^*)\)</span> is positive
definite, then <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a strict local minimum.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Let <span class="math notranslate nohighlight">\(B\)</span> be an open ball of radius <span class="math notranslate nohighlight">\(r &gt; 0\)</span> centered at
<span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> which is contained in the neighborhood. Applying Taylor’s
theorem, we have that for any <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> with <span class="math notranslate nohighlight">\(\|\mathbf{h}\|_2 &lt; r\)</span>,
there exists <span class="math notranslate nohighlight">\(t \in (0,1)\)</span> such that
$<span class="math notranslate nohighlight">\(f(\mathbf{x}^* + \mathbf{h}) = f(\mathbf{x}^*) + \underbrace{\mathbf{h}^{\!\top\!}\nabla f(\mathbf{x}^*)}_0 + \frac{1}{2}\mathbf{h}^{\!\top\!}\nabla^2 f(\mathbf{x}^* + t\mathbf{h})\mathbf{h} \geq f(\mathbf{x}^*)\)</span><span class="math notranslate nohighlight">\(
The last inequality holds because
\)</span>\nabla^2 f(\mathbf{x}^* + t\mathbf{h})<span class="math notranslate nohighlight">\( is positive semi-definite
(since \)</span>|t\mathbf{h}|_2 = t|\mathbf{h}|_2 &lt; |\mathbf{h}|_2 &lt; r<span class="math notranslate nohighlight">\(),
so
\)</span>\mathbf{h}^{!\top!}\nabla^2 f(\mathbf{x}^* + t\mathbf{h})\mathbf{h} \geq 0<span class="math notranslate nohighlight">\(.
Since \)</span>f(\mathbf{x}^<em>) \leq f(\mathbf{x}^</em> + \mathbf{h})<span class="math notranslate nohighlight">\( for all
directions \)</span>\mathbf{h}<span class="math notranslate nohighlight">\( with \)</span>|\mathbf{h}|_2 &lt; r<span class="math notranslate nohighlight">\(, we conclude that
\)</span>\mathbf{x}^*$ is a local minimum.</p>
<p>Now further suppose that <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}^*)\)</span> is strictly positive
definite. Since the Hessian is continuous we can choose another ball
<span class="math notranslate nohighlight">\(B'\)</span> with radius <span class="math notranslate nohighlight">\(r' &gt; 0\)</span> centered at <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> such that
<span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x})\)</span> is positive definite for all
<span class="math notranslate nohighlight">\(\mathbf{x} \in B'\)</span>. Then following the same argument as above (except
with a strict inequality now since the Hessian is positive definite) we
have <span class="math notranslate nohighlight">\(f(\mathbf{x}^* + \mathbf{h}) &gt; f(\mathbf{x}^*)\)</span> for all
<span class="math notranslate nohighlight">\(\mathbf{h}\)</span> with <span class="math notranslate nohighlight">\(0 &lt; \|\mathbf{h}\|_2 &lt; r'\)</span>. Hence <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a
strict local minimum. ◻</p>
</div>
<p>Note that, perhaps counterintuitively, the conditions
<span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}^*) = \mathbf{0}\)</span> and <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}^*)\)</span>
positive semi-definite are not enough to guarantee a local minimum at
<span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>! Consider the function <span class="math notranslate nohighlight">\(f(x) = x^3\)</span>. We have <span class="math notranslate nohighlight">\(f'(0) = 0\)</span>
and <span class="math notranslate nohighlight">\(f''(0) = 0\)</span> (so the Hessian, which in this case is the <span class="math notranslate nohighlight">\(1 \times 1\)</span>
matrix <span class="math notranslate nohighlight">\(\begin{bmatrix}0\end{bmatrix}\)</span>, is positive semi-definite). But
<span class="math notranslate nohighlight">\(f\)</span> has a saddle point at <span class="math notranslate nohighlight">\(x = 0\)</span>. The function <span class="math notranslate nohighlight">\(f(x) = -x^4\)</span> is an even
worse offender – it has the same gradient and Hessian at <span class="math notranslate nohighlight">\(x = 0\)</span>, but
<span class="math notranslate nohighlight">\(x = 0\)</span> is a strict local maximum for this function!</p>
<p>For these reasons we require that the Hessian remains positive
semi-definite as long as we are close to <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>. Unfortunately,
this condition is not practical to check computationally, but in some
cases we can verify it analytically (usually by showing that
<span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x})\)</span> is p.s.d. for all
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span>). Also, if <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}^*)\)</span> is
strictly positive definite, the continuity assumption on <span class="math notranslate nohighlight">\(f\)</span> implies
this condition, so we don’t have to worry.</p>
</section>
<section id="convexity">
<h2>Convexity<a class="headerlink" href="#convexity" title="Link to this heading">#</a></h2>
<p><strong>Convexity</strong> is a term that pertains to both sets and functions. For
functions, there are different degrees of convexity, and how convex a
function is tells us a lot about its minima: do they exist, are they
unique, how quickly can we find them using optimization algorithms, etc.
In this section, we present basic results regarding convexity, strict
convexity, and strong convexity.</p>
<section id="convex-sets">
<h3>Convex sets<a class="headerlink" href="#convex-sets" title="Link to this heading">#</a></h3>
<figure id="fig:convexset">
<figure>
<img src="convex-set.png" />
<figcaption>A convex set</figcaption>
</figure>
<figure>
<img src="nonconvex-set.png" />
<figcaption>A non-convex set</figcaption>
</figure>
<figcaption>What convex sets look like</figcaption>
</figure>
<p>A set <span class="math notranslate nohighlight">\(\mathcal{X} \subseteq \mathbb{R}^d\)</span> is <strong>convex</strong> if
$<span class="math notranslate nohighlight">\(t\mathbf{x} + (1-t)\mathbf{y} \in \mathcal{X}\)</span><span class="math notranslate nohighlight">\( for all
\)</span>\mathbf{x}, \mathbf{y} \in \mathcal{X}<span class="math notranslate nohighlight">\( and all \)</span>t \in [0,1]$.</p>
<p>Geometrically, this means that all the points on the line segment
between any two points in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> are also in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. See
Figure <a class="reference internal" href="#fig:convexset"><span class="xref myst">1</span></a>{reference-type=”ref”
reference=”fig:convexset”} for a visual.</p>
<p>Why do we care whether or not a set is convex? We will see later that
the nature of minima can depend greatly on whether or not the feasible
set is convex. Undesirable pathological results can occur when we allow
the feasible set to be arbitrary, so for proofs we will need to assume
that it is convex. Fortunately, we often want to minimize over all of
<span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>, which is easily seen to be a convex set.</p>
</section>
<section id="basics-of-convex-functions">
<h3>Basics of convex functions<a class="headerlink" href="#basics-of-convex-functions" title="Link to this heading">#</a></h3>
<p>In the remainder of this section, assume
<span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span> unless otherwise noted. We’ll start
with the definitions and then give some results.</p>
<p>A function <span class="math notranslate nohighlight">\(f\)</span> is <strong>convex</strong> if
$<span class="math notranslate nohighlight">\(f(t\mathbf{x} + (1-t)\mathbf{y}) \leq t f(\mathbf{x}) + (1-t)f(\mathbf{y})\)</span><span class="math notranslate nohighlight">\(
for all \)</span>\mathbf{x}, \mathbf{y} \in \dom f<span class="math notranslate nohighlight">\( and all \)</span>t \in [0,1]$.</p>
<p>If the inequality holds strictly (i.e. <span class="math notranslate nohighlight">\(&lt;\)</span> rather than <span class="math notranslate nohighlight">\(\leq\)</span>) for all
<span class="math notranslate nohighlight">\(t \in (0,1)\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{y}\)</span>, then we say that <span class="math notranslate nohighlight">\(f\)</span> is
<strong>strictly convex</strong>.</p>
<p>A function <span class="math notranslate nohighlight">\(f\)</span> is <strong>strongly convex with parameter <span class="math notranslate nohighlight">\(m\)</span></strong> (or
<strong><span class="math notranslate nohighlight">\(m\)</span>-strongly convex</strong>) if the function
$<span class="math notranslate nohighlight">\(\mathbf{x} \mapsto f(\mathbf{x}) - \frac{m}{2}\|\mathbf{x}\|_2^2\)</span>$ is
convex.</p>
<p>These conditions are given in increasing order of strength; strong
convexity implies strict convexity which implies convexity.</p>
<p><img alt="What convex functions looklike" src="chapter_linear_algebra/convex-function.png" />{#fig:convexfunction width=”\linewidth”}</p>
<p>Geometrically, convexity means that the line segment between two points
on the graph of <span class="math notranslate nohighlight">\(f\)</span> lies on or above the graph itself. See Figure
<a class="reference internal" href="#fig:convexfunction"><span class="xref myst">2</span></a>{reference-type=”ref”
reference=”fig:convexfunction”} for a visual.</p>
<p>Strict convexity means that the graph of <span class="math notranslate nohighlight">\(f\)</span> lies strictly above the
line segment, except at the segment endpoints. (So actually the function
in the figure appears to be strictly convex.)</p>
</section>
<section id="consequences-of-convexity">
<h3>Consequences of convexity<a class="headerlink" href="#consequences-of-convexity" title="Link to this heading">#</a></h3>
<p>Why do we care if a function is (strictly/strongly) convex?</p>
<p>Basically, our various notions of convexity have implications about the
nature of minima. It should not be surprising that the stronger
conditions tell us more about the minima.</p>
<div class="proposition docutils">
<p>Let <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> be a convex set. If <span class="math notranslate nohighlight">\(f\)</span> is convex, then any local
minimum of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is also a global minimum.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Suppose <span class="math notranslate nohighlight">\(f\)</span> is convex, and let <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> be a local
minimum of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. Then for some neighborhood
<span class="math notranslate nohighlight">\(N \subseteq \mathcal{X}\)</span> about <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>, we have
<span class="math notranslate nohighlight">\(f(\mathbf{x}) \geq f(\mathbf{x}^*)\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x} \in N\)</span>. Suppose
towards a contradiction that there exists
<span class="math notranslate nohighlight">\(\tilde{\mathbf{x}} \in \mathcal{X}\)</span> such that
<span class="math notranslate nohighlight">\(f(\tilde{\mathbf{x}}) &lt; f(\mathbf{x}^*)\)</span>.</p>
<p>Consider the line segment
<span class="math notranslate nohighlight">\(\mathbf{x}(t) = t\mathbf{x}^* + (1-t)\tilde{\mathbf{x}}, ~ t \in [0,1]\)</span>,
noting that <span class="math notranslate nohighlight">\(\mathbf{x}(t) \in \mathcal{X}\)</span> by the convexity of
<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. Then by the convexity of <span class="math notranslate nohighlight">\(f\)</span>,
$<span class="math notranslate nohighlight">\(f(\mathbf{x}(t)) \leq tf(\mathbf{x}^*) + (1-t)f(\tilde{\mathbf{x}}) &lt; tf(\mathbf{x}^*) + (1-t)f(\mathbf{x}^*) = f(\mathbf{x}^*)\)</span><span class="math notranslate nohighlight">\(
for all \)</span>t \in (0,1)$.</p>
<p>We can pick <span class="math notranslate nohighlight">\(t\)</span> to be sufficiently close to <span class="math notranslate nohighlight">\(1\)</span> that
<span class="math notranslate nohighlight">\(\mathbf{x}(t) \in N\)</span>; then <span class="math notranslate nohighlight">\(f(\mathbf{x}(t)) \geq f(\mathbf{x}^*)\)</span> by
the definition of <span class="math notranslate nohighlight">\(N\)</span>, but <span class="math notranslate nohighlight">\(f(\mathbf{x}(t)) &lt; f(\mathbf{x}^*)\)</span> by the
above inequality, a contradiction.</p>
<p>It follows that <span class="math notranslate nohighlight">\(f(\mathbf{x}^*) \leq f(\mathbf{x})\)</span> for all
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}\)</span>, so <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a global minimum of
<span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. ◻</p>
</div>
<div class="proposition docutils">
<p>Let <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> be a convex set. If <span class="math notranslate nohighlight">\(f\)</span> is strictly convex, then there
exists at most one local minimum of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. Consequently,
if it exists it is the unique global minimum of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> The second sentence follows from the first, so all we must show
is that if a local minimum exists in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> then it is unique.</p>
<p>Suppose <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a local minimum of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, and
suppose towards a contradiction that there exists a local minimum
<span class="math notranslate nohighlight">\(\tilde{\mathbf{x}} \in \mathcal{X}\)</span> such that
<span class="math notranslate nohighlight">\(\tilde{\mathbf{x}} \neq \mathbf{x}^*\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(f\)</span> is strictly convex, it is convex, so <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> and
<span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}\)</span> are both global minima of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> by
the previous result. Hence <span class="math notranslate nohighlight">\(f(\mathbf{x}^*) = f(\tilde{\mathbf{x}})\)</span>.
Consider the line segment
<span class="math notranslate nohighlight">\(\mathbf{x}(t) = t\mathbf{x}^* + (1-t)\tilde{\mathbf{x}}, ~ t \in [0,1]\)</span>,
which again must lie entirely in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. By the strict convexity
of <span class="math notranslate nohighlight">\(f\)</span>,
$<span class="math notranslate nohighlight">\(f(\mathbf{x}(t)) &lt; tf(\mathbf{x}^*) + (1-t)f(\tilde{\mathbf{x}}) = tf(\mathbf{x}^*) + (1-t)f(\mathbf{x}^*) = f(\mathbf{x}^*)\)</span><span class="math notranslate nohighlight">\(
for all \)</span>t \in (0,1)<span class="math notranslate nohighlight">\(. But this contradicts the fact that \)</span>\mathbf{x}^<em><span class="math notranslate nohighlight">\(
is a global minimum. Therefore if \)</span>\tilde{\mathbf{x}}<span class="math notranslate nohighlight">\( is a local
minimum of \)</span>f<span class="math notranslate nohighlight">\( in \)</span>\mathcal{X}<span class="math notranslate nohighlight">\(, then
\)</span>\tilde{\mathbf{x}} = \mathbf{x}^</em><span class="math notranslate nohighlight">\(, so \)</span>\mathbf{x}^*<span class="math notranslate nohighlight">\( is the unique
minimum in \)</span>\mathcal{X}$. ◻</p>
</div>
<p>It is worthwhile to examine how the feasible set affects the
optimization problem. We will see why the assumption that <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>
is convex is needed in the results above.</p>
<p>Consider the function <span class="math notranslate nohighlight">\(f(x) = x^2\)</span>, which is a strictly convex function.
The unique global minimum of this function in <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> is <span class="math notranslate nohighlight">\(x = 0\)</span>.
But let’s see what happens when we change the feasible set
<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>.</p>
<p>(i) <span class="math notranslate nohighlight">\(\mathcal{X} = \{1\}\)</span>: This set is actually convex, so we still have
a unique global minimum. But it is not the same as the unconstrained
minimum!</p>
<p>(ii) <span class="math notranslate nohighlight">\(\mathcal{X} = \mathbb{R} \setminus \{0\}\)</span>: This set is non-convex,
and we can see that <span class="math notranslate nohighlight">\(f\)</span> has no minima in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. For any
point <span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span>, one can find another point
<span class="math notranslate nohighlight">\(y \in \mathcal{X}\)</span> such that <span class="math notranslate nohighlight">\(f(y) &lt; f(x)\)</span>.</p>
<p>(iii) <span class="math notranslate nohighlight">\(\mathcal{X} = (-\infty,-1] \cup [0,\infty)\)</span>: This set is
non-convex, and we can see that there is a local minimum
(<span class="math notranslate nohighlight">\(x = -1\)</span>) which is distinct from the global minimum (<span class="math notranslate nohighlight">\(x = 0\)</span>).</p>
<p>(iv) <span class="math notranslate nohighlight">\(\mathcal{X} = (-\infty,-1] \cup [1,\infty)\)</span>: This set is
non-convex, and we can see that there are two global minima
(<span class="math notranslate nohighlight">\(x = \pm 1\)</span>).</p>
</section>
<section id="showing-that-a-function-is-convex">
<h3>Showing that a function is convex<a class="headerlink" href="#showing-that-a-function-is-convex" title="Link to this heading">#</a></h3>
<p>Hopefully the previous section has convinced the reader that convexity
is an important property. Next we turn to the issue of showing that a
function is (strictly/strongly) convex. It is of course possible (in
principle) to directly show that the condition in the definition holds,
but this is usually not the easiest way.</p>
<div class="proposition docutils">
<p>Norms are convex.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Let <span class="math notranslate nohighlight">\(\|\cdot\|\)</span> be a norm on a vector space <span class="math notranslate nohighlight">\(V\)</span>. Then for all
<span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in V\)</span> and <span class="math notranslate nohighlight">\(t \in [0,1]\)</span>,
$<span class="math notranslate nohighlight">\(\|t\mathbf{x} + (1-t)\mathbf{y}\| \leq \|t\mathbf{x}\| + \|(1-t)\mathbf{y}\| = |t|\|\mathbf{x}\| + |1-t|\|\mathbf{y}\| = t\|\mathbf{x}\| + (1-t)\|\mathbf{y}\|\)</span><span class="math notranslate nohighlight">\(
where we have used respectively the triangle inequality, the homogeneity
of norms, and the fact that \)</span>t<span class="math notranslate nohighlight">\( and \)</span>1-t<span class="math notranslate nohighlight">\( are nonnegative. Hence
\)</span>|\cdot|$ is convex. ◻</p>
</div>
<div class="proposition docutils">
<p>Suppose <span class="math notranslate nohighlight">\(f\)</span> is differentiable. Then <span class="math notranslate nohighlight">\(f\)</span> is convex if and only if
$<span class="math notranslate nohighlight">\(f(\mathbf{y}) \geq f(\mathbf{x}) + \langle \nabla f(\mathbf{x}), \mathbf{y} - \mathbf{x} \rangle\)</span><span class="math notranslate nohighlight">\(
for all \)</span>\mathbf{x}, \mathbf{y} \in \dom f$.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> To-do. ◻</p>
</div>
<div class="proposition docutils">
<p>Suppose <span class="math notranslate nohighlight">\(f\)</span> is twice differentiable. Then</p>
<p>(i) <span class="math notranslate nohighlight">\(f\)</span> is convex if and only if <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}) \succeq 0\)</span> for
all <span class="math notranslate nohighlight">\(\mathbf{x} \in \dom f\)</span>.</p>
<p>(ii) If <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}) \succ 0\)</span> for all
<span class="math notranslate nohighlight">\(\mathbf{x} \in \dom f\)</span>, then <span class="math notranslate nohighlight">\(f\)</span> is strictly convex.</p>
<p>(iii) <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(m\)</span>-strongly convex if and only if
<span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}) \succeq mI\)</span> for all
<span class="math notranslate nohighlight">\(\mathbf{x} \in \dom f\)</span>.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Omitted. ◻</p>
</div>
<div class="proposition docutils">
<p>If <span class="math notranslate nohighlight">\(f\)</span> is convex and <span class="math notranslate nohighlight">\(\alpha \geq 0\)</span>, then <span class="math notranslate nohighlight">\(\alpha f\)</span> is convex.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Suppose <span class="math notranslate nohighlight">\(f\)</span> is convex and <span class="math notranslate nohighlight">\(\alpha \geq 0\)</span>. Then for all
<span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \dom(\alpha f) = \dom f\)</span>, $<span class="math notranslate nohighlight">\(\begin{aligned}
(\alpha f)(t\mathbf{x} + (1-t)\mathbf{y}) &amp;= \alpha f(t\mathbf{x} + (1-t)\mathbf{y}) \\
&amp;\leq \alpha\left(tf(\mathbf{x}) + (1-t)f(\mathbf{y})\right) \\
&amp;= t(\alpha f(\mathbf{x})) + (1-t)(\alpha f(\mathbf{y})) \\
&amp;= t(\alpha f)(\mathbf{x}) + (1-t)(\alpha f)(\mathbf{y})
\end{aligned}\)</span><span class="math notranslate nohighlight">\( so \)</span>\alpha f$ is convex. ◻</p>
</div>
<div class="proposition docutils">
<p>If <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are convex, then <span class="math notranslate nohighlight">\(f+g\)</span> is convex. Furthermore, if <span class="math notranslate nohighlight">\(g\)</span> is
strictly convex, then <span class="math notranslate nohighlight">\(f+g\)</span> is strictly convex, and if <span class="math notranslate nohighlight">\(g\)</span> is
<span class="math notranslate nohighlight">\(m\)</span>-strongly convex, then <span class="math notranslate nohighlight">\(f+g\)</span> is <span class="math notranslate nohighlight">\(m\)</span>-strongly convex.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Suppose <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are convex. Then for all
<span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \dom (f+g) = \dom f \cap \dom g\)</span>,
$<span class="math notranslate nohighlight">\(\begin{aligned}
(f+g)(t\mathbf{x} + (1-t)\mathbf{y}) &amp;= f(t\mathbf{x} + (1-t)\mathbf{y}) + g(t\mathbf{x} + (1-t)\mathbf{y}) \\
&amp;\leq tf(\mathbf{x}) + (1-t)f(\mathbf{y}) + g(t\mathbf{x} + (1-t)\mathbf{y}) &amp; \text{convexity of \)</span>f<span class="math notranslate nohighlight">\(} \\
&amp;\leq tf(\mathbf{x}) + (1-t)f(\mathbf{y}) + tg(\mathbf{x}) + (1-t)g(\mathbf{y}) &amp; \text{convexity of \)</span>g<span class="math notranslate nohighlight">\(} \\
&amp;= t(f(\mathbf{x}) + g(\mathbf{x})) + (1-t)(f(\mathbf{y}) + g(\mathbf{y})) \\
&amp;= t(f+g)(\mathbf{x}) + (1-t)(f+g)(\mathbf{y})
\end{aligned}\)</span><span class="math notranslate nohighlight">\( so \)</span>f + g$ is convex.</p>
<p>If <span class="math notranslate nohighlight">\(g\)</span> is strictly convex, the second inequality above holds strictly
for <span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{y}\)</span> and <span class="math notranslate nohighlight">\(t \in (0,1)\)</span>, so <span class="math notranslate nohighlight">\(f+g\)</span> is strictly
convex.</p>
<p>If <span class="math notranslate nohighlight">\(g\)</span> is <span class="math notranslate nohighlight">\(m\)</span>-strongly convex, then the function
<span class="math notranslate nohighlight">\(h(\mathbf{x}) \equiv g(\mathbf{x}) - \frac{m}{2}\|\mathbf{x}\|_2^2\)</span> is
convex, so <span class="math notranslate nohighlight">\(f+h\)</span> is convex. But
$<span class="math notranslate nohighlight">\((f+h)(\mathbf{x}) \equiv f(\mathbf{x}) + h(\mathbf{x}) \equiv f(\mathbf{x}) + g(\mathbf{x}) - \frac{m}{2}\|\mathbf{x}\|_2^2 \equiv (f+g)(\mathbf{x}) - \frac{m}{2}\|\mathbf{x}\|_2^2\)</span><span class="math notranslate nohighlight">\(
so \)</span>f+g<span class="math notranslate nohighlight">\( is \)</span>m$-strongly convex. ◻</p>
</div>
<div class="proposition docutils">
<p>If <span class="math notranslate nohighlight">\(f_1, \dots, f_n\)</span> are convex and <span class="math notranslate nohighlight">\(\alpha_1, \dots, \alpha_n \geq 0\)</span>,
then $<span class="math notranslate nohighlight">\(\sum_{i=1}^n \alpha_i f_i\)</span>$ is convex.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Follows from the previous two propositions by induction. ◻</p>
</div>
<div class="proposition docutils">
<p>If <span class="math notranslate nohighlight">\(f\)</span> is convex, then
<span class="math notranslate nohighlight">\(g(\mathbf{x}) \equiv f(\mathbf{A}\mathbf{x} + \mathbf{b})\)</span> is convex
for any appropriately-sized <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Suppose <span class="math notranslate nohighlight">\(f\)</span> is convex and <span class="math notranslate nohighlight">\(g\)</span> is defined like so. Then for all
<span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \dom g\)</span>, $<span class="math notranslate nohighlight">\(\begin{aligned}
g(t\mathbf{x} + (1-t)\mathbf{y}) &amp;= f(\mathbf{A}(t\mathbf{x} + (1-t)\mathbf{y}) + \mathbf{b}) \\
&amp;= f(t\mathbf{A}\mathbf{x} + (1-t)\mathbf{A}\mathbf{y} + \mathbf{b}) \\
&amp;= f(t\mathbf{A}\mathbf{x} + (1-t)\mathbf{A}\mathbf{y} + t\mathbf{b} + (1-t)\mathbf{b}) \\
&amp;= f(t(\mathbf{A}\mathbf{x} + \mathbf{b}) + (1-t)(\mathbf{A}\mathbf{y} + \mathbf{b})) \\
&amp;\leq tf(\mathbf{A}\mathbf{x} + \mathbf{b}) + (1-t)f(\mathbf{A}\mathbf{y} + \mathbf{b}) &amp; \text{convexity of \)</span>f<span class="math notranslate nohighlight">\(} \\
&amp;= tg(\mathbf{x}) + (1-t)g(\mathbf{y})
\end{aligned}\)</span><span class="math notranslate nohighlight">\( Thus \)</span>g$ is convex. ◻</p>
</div>
<div class="proposition docutils">
<p>If <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are convex, then
<span class="math notranslate nohighlight">\(h(\mathbf{x}) \equiv \max\{f(\mathbf{x}), g(\mathbf{x})\}\)</span> is convex.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Suppose <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are convex and <span class="math notranslate nohighlight">\(h\)</span> is defined like so. Then
for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \dom h\)</span>, $<span class="math notranslate nohighlight">\(\begin{aligned}
h(t\mathbf{x} + (1-t)\mathbf{y}) &amp;= \max\{f(t\mathbf{x} + (1-t)\mathbf{y}), g(t\mathbf{x} + (1-t)\mathbf{y})\} \\
&amp;\leq \max\{tf(\mathbf{x}) + (1-t)f(\mathbf{y}), tg(\mathbf{x}) + (1-t)g(\mathbf{y})\} \\
&amp;\leq \max\{tf(\mathbf{x}), tg(\mathbf{x})\} + \max\{(1-t)f(\mathbf{y}), (1-t)g(\mathbf{y})\} \\
&amp;= t\max\{f(\mathbf{x}), g(\mathbf{x})\} + (1-t)\max\{f(\mathbf{y}), g(\mathbf{y})\} \\
&amp;= th(\mathbf{x}) + (1-t)h(\mathbf{y})
\end{aligned}\)</span><span class="math notranslate nohighlight">\( Note that in the first inequality we have used convexity
of \)</span>f<span class="math notranslate nohighlight">\( and \)</span>g<span class="math notranslate nohighlight">\( plus the fact that \)</span>a \leq c, b \leq d<span class="math notranslate nohighlight">\( implies
\)</span>\max{a,b} \leq \max{c,d}<span class="math notranslate nohighlight">\(. In the second inequality we have used
the fact that \)</span>\max{a+b, c+d} \leq \max{a,c} + \max{b,d}$.</p>
<p>Thus <span class="math notranslate nohighlight">\(h\)</span> is convex. ◻</p>
</div>
</section>
<section id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Link to this heading">#</a></h3>
<p>A good way to gain intuition about the distinction between convex,
strictly convex, and strongly convex functions is to consider examples
where the stronger property fails to hold.</p>
<p>Functions that are convex but not strictly convex:</p>
<p>(i) <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \mathbf{w}^{\!\top\!}\mathbf{x} + \alpha\)</span> for any
<span class="math notranslate nohighlight">\(\mathbf{w} \in \mathbb{R}^d, \alpha \in \mathbb{R}\)</span>. Such a
function is called an <strong>affine function</strong>, and it is both convex and
concave. (In fact, a function is affine if and only if it is both
convex and concave.) Note that linear functions and constant
functions are special cases of affine functions.</p>
<p>(ii) <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \|\mathbf{x}\|_1\)</span></p>
<p>Functions that are strictly but not strongly convex:</p>
<p>(i) <span class="math notranslate nohighlight">\(f(x) = x^4\)</span>. This example is interesting because it is strictly
convex but you cannot show this fact via a second-order argument
(since <span class="math notranslate nohighlight">\(f''(0) = 0\)</span>).</p>
<p>(ii) <span class="math notranslate nohighlight">\(f(x) = \exp(x)\)</span>. This example is interesting because it’s bounded
below but has no local minimum.</p>
<p>(iii) <span class="math notranslate nohighlight">\(f(x) = -\log x\)</span>. This example is interesting because it’s
strictly convex but not bounded below.</p>
<p>Functions that are strongly convex:</p>
<p>(i) <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \|\mathbf{x}\|_2^2\)</span></p>
</section>
</section>
<section id="orthogonal-projections">
<h2>Orthogonal projections<a class="headerlink" href="#orthogonal-projections" title="Link to this heading">#</a></h2>
<p>We now consider a particular kind of optimization problem that is
particularly well-understood and can often be solved in closed form:
given some point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> in an inner product space <span class="math notranslate nohighlight">\(V\)</span>, find the
closest point to <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> in a subspace <span class="math notranslate nohighlight">\(S\)</span> of <span class="math notranslate nohighlight">\(V\)</span>. This process is
referred to as <strong>projection onto a subspace</strong>.</p>
<p>The following diagram should make it geometrically clear that, at least
in Euclidean space, the solution is intimately related to orthogonality
and the Pythagorean theorem:</p>
<div class="center docutils">
<p><img alt="image" src="chapter_linear_algebra/orthogonal-projection.png" />{width=”50%”}</p>
</div>
<p>Here <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is an arbitrary element of the subspace <span class="math notranslate nohighlight">\(S\)</span>, and
<span class="math notranslate nohighlight">\(\mathbf{y}^*\)</span> is the point in <span class="math notranslate nohighlight">\(S\)</span> such that <span class="math notranslate nohighlight">\(\mathbf{x}-\mathbf{y}^*\)</span>
is perpendicular to <span class="math notranslate nohighlight">\(S\)</span>. The hypotenuse of a right triangle (in this
case <span class="math notranslate nohighlight">\(\|\mathbf{x}-\mathbf{y}\|\)</span>) is always longer than either of the
legs (in this case <span class="math notranslate nohighlight">\(\|\mathbf{x}-\mathbf{y}^*\|\)</span> and
<span class="math notranslate nohighlight">\(\|\mathbf{y}^*-\mathbf{y}\|\)</span>), and when <span class="math notranslate nohighlight">\(\mathbf{y} \neq \mathbf{y}^*\)</span>
there always exists such a triangle between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>,
and <span class="math notranslate nohighlight">\(\mathbf{y}^*\)</span>.</p>
<p>Our intuition from Euclidean space suggests that the closest point to
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> in <span class="math notranslate nohighlight">\(S\)</span> has the perpendicularity property described above,
and we now show that this is indeed the case.</p>
<div class="proposition docutils">
<p>Suppose <span class="math notranslate nohighlight">\(\mathbf{x} \in V\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y} \in S\)</span>. Then <span class="math notranslate nohighlight">\(\mathbf{y}^*\)</span>
is the unique minimizer of <span class="math notranslate nohighlight">\(\|\mathbf{x}-\mathbf{y}\|\)</span> over
<span class="math notranslate nohighlight">\(\mathbf{y} \in S\)</span> if and only if <span class="math notranslate nohighlight">\(\mathbf{x}-\mathbf{y}^* \perp S\)</span>.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> <span class="math notranslate nohighlight">\((\implies)\)</span> Suppose <span class="math notranslate nohighlight">\(\mathbf{y}^*\)</span> is the unique minimizer of
<span class="math notranslate nohighlight">\(\|\mathbf{x}-\mathbf{y}\|\)</span> over <span class="math notranslate nohighlight">\(\mathbf{y} \in S\)</span>. That is,
<span class="math notranslate nohighlight">\(\|\mathbf{x}-\mathbf{y}^*\| \leq \|\mathbf{x}-\mathbf{y}\|\)</span> for all
<span class="math notranslate nohighlight">\(\mathbf{y} \in S\)</span>, with equality only if <span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{y}^*\)</span>.
Fix <span class="math notranslate nohighlight">\(\mathbf{v} \in S\)</span> and observe that $<span class="math notranslate nohighlight">\(\begin{aligned}
g(t) &amp;:= \|\mathbf{x}-\mathbf{y}^*+t\mathbf{v}\|^2 \\
&amp;= \langle \mathbf{x}-\mathbf{y}^*+t\mathbf{v}, \mathbf{x}-\mathbf{y}^*+t\mathbf{v} \rangle \\
&amp;= \langle \mathbf{x}-\mathbf{y}^*, \mathbf{x}-\mathbf{y}^* \rangle - 2t\langle \mathbf{x}-\mathbf{y}^*, \mathbf{v} \rangle + t^2\langle \mathbf{v}, \mathbf{v} \rangle \\
&amp;= \|\mathbf{x}-\mathbf{y}^*\|^2 - 2t\langle \mathbf{x}-\mathbf{y}^*, \mathbf{v} \rangle + t^2\|\mathbf{v}\|^2
\end{aligned}\)</span><span class="math notranslate nohighlight">\( must have a minimum at \)</span>t = 0<span class="math notranslate nohighlight">\( as a consequence of this
assumption. Thus
\)</span><span class="math notranslate nohighlight">\(0 = g'(0) = \left.-2\langle \mathbf{x}-\mathbf{y}^*, \mathbf{v} \rangle + 2t\|\mathbf{v}\|^2\right|_{t=0} = -2\langle \mathbf{x}-\mathbf{y}^*, \mathbf{v} \rangle\)</span><span class="math notranslate nohighlight">\(
giving \)</span>\mathbf{x}-\mathbf{y}^* \perp \mathbf{v}<span class="math notranslate nohighlight">\(. Since \)</span>\mathbf{v}<span class="math notranslate nohighlight">\(
was arbitrary in \)</span>S<span class="math notranslate nohighlight">\(, we have \)</span>\mathbf{x}-\mathbf{y}^* \perp S$ as
claimed.</p>
<p><span class="math notranslate nohighlight">\((\impliedby)\)</span> Suppose <span class="math notranslate nohighlight">\(\mathbf{x}-\mathbf{y}^* \perp S\)</span>. Observe that
for any <span class="math notranslate nohighlight">\(\mathbf{y} \in S\)</span>, <span class="math notranslate nohighlight">\(\mathbf{y}^*-\mathbf{y} \in S\)</span> because
<span class="math notranslate nohighlight">\(\mathbf{y}^* \in S\)</span> and <span class="math notranslate nohighlight">\(S\)</span> is closed under subtraction. Under the
hypothesis, <span class="math notranslate nohighlight">\(\mathbf{x}-\mathbf{y}^* \perp \mathbf{y}^*-\mathbf{y}\)</span>, so
by the Pythagorean theorem,
$<span class="math notranslate nohighlight">\(\|\mathbf{x}-\mathbf{y}\| = \|\mathbf{x}-\mathbf{y}^*+\mathbf{y}^*-\mathbf{y}\| = \|\mathbf{x}-\mathbf{y}^*\| + \|\mathbf{y}^*-\mathbf{y}\| \geq \|\mathbf{x} - \mathbf{y}^*\|\)</span><span class="math notranslate nohighlight">\(
and in fact the inequality is strict when \)</span>\mathbf{y} \neq \mathbf{y}^<em><span class="math notranslate nohighlight">\(
since this implies \)</span>|\mathbf{y}^</em>-\mathbf{y}| &gt; 0<span class="math notranslate nohighlight">\(. Thus
\)</span>\mathbf{y}^*<span class="math notranslate nohighlight">\( is the unique minimizer of \)</span>|\mathbf{x}-\mathbf{y}|<span class="math notranslate nohighlight">\(
over \)</span>\mathbf{y} \in S$. ◻</p>
</div>
<p>Since a unique minimizer in <span class="math notranslate nohighlight">\(S\)</span> can be found for any <span class="math notranslate nohighlight">\(\mathbf{x} \in V\)</span>,
we can define an operator
$<span class="math notranslate nohighlight">\(P\mathbf{x} = \argmin_{\mathbf{y} \in S} \|\mathbf{x}-\mathbf{y}\|\)</span><span class="math notranslate nohighlight">\(
Observe that \)</span>P\mathbf{y} = \mathbf{y}<span class="math notranslate nohighlight">\( for any \)</span>\mathbf{y} \in S<span class="math notranslate nohighlight">\(,
since \)</span>\mathbf{y}<span class="math notranslate nohighlight">\( has distance zero from itself and every other point
in \)</span>S<span class="math notranslate nohighlight">\( has positive distance from \)</span>\mathbf{y}<span class="math notranslate nohighlight">\(. Thus
\)</span>P(P\mathbf{x}) = P\mathbf{x}<span class="math notranslate nohighlight">\( for any \)</span>\mathbf{x}<span class="math notranslate nohighlight">\( (i.e., \)</span>P^2 = P<span class="math notranslate nohighlight">\()
because \)</span>P\mathbf{x} \in S<span class="math notranslate nohighlight">\(. The identity \)</span>P^2 = P$ is actually one of
the defining properties of a <strong>projection</strong>, the other being linearity.</p>
<p>An immediate consequence of the previous result is that
<span class="math notranslate nohighlight">\(\mathbf{x} - P\mathbf{x} \perp S\)</span> for any <span class="math notranslate nohighlight">\(\mathbf{x} \in V\)</span>, and
conversely that <span class="math notranslate nohighlight">\(P\)</span> is the unique operator that satisfies this property
for all <span class="math notranslate nohighlight">\(\mathbf{x} \in V\)</span>. For this reason, <span class="math notranslate nohighlight">\(P\)</span> is known as an
<strong>orthogonal projection</strong>.</p>
<p>If we choose an orthonormal basis for the target subspace <span class="math notranslate nohighlight">\(S\)</span>, it is
possible to write down a more specific expression for <span class="math notranslate nohighlight">\(P\)</span>.</p>
<div class="proposition docutils">
<p>If <span class="math notranslate nohighlight">\(\mathbf{e}_1, \dots, \mathbf{e}_m\)</span> is an orthonormal basis for <span class="math notranslate nohighlight">\(S\)</span>,
then
$<span class="math notranslate nohighlight">\(P\mathbf{x} = \sum_{i=1}^m \langle \mathbf{x}, \mathbf{e}_i \rangle\mathbf{e}_i\)</span>$</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> Let <span class="math notranslate nohighlight">\(\mathbf{e}_1, \dots, \mathbf{e}_m\)</span> be an orthonormal basis
for <span class="math notranslate nohighlight">\(S\)</span>, and suppose <span class="math notranslate nohighlight">\(\mathbf{x} \in V\)</span>. Then for all <span class="math notranslate nohighlight">\(j = 1, \dots, m\)</span>,
$<span class="math notranslate nohighlight">\(\begin{aligned}
\left\langle \mathbf{x}-\sum_{i=1}^m \langle \mathbf{x}, \mathbf{e}_i \rangle\mathbf{e}_i, \mathbf{e}_j \right\rangle &amp;= \langle \mathbf{x}, \mathbf{e}_j \rangle - \sum_{i=1}^m \langle \mathbf{x}, \mathbf{e}_i \rangle\underbrace{\langle \mathbf{e}_i, \mathbf{e}_j \rangle}_{\delta_{ij}} \\
&amp;= \langle \mathbf{x}, \mathbf{e}_j \rangle - \langle \mathbf{x}, \mathbf{e}_j \rangle \\
&amp;= 0
\end{aligned}\)</span><span class="math notranslate nohighlight">\( We have shown that the claimed expression, call it
\)</span>\tilde{P}\mathbf{x}<span class="math notranslate nohighlight">\(, satisfies
\)</span>\mathbf{x} - \tilde{P}\mathbf{x} \perp \mathbf{e}_j<span class="math notranslate nohighlight">\( for every element
\)</span>\mathbf{e}_j<span class="math notranslate nohighlight">\( of the orthonormal basis for \)</span>S<span class="math notranslate nohighlight">\(. It follows (by
linearity of the inner product) that
\)</span>\mathbf{x} - \tilde{P}\mathbf{x} \perp S<span class="math notranslate nohighlight">\(, so the previous result
implies \)</span>P = \tilde{P}$. ◻</p>
</div>
<p>The fact that <span class="math notranslate nohighlight">\(P\)</span> is a linear operator (and thus a proper projection, as
earlier we showed <span class="math notranslate nohighlight">\(P^2 = P\)</span>) follows readily from this result.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="probability">
<h1>Probability<a class="headerlink" href="#probability" title="Link to this heading">#</a></h1>
<p>Probability theory provides powerful tools for modeling and dealing with
uncertainty.</p>
<section id="basics">
<h2>Basics<a class="headerlink" href="#basics" title="Link to this heading">#</a></h2>
<p>Suppose we have some sort of randomized experiment (e.g. a coin toss,
die roll) that has a fixed set of possible outcomes. This set is called
the <strong>sample space</strong> and denoted <span class="math notranslate nohighlight">\(\Omega\)</span>.</p>
<p>We would like to define probabilities for some <strong>events</strong>, which are
subsets of <span class="math notranslate nohighlight">\(\Omega\)</span>. The set of events is denoted <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>.<a class="footnote-reference brackets" href="#id21" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> The
<strong>complement</strong> of the event <span class="math notranslate nohighlight">\(A\)</span> is another event,
<span class="math notranslate nohighlight">\(A^\text{c} = \Omega \setminus A\)</span>.</p>
<p>Then we can define a <strong>probability measure</strong>
<span class="math notranslate nohighlight">\(\mathbb{P} : \mathcal{F} \to [0,1]\)</span> which must satisfy</p>
<p>(i) <span class="math notranslate nohighlight">\(\mathbb{P}(\Omega) = 1\)</span></p>
<p>(ii) <strong>Countable additivity</strong>: for any countable collection of disjoint
sets <span class="math notranslate nohighlight">\(\{A_i\} \subseteq \mathcal{F}\)</span>,
$<span class="math notranslate nohighlight">\(\mathbb{P}\bigg(\bigcup_i A_i\bigg) = \sum_i \mathbb{P}(A_i)\)</span>$</p>
<p>The triple <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, \mathbb{P})\)</span> is called a <strong>probability
space</strong>.<a class="footnote-reference brackets" href="#id22" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a></p>
<p>If <span class="math notranslate nohighlight">\(\mathbb{P}(A) = 1\)</span>, we say that <span class="math notranslate nohighlight">\(A\)</span> occurs <strong>almost surely</strong> (often
abbreviated a.s.).<a class="footnote-reference brackets" href="#id23" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>, and conversely <span class="math notranslate nohighlight">\(A\)</span> occurs <strong>almost never</strong> if
<span class="math notranslate nohighlight">\(\mathbb{P}(A) = 0\)</span>.</p>
<p>From these axioms, a number of useful rules can be derived.</p>
<div class="proposition docutils">
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be an event. Then</p>
<p>(i) <span class="math notranslate nohighlight">\(\mathbb{P}(A^\text{c}) = 1 - \mathbb{P}(A)\)</span>.</p>
<p>(ii) If <span class="math notranslate nohighlight">\(B\)</span> is an event and <span class="math notranslate nohighlight">\(B \subseteq A\)</span>, then
<span class="math notranslate nohighlight">\(\mathbb{P}(B) \leq \mathbb{P}(A)\)</span>.</p>
<p>(iii) <span class="math notranslate nohighlight">\(0 = \mathbb{P}(\varnothing) \leq \mathbb{P}(A) \leq \mathbb{P}(\Omega) = 1\)</span></p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> (i) Using the countable additivity of <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>, we have
$<span class="math notranslate nohighlight">\(\mathbb{P}(A) + \mathbb{P}(A^\text{c}) = \mathbb{P}(A \mathbin{\dot{\cup}} A^\text{c}) = \mathbb{P}(\Omega) = 1\)</span>$</p>
<p>To show (ii), suppose <span class="math notranslate nohighlight">\(B \in \mathcal{F}\)</span> and <span class="math notranslate nohighlight">\(B \subseteq A\)</span>. Then
$<span class="math notranslate nohighlight">\(\mathbb{P}(A) = \mathbb{P}(B \mathbin{\dot{\cup}} (A \setminus B)) = \mathbb{P}(B) + \mathbb{P}(A \setminus B) \geq \mathbb{P}(B)\)</span>$
as claimed.</p>
<p>For (iii): the middle inequality follows from (ii) since
<span class="math notranslate nohighlight">\(\varnothing \subseteq A \subseteq \Omega\)</span>. We also have
$<span class="math notranslate nohighlight">\(\mathbb{P}(\varnothing) = \mathbb{P}(\varnothing \mathbin{\dot{\cup}} \varnothing) = \mathbb{P}(\varnothing) + \mathbb{P}(\varnothing)\)</span><span class="math notranslate nohighlight">\(
by countable additivity, which shows \)</span>\mathbb{P}(\varnothing) = 0$. ◻</p>
</div>
<div class="proposition docutils">
<p>If <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are events, then
<span class="math notranslate nohighlight">\(\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)\)</span>.</p>
</div>
<div class="proof docutils">
<p><em>Proof.</em> The key is to break the events up into their various
overlapping and non-overlapping parts. $<span class="math notranslate nohighlight">\(\begin{aligned}
\mathbb{P}(A \cup B) &amp;= \mathbb{P}((A \cap B) \mathbin{\dot{\cup}} (A \setminus B) \mathbin{\dot{\cup}} (B \setminus A)) \\
&amp;= \mathbb{P}(A \cap B) + \mathbb{P}(A \setminus B) + \mathbb{P}(B \setminus A) \\
&amp;= \mathbb{P}(A \cap B) + \mathbb{P}(A) - \mathbb{P}(A \cap B) + \mathbb{P}(B) - \mathbb{P}(A \cap B) \\
&amp;= \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)
\end{aligned}\)</span>$ ◻</p>
</div>
<div class="proposition docutils">
<p>If <span class="math notranslate nohighlight">\(\{A_i\} \subseteq \mathcal{F}\)</span> is a countable set of events,
disjoint or not, then
$<span class="math notranslate nohighlight">\(\mathbb{P}\bigg(\bigcup_i A_i\bigg) \leq \sum_i \mathbb{P}(A_i)\)</span>$</p>
</div>
<p>This inequality is sometimes referred to as <strong>Boole’s inequality</strong> or
the <strong>union bound</strong>.</p>
<div class="proof docutils">
<p><em>Proof.</em> Define <span class="math notranslate nohighlight">\(B_1 = A_1\)</span> and
<span class="math notranslate nohighlight">\(B_i = A_i \setminus (\bigcup_{j &lt; i} A_j)\)</span> for <span class="math notranslate nohighlight">\(i &gt; 1\)</span>, noting that
<span class="math notranslate nohighlight">\(\bigcup_{j \leq i} B_j = \bigcup_{j \leq i} A_j\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and the
<span class="math notranslate nohighlight">\(B_i\)</span> are disjoint. Then
$<span class="math notranslate nohighlight">\(\mathbb{P}\bigg(\bigcup_i A_i\bigg) = \mathbb{P}\bigg(\bigcup_i B_i\bigg) = \sum_i \mathbb{P}(B_i) \leq \sum_i \mathbb{P}(A_i)\)</span><span class="math notranslate nohighlight">\(
where the last inequality follows by monotonicity since
\)</span>B_i \subseteq A_i<span class="math notranslate nohighlight">\( for all \)</span>i$. ◻</p>
</div>
<section id="conditional-probability">
<h3>Conditional probability<a class="headerlink" href="#conditional-probability" title="Link to this heading">#</a></h3>
<p>The <strong>conditional probability</strong> of event <span class="math notranslate nohighlight">\(A\)</span> given that event <span class="math notranslate nohighlight">\(B\)</span> has
occurred is written <span class="math notranslate nohighlight">\(\mathbb{P}(A | B)\)</span> and defined as
$<span class="math notranslate nohighlight">\(\mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}\)</span><span class="math notranslate nohighlight">\(
assuming \)</span>\mathbb{P}(B) &gt; 0$.<a class="footnote-reference brackets" href="#id24" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a></p>
</section>
<section id="chain-rule">
<h3>Chain rule<a class="headerlink" href="#chain-rule" title="Link to this heading">#</a></h3>
<p>Another very useful tool, the <strong>chain rule</strong>, follows immediately from
this definition:
$<span class="math notranslate nohighlight">\(\mathbb{P}(A \cap B) = \mathbb{P}(A | B)\mathbb{P}(B) = \mathbb{P}(B | A)\mathbb{P}(A)\)</span>$</p>
</section>
<section id="bayes-rule">
<h3>Bayes’ rule<a class="headerlink" href="#bayes-rule" title="Link to this heading">#</a></h3>
<p>Taking the equality from above one step further, we arrive at the simple
but crucial <strong>Bayes’ rule</strong>:
$<span class="math notranslate nohighlight">\(\mathbb{P}(A | B) = \frac{\mathbb{P}(B | A)\mathbb{P}(A)}{\mathbb{P}(B)}\)</span><span class="math notranslate nohighlight">\(
It is sometimes beneficial to omit the normalizing constant and write
\)</span><span class="math notranslate nohighlight">\(\mathbb{P}(A | B) \propto \mathbb{P}(A)\mathbb{P}(B | A)\)</span><span class="math notranslate nohighlight">\( Under this
formulation, \)</span>\mathbb{P}(A)<span class="math notranslate nohighlight">\( is often referred to as the **prior**,
\)</span>\mathbb{P}(A | B)<span class="math notranslate nohighlight">\( as the **posterior**, and \)</span>\mathbb{P}(B | A)$ as the
<strong>likelihood</strong>.</p>
<p>In the context of machine learning, we can use Bayes’ rule to update our
“beliefs” (e.g. values of our model parameters) given some data that
we’ve observed.</p>
</section>
</section>
<section id="random-variables">
<h2>Random variables<a class="headerlink" href="#random-variables" title="Link to this heading">#</a></h2>
<p>A <strong>random variable</strong> is some uncertain quantity with an associated
probability distribution over the values it can assume.</p>
<p>Formally, a random variable on a probability space
<span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, \mathbb{P})\)</span> is a function<a class="footnote-reference brackets" href="#id25" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a>
<span class="math notranslate nohighlight">\(X: \Omega \to \mathbb{R}\)</span>.<a class="footnote-reference brackets" href="#id26" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a></p>
<p>We denote the range of <span class="math notranslate nohighlight">\(X\)</span> by
<span class="math notranslate nohighlight">\(X(\Omega) = \{X(\omega) : \omega \in \Omega\}\)</span>. To give a concrete
example (taken from [&#64;pitman]), suppose <span class="math notranslate nohighlight">\(X\)</span> is the number of heads in
two tosses of a fair coin. The sample space is
$<span class="math notranslate nohighlight">\(\Omega = \{hh, tt, ht, th\}\)</span><span class="math notranslate nohighlight">\( and \)</span>X<span class="math notranslate nohighlight">\( is determined completely by the
outcome \)</span>\omega<span class="math notranslate nohighlight">\(, i.e. \)</span>X = X(\omega)<span class="math notranslate nohighlight">\(. For example, the event \)</span>X = 1<span class="math notranslate nohighlight">\(
is the set of outcomes \)</span>{ht, th}$.</p>
<p>It is common to talk about the values of a random variable without
directly referencing its sample space. The two are related by the
following definition: the event that the value of <span class="math notranslate nohighlight">\(X\)</span> lies in some set
<span class="math notranslate nohighlight">\(S \subseteq \mathbb{R}\)</span> is
$<span class="math notranslate nohighlight">\(X \in S = \{\omega \in \Omega : X(\omega) \in S\}\)</span><span class="math notranslate nohighlight">\( Note that special
cases of this definition include \)</span>X<span class="math notranslate nohighlight">\( being equal to, less than, or
greater than some specified value. For example
\)</span><span class="math notranslate nohighlight">\(\mathbb{P}(X = x) = \mathbb{P}(\{\omega \in \Omega : X(\omega) = x\})\)</span>$</p>
<p>A word on notation: we write <span class="math notranslate nohighlight">\(p(X)\)</span> to denote the entire probability
distribution of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(p(x)\)</span> for the evaluation of the function <span class="math notranslate nohighlight">\(p\)</span> at
a particular value <span class="math notranslate nohighlight">\(x \in X(\Omega)\)</span>. Hopefully this (reasonably
standard) abuse of notation is not too distracting. If <span class="math notranslate nohighlight">\(p\)</span> is
parameterized by some parameters <span class="math notranslate nohighlight">\(\theta\)</span>, we write
<span class="math notranslate nohighlight">\(p(X; \mathbf{\theta})\)</span> or <span class="math notranslate nohighlight">\(p(x; \mathbf{\theta})\)</span>, unless we are in a
Bayesian setting where the parameters are considered a random variable,
in which case we condition on the parameters.</p>
<section id="the-cumulative-distribution-function">
<h3>The cumulative distribution function<a class="headerlink" href="#the-cumulative-distribution-function" title="Link to this heading">#</a></h3>
<p>The <strong>cumulative distribution function</strong> (c.d.f.) gives the probability
that a random variable is at most a certain value:
$<span class="math notranslate nohighlight">\(F(x) = \mathbb{P}(X \leq x)\)</span><span class="math notranslate nohighlight">\( The c.d.f. can be used to give the
probability that a variable lies within a certain range:
\)</span><span class="math notranslate nohighlight">\(\mathbb{P}(a &lt; X \leq b) = F(b) - F(a)\)</span>$</p>
</section>
<section id="discrete-random-variables">
<h3>Discrete random variables<a class="headerlink" href="#discrete-random-variables" title="Link to this heading">#</a></h3>
<p>A <strong>discrete random variable</strong> is a random variable that has a countable
range and assumes each value in this range with positive probability.
Discrete random variables are completely specified by their
<strong>probability mass function</strong> (p.m.f.) <span class="math notranslate nohighlight">\(p : X(\Omega) \to [0,1]\)</span> which
satisfies $<span class="math notranslate nohighlight">\(\sum_{x \in X(\Omega)} p(x) = 1\)</span><span class="math notranslate nohighlight">\( For a discrete \)</span>X<span class="math notranslate nohighlight">\(, the
probability of a particular value is given exactly by its p.m.f.:
\)</span><span class="math notranslate nohighlight">\(\mathbb{P}(X = x) = p(x)\)</span>$</p>
</section>
<section id="continuous-random-variables">
<h3>Continuous random variables<a class="headerlink" href="#continuous-random-variables" title="Link to this heading">#</a></h3>
<p>A <strong>continuous random variable</strong> is a random variable that has an
uncountable range and assumes each value in this range with probability
zero. Most of the continuous random variables that one would encounter
in practice are <strong>absolutely continuous random variables</strong><a class="footnote-reference brackets" href="#id27" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a>, which
means that there exists a function <span class="math notranslate nohighlight">\(p : \mathbb{R} \to [0,\infty)\)</span> that
satisfies $<span class="math notranslate nohighlight">\(F(x) \equiv \int_{-\infty}^x p(z)\dd{z}\)</span><span class="math notranslate nohighlight">\( The function \)</span>p<span class="math notranslate nohighlight">\(
is called a **probability density function** (abbreviated p.d.f.) and
must satisfy \)</span><span class="math notranslate nohighlight">\(\int_{-\infty}^\infty p(x)\dd{x} = 1\)</span>$ The values of this
function are not themselves probabilities, since they could exceed 1.
However, they do have a couple of reasonable interpretations. One is as
relative probabilities; even though the probability of each particular
value being picked is technically zero, some points are still in a sense
more likely than others.</p>
<p>One can also think of the density as determining the probability that
the variable will lie in a small range about a given value. This is
because, for small <span class="math notranslate nohighlight">\(\epsilon &gt; 0\)</span>,
$<span class="math notranslate nohighlight">\(\mathbb{P}(x-\epsilon \leq X \leq x+\epsilon) = \int_{x-\epsilon}^{x+\epsilon} p(z)\dd{z} \approx 2\epsilon p(x)\)</span>$
using a midpoint approximation to the integral.</p>
<p>Here are some useful identities that follow from the definitions above:
$<span class="math notranslate nohighlight">\(\begin{aligned}
\mathbb{P}(a \leq X \leq b) &amp;= \int_a^b p(x)\dd{x} \\
p(x) &amp;= F'(x)
\end{aligned}\)</span>$</p>
</section>
<section id="other-kinds-of-random-variables">
<h3>Other kinds of random variables<a class="headerlink" href="#other-kinds-of-random-variables" title="Link to this heading">#</a></h3>
<p>There are random variables that are neither discrete nor continuous. For
example, consider a random variable determined as follows: flip a fair
coin, then the value is zero if it comes up heads, otherwise draw a
number uniformly at random from <span class="math notranslate nohighlight">\([1,2]\)</span>. Such a random variable can take
on uncountably many values, but only finitely many of these with
positive probability. We will not discuss such random variables because
they are rather pathological and require measure theory to analyze.</p>
</section>
</section>
<section id="joint-distributions">
<h2>Joint distributions<a class="headerlink" href="#joint-distributions" title="Link to this heading">#</a></h2>
<p>Often we have several random variables and we would like to get a
distribution over some combination of them. A <strong>joint distribution</strong> is
exactly this. For some random variables <span class="math notranslate nohighlight">\(X_1, \dots, X_n\)</span>, the joint
distribution is written <span class="math notranslate nohighlight">\(p(X_1, \dots, X_n)\)</span> and gives probabilities
over entire assignments to all the <span class="math notranslate nohighlight">\(X_i\)</span> simultaneously.</p>
<section id="independence-of-random-variables">
<h3>Independence of random variables<a class="headerlink" href="#independence-of-random-variables" title="Link to this heading">#</a></h3>
<p>We say that two variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are <strong>independent</strong> if their joint
distribution factors into their respective distributions, i.e.
$<span class="math notranslate nohighlight">\(p(X, Y) = p(X)p(Y)\)</span><span class="math notranslate nohighlight">\( We can also define independence for more than two
random variables, although it is more complicated. Let
\)</span>{X_i}_{i \in I}<span class="math notranslate nohighlight">\( be a collection of random variables indexed by \)</span>I<span class="math notranslate nohighlight">\(,
which may be infinite. Then \)</span>{X_i}<span class="math notranslate nohighlight">\( are independent if for every
finite subset of indices \)</span>i_1, \dots, i_k \in I<span class="math notranslate nohighlight">\( we have
\)</span><span class="math notranslate nohighlight">\(p(X_{i_1}, \dots, X_{i_k}) = \prod_{j=1}^k p(X_{i_j})\)</span><span class="math notranslate nohighlight">\( For example,
in the case of three random variables, \)</span>X, Y, Z<span class="math notranslate nohighlight">\(, we require that
\)</span>p(X,Y,Z) = p(X)p(Y)p(Z)<span class="math notranslate nohighlight">\( as well as \)</span>p(X,Y) = p(X)p(Y)<span class="math notranslate nohighlight">\(,
\)</span>p(X,Z) = p(X)p(Z)<span class="math notranslate nohighlight">\(, and \)</span>p(Y,Z) = p(Y)p(Z)$.</p>
<p>It is often convenient (though perhaps questionable) to assume that a
bunch of random variables are <strong>independent and identically
distributed</strong> (i.i.d.) so that their joint distribution can be factored
entirely: $<span class="math notranslate nohighlight">\(p(X_1, \dots, X_n) = \prod_{i=1}^n p(X_i)\)</span><span class="math notranslate nohighlight">\( where
\)</span>X_1, \dots, X_n$ all share the same p.m.f./p.d.f.</p>
</section>
<section id="marginal-distributions">
<h3>Marginal distributions<a class="headerlink" href="#marginal-distributions" title="Link to this heading">#</a></h3>
<p>If we have a joint distribution over some set of random variables, it is
possible to obtain a distribution for a subset of them by “summing out”
(or “integrating out” in the continuous case) the variables we don’t
care about: $<span class="math notranslate nohighlight">\(p(X) = \sum_{y} p(X, y)\)</span>$</p>
</section>
</section>
<section id="great-expectations">
<h2>Great Expectations<a class="headerlink" href="#great-expectations" title="Link to this heading">#</a></h2>
<p>If we have some random variable <span class="math notranslate nohighlight">\(X\)</span>, we might be interested in knowing
what is the “average” value of <span class="math notranslate nohighlight">\(X\)</span>. This concept is captured by the
<strong>expected value</strong> (or <strong>mean</strong>) <span class="math notranslate nohighlight">\(\mathbb{E}[X]\)</span>, which is defined as
$<span class="math notranslate nohighlight">\(\mathbb{E}[X] = \sum_{x \in X(\Omega)} xp(x)\)</span><span class="math notranslate nohighlight">\( for discrete \)</span>X<span class="math notranslate nohighlight">\( and as
\)</span><span class="math notranslate nohighlight">\(\mathbb{E}[X] = \int_{-\infty}^\infty xp(x)\dd{x}\)</span><span class="math notranslate nohighlight">\( for continuous
\)</span>X$.</p>
<p>In words, we are taking a weighted sum of the values that <span class="math notranslate nohighlight">\(X\)</span> can take
on, where the weights are the probabilities of those respective values.
The expected value has a physical interpretation as the “center of mass”
of the distribution.</p>
<section id="properties-of-expected-value">
<h3>Properties of expected value<a class="headerlink" href="#properties-of-expected-value" title="Link to this heading">#</a></h3>
<p>A very useful property of expectation is that of linearity:
$<span class="math notranslate nohighlight">\(\mathbb{E}\left[\sum_{i=1}^n \alpha_i X_i + \beta\right] = \sum_{i=1}^n \alpha_i \mathbb{E}[X_i] + \beta\)</span><span class="math notranslate nohighlight">\(
Note that this holds even if the \)</span>X_i$ are not independent!</p>
<p>But if they are independent, the product rule also holds:
$<span class="math notranslate nohighlight">\(\mathbb{E}\left[\prod_{i=1}^n X_i\right] = \prod_{i=1}^n \mathbb{E}[X_i]\)</span>$</p>
</section>
</section>
<section id="variance">
<h2>Variance<a class="headerlink" href="#variance" title="Link to this heading">#</a></h2>
<p>Expectation provides a measure of the “center” of a distribution, but
frequently we are also interested in what the “spread” is about that
center. We define the variance <span class="math notranslate nohighlight">\(\operatorname{Var}(X)\)</span> of a random
variable <span class="math notranslate nohighlight">\(X\)</span> by
$<span class="math notranslate nohighlight">\(\operatorname{Var}(X) = \mathbb{E}\left[\left(X - \mathbb{E}[X]\right)^2\right]\)</span><span class="math notranslate nohighlight">\(
In words, this is the average squared deviation of the values of \)</span>X<span class="math notranslate nohighlight">\(
from the mean of \)</span>X<span class="math notranslate nohighlight">\(. Using a little algebra and the linearity of
expectation, it is straightforward to show that
\)</span><span class="math notranslate nohighlight">\(\operatorname{Var}(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2\)</span>$</p>
<section id="properties-of-variance">
<h3>Properties of variance<a class="headerlink" href="#properties-of-variance" title="Link to this heading">#</a></h3>
<p>Variance is not linear (because of the squaring in the definition), but
one can show the following:
$<span class="math notranslate nohighlight">\(\operatorname{Var}(\alpha X + \beta) = \alpha^2 \operatorname{Var}(X)\)</span>$
Basically, multiplicative constants become squared when they are pulled
out, and additive constants disappear (since the variance contributed by
a constant is zero).</p>
<p>Furthermore, if <span class="math notranslate nohighlight">\(X_1, \dots, X_n\)</span> are uncorrelated<a class="footnote-reference brackets" href="#id28" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a>, then
$<span class="math notranslate nohighlight">\(\operatorname{Var}(X_1 + \dots + X_n) = \operatorname{Var}(X_1) + \dots + \operatorname{Var}(X_n)\)</span>$</p>
</section>
<section id="standard-deviation">
<h3>Standard deviation<a class="headerlink" href="#standard-deviation" title="Link to this heading">#</a></h3>
<p>Variance is a useful notion, but it suffers from that fact the units of
variance are not the same as the units of the random variable (again
because of the squaring). To overcome this problem we can use <strong>standard
deviation</strong>, which is defined as <span class="math notranslate nohighlight">\(\sqrt{\operatorname{Var}(X)}\)</span>. The
standard deviation of <span class="math notranslate nohighlight">\(X\)</span> has the same units as <span class="math notranslate nohighlight">\(X\)</span>.</p>
</section>
</section>
<section id="covariance">
<h2>Covariance<a class="headerlink" href="#covariance" title="Link to this heading">#</a></h2>
<p>Covariance is a measure of the linear relationship between two random
variables. We denote the covariance between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> as
<span class="math notranslate nohighlight">\(\operatorname{Cov}(X, Y)\)</span>, and it is defined to be
$<span class="math notranslate nohighlight">\(\operatorname{Cov}(X, Y) = \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]\)</span><span class="math notranslate nohighlight">\(
Note that the outer expectation must be taken over the joint
distribution of \)</span>X<span class="math notranslate nohighlight">\( and \)</span>Y$.</p>
<p>Again, the linearity of expectation allows us to rewrite this as
$<span class="math notranslate nohighlight">\(\operatorname{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]\)</span><span class="math notranslate nohighlight">\(
Comparing these formulas to the ones for variance, it is not hard to see
that \)</span>\operatorname{Var}(X) = \operatorname{Cov}(X, X)$.</p>
<p>A useful property of covariance is that of <strong>bilinearity</strong>:
$<span class="math notranslate nohighlight">\(\begin{aligned}
\operatorname{Cov}(\alpha X + \beta Y, Z) &amp;= \alpha\operatorname{Cov}(X, Z) + \beta\operatorname{Cov}(Y, Z) \\
\operatorname{Cov}(X, \alpha Y + \beta Z) &amp;= \alpha\operatorname{Cov}(X, Y) + \beta\operatorname{Cov}(X, Z)
\end{aligned}\)</span>$</p>
<section id="correlation">
<h3>Correlation<a class="headerlink" href="#correlation" title="Link to this heading">#</a></h3>
<p>Normalizing the covariance gives the <strong>correlation</strong>:
$<span class="math notranslate nohighlight">\(\rho(X, Y) = \frac{\operatorname{Cov}(X, Y)}{\sqrt{\operatorname{Var}(X)\operatorname{Var}(Y)}}\)</span><span class="math notranslate nohighlight">\(
Correlation also measures the linear relationship between two variables,
but unlike covariance always lies between \)</span>-1<span class="math notranslate nohighlight">\( and \)</span>1$.</p>
<p>Two variables are said to be <strong>uncorrelated</strong> if
<span class="math notranslate nohighlight">\(\operatorname{Cov}(X, Y) = 0\)</span> because <span class="math notranslate nohighlight">\(\operatorname{Cov}(X, Y) = 0\)</span>
implies that <span class="math notranslate nohighlight">\(\rho(X, Y) = 0\)</span>. If two variables are independent, then
they are uncorrelated, but the converse does not hold in general.</p>
</section>
</section>
<section id="random-vectors">
<h2>Random vectors<a class="headerlink" href="#random-vectors" title="Link to this heading">#</a></h2>
<p>So far we have been talking about <strong>univariate distributions</strong>, that is,
distributions of single variables. But we can also talk about
<strong>multivariate distributions</strong> which give distributions of <strong>random
vectors</strong>:
$<span class="math notranslate nohighlight">\(\mathbf{X} = \begin{bmatrix}X_1 \\ \vdots \\ X_n\end{bmatrix}\)</span>$ The
summarizing quantities we have discussed for single variables have
natural generalizations to the multivariate case.</p>
<p>Expectation of a random vector is simply the expectation applied to each
component:
$<span class="math notranslate nohighlight">\(\mathbb{E}[\mathbf{X}] = \begin{bmatrix}\mathbb{E}[X_1] \\ \vdots \\ \mathbb{E}[X_n]\end{bmatrix}\)</span>$</p>
<p>The variance is generalized by the <strong>covariance matrix</strong>:
$<span class="math notranslate nohighlight">\(\mathbf{\Sigma} = \mathbb{E}[(\mathbf{X} - \mathbb{E}[\mathbf{X}])(\mathbf{X} - \mathbb{E}[\mathbf{X}])^{\!\top\!}] = \begin{bmatrix}
\operatorname{Var}(X_1) &amp; \operatorname{Cov}(X_1, X_2) &amp; \hdots &amp; \operatorname{Cov}(X_1, X_n) \\
\operatorname{Cov}(X_2, X_1) &amp; \operatorname{Var}(X_2) &amp; \hdots &amp; \operatorname{Cov}(X_2, X_n) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\operatorname{Cov}(X_n, X_1) &amp; \operatorname{Cov}(X_n, X_2) &amp; \hdots &amp; \operatorname{Var}(X_n)
\end{bmatrix}\)</span><span class="math notranslate nohighlight">\( That is, \)</span>\Sigma_{ij} = \operatorname{Cov}(X_i, X_j)<span class="math notranslate nohighlight">\(.
Since covariance is symmetric in its arguments, the covariance matrix is
also symmetric. It's also positive semi-definite: for any \)</span>\mathbf{x}<span class="math notranslate nohighlight">\(,
\)</span><span class="math notranslate nohighlight">\(\mathbf{x}^{\!\top\!}\mathbf{\Sigma}\mathbf{x} = \mathbf{x}^{\!\top\!}\mathbb{E}[(\mathbf{X} - \mathbb{E}[\mathbf{X}])(\mathbf{X} - \mathbb{E}[\mathbf{X}])^{\!\top\!}]\mathbf{x} = \mathbb{E}[\mathbf{x}^{\!\top\!}(\mathbf{X} - \mathbb{E}[\mathbf{X}])(\mathbf{X} - \mathbb{E}[\mathbf{X}])^{\!\top\!}\mathbf{x}] = \mathbb{E}[((\mathbf{X} - \mathbb{E}[\mathbf{X}])^{\!\top\!}\mathbf{x})^2] \geq 0\)</span><span class="math notranslate nohighlight">\(
The inverse of the covariance matrix, \)</span>\mathbf{\Sigma}^{-1}$, is
sometimes called the <strong>precision matrix</strong>.</p>
</section>
<section id="estimation-of-parameters">
<h2>Estimation of Parameters<a class="headerlink" href="#estimation-of-parameters" title="Link to this heading">#</a></h2>
<p>Now we get into some basic topics from statistics. We make some
assumptions about our problem by prescribing a <strong>parametric</strong> model
(e.g. a distribution that describes how the data were generated), then
we fit the parameters of the model to the data. How do we choose the
values of the parameters?</p>
<section id="maximum-likelihood-estimation">
<h3>Maximum likelihood estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Link to this heading">#</a></h3>
<p>A common way to fit parameters is <strong>maximum likelihood estimation</strong>
(MLE). The basic principle of MLE is to choose values that “explain” the
data best by maximizing the probability/density of the data we’ve seen
as a function of the parameters. Suppose we have random variables
<span class="math notranslate nohighlight">\(X_1, \dots, X_n\)</span> and corresponding observations <span class="math notranslate nohighlight">\(x_1, \dots, x_n\)</span>. Then
$<span class="math notranslate nohighlight">\(\hat{\mathbf{\theta}}_\textsc{mle} = \argmax_\mathbf{\theta} \mathcal{L}(\mathbf{\theta})\)</span><span class="math notranslate nohighlight">\(
where \)</span>\mathcal{L}<span class="math notranslate nohighlight">\( is the **likelihood function**
\)</span><span class="math notranslate nohighlight">\(\mathcal{L}(\mathbf{\theta}) = p(x_1, \dots, x_n; \mathbf{\theta})\)</span><span class="math notranslate nohighlight">\(
Often, we assume that \)</span>X_1, \dots, X_n<span class="math notranslate nohighlight">\( are i.i.d. Then we can write
\)</span><span class="math notranslate nohighlight">\(p(x_1, \dots, x_n; \theta) = \prod_{i=1}^n p(x_i; \mathbf{\theta})\)</span><span class="math notranslate nohighlight">\(
At this point, it is usually convenient to take logs, giving rise to the
**log-likelihood**
\)</span><span class="math notranslate nohighlight">\(\log\mathcal{L}(\mathbf{\theta}) = \sum_{i=1}^n \log p(x_i; \mathbf{\theta})\)</span><span class="math notranslate nohighlight">\(
This is a valid operation because the probabilities/densities are
assumed to be positive, and since log is a monotonically increasing
function, it preserves ordering. In other words, any maximizer of
\)</span>\log\mathcal{L}<span class="math notranslate nohighlight">\( will also maximize \)</span>\mathcal{L}$.</p>
<p>For some distributions, it is possible to analytically solve for the
maximum likelihood estimator. If <span class="math notranslate nohighlight">\(\log\mathcal{L}\)</span> is differentiable,
setting the derivatives to zero and trying to solve for
<span class="math notranslate nohighlight">\(\mathbf{\theta}\)</span> is a good place to start.</p>
</section>
<section id="maximum-a-posteriori-estimation">
<h3>Maximum a posteriori estimation<a class="headerlink" href="#maximum-a-posteriori-estimation" title="Link to this heading">#</a></h3>
<p>A more Bayesian way to fit parameters is through <strong>maximum a posteriori
estimation</strong> (MAP). In this technique we assume that the parameters are
a random variable, and we specify a prior distribution
<span class="math notranslate nohighlight">\(p(\mathbf{\theta})\)</span>. Then we can employ Bayes’ rule to compute the
posterior distribution of the parameters given the observed data:
$<span class="math notranslate nohighlight">\(p(\mathbf{\theta} | x_1, \dots, x_n) \propto p(\mathbf{\theta})p(x_1, \dots, x_n | \mathbf{\theta})\)</span><span class="math notranslate nohighlight">\(
Computing the normalizing constant is often intractable, because it
involves integrating over the parameter space, which may be very
high-dimensional. Fortunately, if we just want the MAP estimate, we
don't care about the normalizing constant! It does not affect which
values of \)</span>\mathbf{\theta}<span class="math notranslate nohighlight">\( maximize the posterior. So we have
\)</span><span class="math notranslate nohighlight">\(\hat{\mathbf{\theta}}_\textsc{map} = \argmax_\mathbf{\theta} p(\mathbf{\theta})p(x_1, \dots, x_n | \mathbf{\theta})\)</span><span class="math notranslate nohighlight">\(
Again, if we assume the observations are i.i.d., then we can express
this in the equivalent, and possibly friendlier, form
\)</span><span class="math notranslate nohighlight">\(\hat{\mathbf{\theta}}_\textsc{map} = \argmax_\mathbf{\theta} \left(\log p(\mathbf{\theta}) + \sum_{i=1}^n \log p(x_i | \mathbf{\theta})\right)\)</span>$
A particularly nice case is when the prior is chosen carefully such that
the posterior comes from the same family as the prior. In this case the
prior is called a <strong>conjugate prior</strong>. For example, if the likelihood is
binomial and the prior is beta, the posterior is also beta. There are
many conjugate priors; the reader may find this <a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions">table of conjugate
priors</a>
useful.</p>
</section>
</section>
<section id="the-gaussian-distribution">
<h2>The Gaussian distribution<a class="headerlink" href="#the-gaussian-distribution" title="Link to this heading">#</a></h2>
<p>There are many distributions, but one of particular importance is the
<strong>Gaussian distribution</strong>, also known as the <strong>normal distribution</strong>. It
is a continuous distribution, parameterized by its mean
<span class="math notranslate nohighlight">\(\bm\mu \in \mathbb{R}^d\)</span> and positive-definite covariance matrix
<span class="math notranslate nohighlight">\(\mathbf{\Sigma} \in \mathbb{R}^{d \times d}\)</span>, with density
$<span class="math notranslate nohighlight">\(p(\mathbf{x}; \bm\mu, \mathbf{\Sigma}) = \frac{1}{\sqrt{(2\pi)^d \det(\mathbf{\Sigma})}}\exp\left(-\frac{1}{2}(\mathbf{x} - \bm\mu)^{\!\top\!}\mathbf{\Sigma}^{-1}(\mathbf{x} - \bm\mu)\right)\)</span><span class="math notranslate nohighlight">\(
Note that in the special case \)</span>d = 1<span class="math notranslate nohighlight">\(, the density is written in the
more recognizable form
\)</span><span class="math notranslate nohighlight">\(p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\)</span><span class="math notranslate nohighlight">\(
We write \)</span>\mathbf{X} \sim \mathcal{N}(\bm\mu, \mathbf{\Sigma})<span class="math notranslate nohighlight">\( to
denote that \)</span>\mathbf{X}<span class="math notranslate nohighlight">\( is normally distributed with mean \)</span>\bm\mu<span class="math notranslate nohighlight">\( and
variance \)</span>\mathbf{\Sigma}$.</p>
<section id="the-geometry-of-multivariate-gaussians">
<h3>The geometry of multivariate Gaussians<a class="headerlink" href="#the-geometry-of-multivariate-gaussians" title="Link to this heading">#</a></h3>
<p>The geometry of the multivariate Gaussian density is intimately related
to the geometry of positive definite quadratic forms, so make sure the
material in that section is well-understood before tackling this
section.</p>
<p>First observe that the p.d.f. of the multivariate Gaussian can be
rewritten as
$<span class="math notranslate nohighlight">\(p(\mathbf{x}; \bm\mu, \mathbf{\Sigma}) = g(\tilde{\mathbf{x}}^{\!\top\!}\mathbf{\Sigma}^{-1}\tilde{\mathbf{x}})\)</span><span class="math notranslate nohighlight">\(
where \)</span>\tilde{\mathbf{x}} = \mathbf{x} - \bm\mu<span class="math notranslate nohighlight">\( and
\)</span>g(z) = [(2\pi)^d \det(\mathbf{\Sigma})]^{-\frac{1}{2}}\exp\left(-\frac{z}{2}\right)<span class="math notranslate nohighlight">\(.
Writing the density in this way, we see that after shifting by the mean
\)</span>\bm\mu$, the density is really just a simple function of its precision
matrix’s quadratic form.</p>
<p>Here is a key observation: this function <span class="math notranslate nohighlight">\(g\)</span> is <strong>strictly monotonically
decreasing</strong> in its argument. That is, <span class="math notranslate nohighlight">\(g(a) &gt; g(b)\)</span> whenever <span class="math notranslate nohighlight">\(a &lt; b\)</span>.
Therefore, small values of
<span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}^{\!\top\!}\mathbf{\Sigma}^{-1}\tilde{\mathbf{x}}\)</span>
(which generally correspond to points where <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}\)</span> is
closer to <span class="math notranslate nohighlight">\(\mathbf{0}\)</span>, i.e. <span class="math notranslate nohighlight">\(\mathbf{x} \approx \bm\mu\)</span>) have
relatively high probability densities, and vice-versa. Furthermore,
because <span class="math notranslate nohighlight">\(g\)</span> is <em>strictly</em> monotonic, it is injective, so the
<span class="math notranslate nohighlight">\(c\)</span>-isocontours of <span class="math notranslate nohighlight">\(p(\mathbf{x}; \bm\mu, \mathbf{\Sigma})\)</span> are the
<span class="math notranslate nohighlight">\(g^{-1}(c)\)</span>-isocontours of the function
<span class="math notranslate nohighlight">\(\mathbf{x} \mapsto \tilde{\mathbf{x}}^{\!\top\!}\mathbf{\Sigma}^{-1}\tilde{\mathbf{x}}\)</span>.
That is, for any <span class="math notranslate nohighlight">\(c\)</span>,
$<span class="math notranslate nohighlight">\(\{\mathbf{x} \in \mathbb{R}^d : p(\mathbf{x}; \bm\mu, \mathbf{\Sigma}) = c\} = \{\mathbf{x} \in \mathbb{R}^d : \tilde{\mathbf{x}}^{\!\top\!}\mathbf{\Sigma}^{-1}\tilde{\mathbf{x}} = g^{-1}(c)\}\)</span>$
In words, these functions have the same isocontours but different
isovalues.</p>
<p>Recall the executive summary of the geometry of positive definite
quadratic forms: the isocontours of
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = \mathbf{x}^{\!\top\!}\mathbf{A}\mathbf{x}\)</span> are
ellipsoids such that the axes point in the directions of the
eigenvectors of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, and the lengths of these axes are
proportional to the inverse square roots of the corresponding
eigenvalues. Therefore in this case, the isocontours of the density are
ellipsoids (centered at <span class="math notranslate nohighlight">\(\bm\mu\)</span>) with axis lengths proportional to the
inverse square roots of the eigenvalues of <span class="math notranslate nohighlight">\(\mathbf{\Sigma}^{-1}\)</span>, or
equivalently, the square roots of the eigenvalues of <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>.</p>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id15" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>More generally, vector spaces can be defined over any <strong>field</strong>
<span class="math notranslate nohighlight">\(\mathbb{F}\)</span>. We take <span class="math notranslate nohighlight">\(\mathbb{F} = \mathbb{R}\)</span> in this document to
avoid an unnecessary diversion into abstract algebra.</p>
</aside>
<aside class="footnote brackets" id="id16" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>It is sometimes called the <strong>kernel</strong> by algebraists, but we
eschew this terminology because the word “kernel” has another
meaning in machine learning.</p>
</aside>
<aside class="footnote brackets" id="id17" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>If a normed space is complete with respect to the distance metric
induced by its norm, we say that it is a <strong>Banach space</strong>.</p>
</aside>
<aside class="footnote brackets" id="id18" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>If an inner product space is complete with respect to the distance
metric induced by its inner product, we say that it is a <strong>Hilbert
space</strong>.</p>
</aside>
<aside class="footnote brackets" id="id19" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p>Recall that <span class="math notranslate nohighlight">\(\mathbf{A}^{\!\top\!}\mathbf{A}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{A}^{\!\top\!}\)</span> are positive semi-definite, so
their eigenvalues are nonnegative, and thus taking square roots is
always well-defined.</p>
</aside>
<aside class="footnote brackets" id="id20" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">6</a><span class="fn-bracket">]</span></span>
<p>A <strong>neighborhood</strong> about <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is an open set which
contains <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
</aside>
<aside class="footnote brackets" id="id21" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">7</a><span class="fn-bracket">]</span></span>
<p><span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is required to be a <span class="math notranslate nohighlight">\(\sigma\)</span>-algebra for technical
reasons; see [&#64;rigorousprob].</p>
</aside>
<aside class="footnote brackets" id="id22" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">8</a><span class="fn-bracket">]</span></span>
<p>Note that a probability space is simply a measure space in which
the measure of the whole space equals 1.</p>
</aside>
<aside class="footnote brackets" id="id23" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">9</a><span class="fn-bracket">]</span></span>
<p>This is a probabilist’s version of the measure-theoretic term
<em>almost everywhere</em>.</p>
</aside>
<aside class="footnote brackets" id="id24" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">10</a><span class="fn-bracket">]</span></span>
<p>In some cases it is possible to define conditional probability on
events of probability zero, but this is significantly more technical
so we omit it.</p>
</aside>
<aside class="footnote brackets" id="id25" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">11</a><span class="fn-bracket">]</span></span>
<p>The function must be measurable.</p>
</aside>
<aside class="footnote brackets" id="id26" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">12</a><span class="fn-bracket">]</span></span>
<p>More generally, the codomain can be any measurable space, but
<span class="math notranslate nohighlight">\(\mathbb{R}\)</span> is the most common case by far and sufficient for our
purposes.</p>
</aside>
<aside class="footnote brackets" id="id27" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">13</a><span class="fn-bracket">]</span></span>
<p>Random variables that are continuous but not absolutely
continuous are called <strong>singular random variables</strong>. We will not
discuss them, assuming rather that all continuous random variables
admit a density function.</p>
</aside>
<aside class="footnote brackets" id="id28" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">14</a><span class="fn-bracket">]</span></span>
<p>We haven’t defined this yet; see the Correlation section below</p>
</aside>
</aside>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_linear_algebra"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_preface/intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Notation</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter_calculus/calculus.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Calculus and Optimization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Linear Algebra</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-spaces">Vector spaces</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#euclidean-space">Euclidean space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subspaces">Subspaces</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metric-spaces">Metric spaces</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normed-spaces">Normed spaces</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inner-product-spaces">Inner product spaces</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pythagorean-theorem">Pythagorean Theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cauchy-schwarz-inequality">Cauchy-Schwarz inequality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transposition">Transposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenthings">Eigenthings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#trace">Trace</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determinant">Determinant</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#orthogonal-matrices">Orthogonal matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#symmetric-matrices">Symmetric matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rayleigh-quotients">Rayleigh quotients</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positive-semi-definite-matrices">Positive (semi-)definite matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-geometry-of-positive-definite-quadratic-forms">The geometry of positive definite quadratic forms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#singular-value-decomposition">Singular value decomposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-useful-matrix-identities">Some useful matrix identities</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-vector-product-as-linear-combination-of-matrix-columns">Matrix-vector product as linear combination of matrix columns</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sum-of-outer-products-as-matrix-matrix-product">Sum of outer products as matrix-matrix product</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadratic-forms">Quadratic forms</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#calculus-and-optimization">Calculus and Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extrema">Extrema</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients">Gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-jacobian">The Jacobian</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hessian">The Hessian</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-calculus">Matrix calculus</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chain-rule">The chain rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#taylor-s-theorem">Taylor’s theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditions-for-local-minima">Conditions for local minima</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity">Convexity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-sets">Convex sets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basics-of-convex-functions">Basics of convex functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#consequences-of-convexity">Consequences of convexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#showing-that-a-function-is-convex">Showing that a function is convex</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#orthogonal-projections">Orthogonal projections</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#probability">Probability</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basics">Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability">Conditional probability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule">Chain rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-rule">Bayes’ rule</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-variables">Random variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cumulative-distribution-function">The cumulative distribution function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-random-variables">Discrete random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-random-variables">Continuous random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-kinds-of-random-variables">Other kinds of random variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distributions">Joint distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-of-random-variables">Independence of random variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-distributions">Marginal distributions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#great-expectations">Great Expectations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-expected-value">Properties of expected value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#properties-of-variance">Properties of variance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-deviation">Standard deviation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation">Correlation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-vectors">Random vectors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-parameters">Estimation of Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum likelihood estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-estimation">Maximum a posteriori estimation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-distribution">The Gaussian distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-geometry-of-multivariate-gaussians">The geometry of multivariate Gaussians</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christoph Lippert
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>