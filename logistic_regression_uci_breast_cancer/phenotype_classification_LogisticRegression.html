
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Machine Learning for Human Phenotype Classification &#8212; Mathematics for Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'logistic_regression_uci_breast_cancer/phenotype_classification_LogisticRegression';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Probability" href="../chapter_probability/probability.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/hpi-logo-colored.svg" class="logo__image only-light" alt="Mathematics for Machine Learning - Home"/>
    <img src="../_static/hpi-logo-colored.svg" class="logo__image only-dark pst-js-only" alt="Mathematics for Machine Learning - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Mathematics for Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/intro.html">Notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear_algebra/linear_algebra.html">Linear Algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_calculus/calculus.html">Calculus and Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_probability/probability.html">Probability</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Machine Learning for Human Phenotype Classification</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Flogistic_regression_uci_breast_cancer/phenotype_classification_LogisticRegression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/logistic_regression_uci_breast_cancer/phenotype_classification_LogisticRegression.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Machine Learning for Human Phenotype Classification</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Machine Learning for Human Phenotype Classification</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnosing-breast-cancer-biopsies-using-logistic-regression">Diagnosing Breast cancer biopsies using Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-classificaiton">Binary Classificaiton</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-data">plot the data</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-sigmoid">The logistic sigmoid</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objective">Objective</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularizer">Regularizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combined-objective">Combined objective</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-model">Fitting the model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#taking-the-derivative">Taking the derivative</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gradient">The gradient</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#steepest-descent">Steepest descent</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-curvature-information">Using curvature information</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hessian">The Hessian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-newton-raphson-algorithm">The Newton-Raphson algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-implementation-of-logistic-regression">The implementation of logistic regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-model">Evaluating the model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">Confusion matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#trade-off-between-sensitivity-and-specificity">Trade-off between Sensitivity and Specificity</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-error-vs-test-error">Train error vs. Test error</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusions">Conclusions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="machine-learning-for-human-phenotype-classification">
<h1>Machine Learning for Human Phenotype Classification<a class="headerlink" href="#machine-learning-for-human-phenotype-classification" title="Link to this heading">#</a></h1>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### Some imports</span>
<span class="c1"># All packages are included in the Anaconda python distribution and integral part of a machine learning Python environment).</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>               <span class="c1"># efficient matrix-vector operations</span>
<span class="kn">import</span> <span class="nn">numpy.linalg</span> <span class="k">as</span> <span class="nn">la</span>        <span class="c1"># linear algebra (solvers etc.)</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>              <span class="c1"># data processing, CSV file I/O (e.g. pd.read_csv)</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>            <span class="c1"># data visualization  </span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>        <span class="c1"># set the figure default style</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>          <span class="c1"># bigger fonts in images</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  <span class="c1"># basic plotting</span>

<span class="c1"># some not so standard imports:</span>
<span class="kn">import</span> <span class="nn">importlib</span>                 <span class="c1"># enable reloading of libraries</span>
<span class="kn">import</span> <span class="nn">plotting_util</span> <span class="k">as</span> <span class="nn">util</span>     <span class="c1"># useful plotting tools for teaching (see plotting_utils.py)</span>
<span class="n">importlib</span><span class="o">.</span><span class="n">reload</span><span class="p">(</span><span class="n">util</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">time</span>                      <span class="c1"># timing (for example to benchmark an algorithm)</span>
</pre></div>
</div>
</div>
</details>
</div>
<section id="diagnosing-breast-cancer-biopsies-using-logistic-regression">
<h2>Diagnosing Breast cancer biopsies using Logistic Regression<a class="headerlink" href="#diagnosing-breast-cancer-biopsies-using-logistic-regression" title="Link to this heading">#</a></h2>
<p>Wisconsin Diagnostic Breast Cancer (WDBC, 1993) data from UCI Machine Learning repository.</p>
<ul class="simple">
<li><p>569 samples from patients with known diagnosis</p></li>
<li><p>357 benign</p></li>
<li><p>212 malignant</p></li>
<li><p>30 features extracted from fine needle aspirate slides</p></li>
</ul>
<p><img alt="title" src="../_images/breast_cancer_nuclei_12938_2011_Article_597_Fig3_HTML.jpg" /></p>
<p>We are given a number of features that describe the nuclei that have been determined from image processing techniques [Street et al, 1992].
While the original data consists of 30 features and all presented methods work with 30 features, we restrict ourselves to 2 features, the <em>concavity</em> and the <em>texture</em> of the nuclei for illustrative purposes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;concavity_mean&quot;</span><span class="p">,</span> <span class="s2">&quot;texture_mean&quot;</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;bias&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;bias&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(398, 2)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>concavity_mean</th>
      <th>texture_mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>249</th>
      <td>0.043280</td>
      <td>14.93</td>
    </tr>
    <tr>
      <th>58</th>
      <td>0.000692</td>
      <td>19.31</td>
    </tr>
    <tr>
      <th>476</th>
      <td>0.050630</td>
      <td>20.53</td>
    </tr>
    <tr>
      <th>529</th>
      <td>0.037810</td>
      <td>13.44</td>
    </tr>
    <tr>
      <th>422</th>
      <td>0.070970</td>
      <td>16.02</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Note that we have added a constant feature to the matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> with the column name ‘bias’.</p>
<section id="binary-classificaiton">
<h3>Binary Classificaiton<a class="headerlink" href="#binary-classificaiton" title="Link to this heading">#</a></h3>
<p><strong>Classification</strong> refers to the task of predicting a <strong>class label</strong> <span class="math notranslate nohighlight">\(y\)</span>, <em>i.e.</em>, the diagnosis, from a <strong>feature vector</strong> <span class="math notranslate nohighlight">\(\bf{x}\)</span>.
For the case, where <span class="math notranslate nohighlight">\(y\)</span> can take one of two values, we speak of binary classification.</p>
<p>In machine learning, we assume that we are given pairs of <span class="math notranslate nohighlight">\((\mathbf{x}, y)\)</span>, the so-called <strong>training data</strong>, we would like to <strong>train</strong> a function <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> that predicts the value of <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>For the task at hand, this means that for the image features, we
Then given a new image for which we don’t know the diagnosis, we can predict the diagnosis based on what we have learned from from the training data.
We call the new image the <strong>test data</strong>.</p>
<p>The shape of the nulei has been determined and coded in a number of features.
Let’s look at the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Benign samples    &#39;B&#39;: </span><span class="si">{:}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">((</span><span class="n">y</span><span class="o">==</span><span class="s1">&#39;B&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Malignant samples &#39;M&#39;: </span><span class="si">{:}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">((</span><span class="n">y</span><span class="o">==</span><span class="s1">&#39;M&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(398,)
[&#39;B&#39; &#39;M&#39;]
Benign samples    &#39;B&#39;: 249
Malignant samples &#39;M&#39;: 149
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="plot-the-data">
<h1>plot the data<a class="headerlink" href="#plot-the-data" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">scatter_plot_kde2</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span><span class="mf">39.9</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.45</span><span class="p">])</span>
<span class="c1"># plt.savefig(&quot;./uci_breast_cancer/plots/scatter.png&quot;, dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-0.01, 0.45)
</pre></div>
</div>
<img alt="../_images/44e6ac127b26e75c82cd0f7c43eb8d20cd4710760c0977495b3db20107e54699.png" src="../_images/44e6ac127b26e75c82cd0f7c43eb8d20cd4710760c0977495b3db20107e54699.png" />
</div>
</div>
<p>There are many ways to draw a function that separates the samples. But what is a good one?</p>
<p>In this lecture, we will look for a <strong>linear</strong> function for the features <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> that separates the two classes.</p>
<div class="math notranslate nohighlight">
\[
x_{1} \cdot w_1 + x_{2} \cdot w_2 + b =0
\]</div>
<p>Equivalently, we can use vector-notation:</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}\mathbf{w}=0,\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\mathbf{x}=
\left[
\begin{matrix}
x_{1} &amp; x_{2} &amp; 1
\end{matrix}
\right],
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{w}=
\left[
\begin{matrix}
w_{1} \\ w_{2} \\ b
\end{matrix}
\right].\end{split}\]</div>
<p>Note, that we have included the bias <span class="math notranslate nohighlight">\(b\)</span> into the vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> by creating a new feature in <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> equal to 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">importlib</span><span class="o">.</span><span class="n">reload</span><span class="p">(</span><span class="n">util</span><span class="p">)</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="p">,</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">plotfun2D_logreg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span><span class="mf">39.9</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.45</span><span class="p">])</span>
<span class="c1"># plt.savefig(&quot;./uci_breast_cancer/plots/scatter_decision_boundary.png&quot;, dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-0.01, 0.45)
</pre></div>
</div>
<img alt="../_images/cb80f14f9ee62d25a0e87e549095c3a6904d2de9533e71a54e60a817179a67c2.png" src="../_images/cb80f14f9ee62d25a0e87e549095c3a6904d2de9533e71a54e60a817179a67c2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">importlib</span><span class="o">.</span><span class="n">reload</span><span class="p">(</span><span class="n">util</span><span class="p">)</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="p">,</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">plotfun2D_logreg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">second_line</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span><span class="mf">39.9</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.45</span><span class="p">])</span>
<span class="c1"># plt.savefig(&quot;./uci_breast_cancer/plots/scatter_decision_boundary_secondline.png&quot;, dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-0.01, 0.45)
</pre></div>
</div>
<img alt="../_images/a17542888a33125c991b25c99f333098ebd061de49320beb504bb7533d9b08ce.png" src="../_images/a17542888a33125c991b25c99f333098ebd061de49320beb504bb7533d9b08ce.png" />
</div>
</div>
<p>The classifier predicts all samples on one side of the <strong>decision boundary</strong> to belong to one class, all others to the other class.</p>
<p>As we observe, with a linear function, we have to accept that there are missclassifications, especially if we have a large training data set, the training data is rarely <strong>linearly separable</strong>.</p>
<section id="the-logistic-sigmoid">
<h2>The logistic sigmoid<a class="headerlink" href="#the-logistic-sigmoid" title="Link to this heading">#</a></h2>
<p>As the data are not separable, it is desirable to encode our degree of (un)-certainty by assigning a <strong>probability</strong> for a sample belonging to class <span class="math notranslate nohighlight">\(c_1\)</span> each sample.</p>
<p>In this example, <span class="math notranslate nohighlight">\(c_1\)</span> refers to a Malignant diagosis.</p>
<div class="math notranslate nohighlight">
\[
p(y=c_1|\mathbf{x}) = \pi(\mathbf{xw})
\]</div>
<p><span class="math notranslate nohighlight">\(y\)</span> is the class label of the sample and <span class="math notranslate nohighlight">\(\pi\)</span> is the <strong>logistic sigmoid</strong>.</p>
<p>The <strong>logistic</strong> <span class="math notranslate nohighlight">\(\pi(a)\)</span> is a function between 0 and 1, making it suited for modeling probabilities. It is called a <strong>sigmoid</strong> function because of its <em>s</em>-shape.</p>
<div class="math notranslate nohighlight">
\[
\pi(a) := \frac{1}{1+\exp{\left(-a\right)}} = \frac{\exp{\left(a\right)}}{1+\exp{\left(a\right)}}
\]</div>
<p>Here, as we are modeling linear functions, <span class="math notranslate nohighlight">\(a=\mathbf{x}_n\mathbf{w}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span> is the <strong>feature vector</strong> for the <span class="math notranslate nohighlight">\(n\)</span>-th individual (given), and <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> is a <strong>weight vector</strong> that we would like to find.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    returns the logistic sigmoid pi(a)</span>
<span class="sd">    Keyword arguments:</span>
<span class="sd">    a -- scalar or numpy array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">expa</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">expa</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">expa</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">8.0</span><span class="p">,</span><span class="mf">8.0</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># create 100 points on a line</span>
<span class="c1"># Set up the figure</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">linex</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mf">0.003</span><span class="p">)</span>
<span class="n">liney</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="s1">&#39;:k&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="c1"># plt.plot(a, 1.0-logistic(a), &#39;k:&#39;, linewidth=5, alpha=0.5)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.02</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">])</span>

<span class="c1"># ax.patch.set_facecolor(&#39;white&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;decision function&#39;</span><span class="p">,</span><span class="sa">r</span><span class="s1">&#39;$\pi(\mathbf</span><span class="si">{xw}</span><span class="s1">)$&#39;</span><span class="p">,</span><span class="s1">&#39;B&#39;</span><span class="p">,</span><span class="s2">&quot;R&quot;</span><span class="p">])</span>

<span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">linex</span><span class="p">,</span> <span class="n">liney</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">xx1</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;bwr&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">patch</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>


<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{xw}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(y=c_1|\mathbf</span><span class="si">{x}</span><span class="s1">)$&#39;</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
<span class="n">Xw</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
<span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">Xw</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="o">==</span><span class="s1">&#39;M&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">Xw</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="o">==</span><span class="s1">&#39;B&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;decision function&#39;</span><span class="p">,</span><span class="sa">r</span><span class="s1">&#39;$\pi(\mathbf</span><span class="si">{xw}</span><span class="s1">)$&#39;</span><span class="p">,</span><span class="s1">&#39;$c_1$ (M)&#39;</span><span class="p">,</span><span class="s2">&quot;$c_2$ (B)&quot;</span><span class="p">])</span>


<span class="c1"># plt.scatter(Xw, (y.values[:,np.newaxis]==&quot;M&quot;) , (y.values[:,np.newaxis]==&quot;M&quot;), size=20)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;The logistic sigmoid&quot;</span><span class="p">)</span>
<span class="c1"># plt.savefig(&quot;./uci_breast_cancer/plots/logistic_sigmoid_data.png&quot;, dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/291124841f3e66e99a66ea54197679adc1944edbd40d7afbfa510e66caf130c0.png" src="../_images/291124841f3e66e99a66ea54197679adc1944edbd40d7afbfa510e66caf130c0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">8.0</span><span class="p">,</span><span class="mf">8.0</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># create 100 points on a line</span>
<span class="c1"># Set up the figure</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">linex</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mf">0.003</span><span class="p">)</span>
<span class="n">liney</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># xx1, xx2 = np.meshgrid(linex, liney)</span>
<span class="c1"># plt.pcolormesh(xx1, xx2, logistic(xx1), cmap=&#39;bwr&#39;, alpha=0.1)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="s1">&#39;:k&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="c1"># plt.plot(a, 1.0-logistic(a), &#39;k:&#39;, linewidth=5, alpha=0.5)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.02</span><span class="p">,</span><span class="mf">1.02</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">8</span><span class="p">])</span>

<span class="c1"># ax.patch.set_facecolor(&#39;white&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;decision function&#39;</span><span class="p">,</span><span class="sa">r</span><span class="s1">&#39;$\pi(\mathbf</span><span class="si">{xw}</span><span class="s1">)$&#39;</span><span class="p">,</span><span class="s1">&#39;B&#39;</span><span class="p">,</span><span class="s2">&quot;R&quot;</span><span class="p">])</span>


<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{xw}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$p(y=c_1|\mathbf</span><span class="si">{x}</span><span class="s1">)$&#39;</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
<span class="n">Xw</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
<span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">Xw</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="o">==</span><span class="s1">&#39;B&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;B&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">Xw</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="o">==</span><span class="s1">&#39;M&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;M&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># plt.scatter(Xw, (y.values[:,np.newaxis]==&quot;M&quot;) , (y.values[:,np.newaxis]==&quot;M&quot;), size=20)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;The logistic sigmoid&quot;</span><span class="p">)</span>
<span class="c1"># plt.savefig(&quot;./uci_breast_cancer/plots/logistic_sigmoid_data.png&quot;, dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/df04f58c1cb4604fc4aabf71bc5fd8a6beae969340981123a961591894ccb041.png" src="../_images/df04f58c1cb4604fc4aabf71bc5fd8a6beae969340981123a961591894ccb041.png" />
</div>
</div>
<p>Conversely, the probabily to belong to the class <span class="math notranslate nohighlight">\(c_2\)</span> (Benign) is given by <span class="math notranslate nohighlight">\(1-\pi(a)\)</span>.</p>
<div class="math notranslate nohighlight">
\[
p(y_n=c_2|\mathbf{x}_n; \mathbf{w})= 1-\pi(\mathbf{x}_n\mathbf{w}) = \frac{1}{1+\exp{(\mathbf{x}_n\mathbf{w}})} 
\]</div>
</section>
<section id="objective">
<h2>Objective<a class="headerlink" href="#objective" title="Link to this heading">#</a></h2>
<p>We need to find a way to obtain a suitable set of weights.</p>
<p>We do so by writing down a function, the so-called <strong>objective</strong> function <span class="math notranslate nohighlight">\(L\)</span> and then determine the weights <span class="math notranslate nohighlight">\(\mathbf{w}^{opt}\)</span> that minimize <span class="math notranslate nohighlight">\(L\)</span>.</p>
<p>We would like to assign high probability to all the instances that belong to the target class and low probability otherwise.</p>
</section>
<section id="loss-function">
<h2>Loss Function<a class="headerlink" href="#loss-function" title="Link to this heading">#</a></h2>
<p>The <strong>loss function</strong> measures the <strong>errors</strong> we make on the <strong>training data</strong>.
So for each error, we record a loss.
As we are trying to assign high probabilities to the correct class, we would like to obtain a function that records a loss, whenever we assign low probability to the correct class <span class="math notranslate nohighlight">\(c_{true}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.00001</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.00001</span><span class="p">)</span>
<span class="n">yy</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span><span class="n">yy</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;error function for a single $y_n$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$-\ln( p(c_</span><span class="si">{true}</span><span class="s2">|\mathbf</span><span class="si">{x}</span><span class="s2">) )$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$p(y_n=c_</span><span class="si">{true}</span><span class="s2">|\mathbf</span><span class="si">{x}</span><span class="s2">)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
<span class="c1"># plt.savefig(&quot;./uci_breast_cancer/plots/log_error.png&quot;, dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.0, 10.0)
</pre></div>
</div>
<img alt="../_images/ca97a551d5c80d3248c7ad58aff37c77d17b398f7c7925ea14a987354d4f5c5d.png" src="../_images/ca97a551d5c80d3248c7ad58aff37c77d17b398f7c7925ea14a987354d4f5c5d.png" />
</div>
</div>
<p>By summing this loss over all samples, we obtain the <strong>log-loss</strong> or <strong>cross-entropy</strong> function.</p>
<div class="math notranslate nohighlight">
\[
loss = -\sum_{n\in c_1} \ln( \pi(\mathbf{x}_n\mathbf{w}) ) - \sum_{n'\in c_2} \ln( 1-\pi(\mathbf{x}_{n'}\mathbf{w}) )
\]</div>
</section>
<section id="regularizer">
<h2>Regularizer<a class="headerlink" href="#regularizer" title="Link to this heading">#</a></h2>
<p>In addition to the loss, we define the regularizer as a fcuntion that penalizes very large values of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and to improve numerical stability.
In this example, we use the square of the Euclidean norm of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> as regularizer.</p>
<div class="math notranslate nohighlight">
\[
regularizer = 0.5 \cdot \lambda \cdot \sum_{d=1}^{D}{w_d}^2
\]</div>
<p>The (hyper)-parameter <span class="math notranslate nohighlight">\(\lambda\)</span> weighs the importance of the regularizer vs. the loss. In this example, we always use <span class="math notranslate nohighlight">\(\lambda=10^{-3}\)</span>.</p>
<p>In-depth discussion of the regularizer is beyond the scope of this lecture.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">yy</span> <span class="o">=</span> <span class="mf">0.001</span><span class="o">*</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">xx</span> <span class="o">*</span> <span class="n">xx</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span><span class="n">yy</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.00006</span><span class="p">,</span><span class="mf">0.013</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$w_d$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;regularizer ($\lambda=10^{-3}$)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Weight Shrinkage&quot;</span><span class="p">)</span>
<span class="c1"># plt.savefig(&quot;./uci_breast_cancer/plots/regularizer.png&quot;,dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Weight Shrinkage&#39;)
</pre></div>
</div>
<img alt="../_images/ea280625f6dcca8a85696cdd3ba86f8a0f652fae22c16305b49aaf1729e023a8.png" src="../_images/ea280625f6dcca8a85696cdd3ba86f8a0f652fae22c16305b49aaf1729e023a8.png" />
</div>
</div>
</section>
<section id="combined-objective">
<h2>Combined objective<a class="headerlink" href="#combined-objective" title="Link to this heading">#</a></h2>
<p>Taken together, we obtain the <strong>Logistic Regression objective</strong> <span class="math notranslate nohighlight">\(L(\mathbf{w})\)</span>.</p>
<div class="math notranslate nohighlight">
\[
L(\mathbf{w}) = \underbrace{-\sum_{n\in c_1} \ln( \pi(\mathbf{x}_n\mathbf{w}) ) - \sum_{n'\in c_2} \ln( 1-\pi(\mathbf{x}_{n'}\mathbf{w}) )}_{loss} + \underbrace{ \lambda \cdot 0.5 \cdot \sum_{d=1}^{D}{w_d}^2}_{regularizer}
\]</div>
</section>
<section id="fitting-the-model">
<h2>Fitting the model<a class="headerlink" href="#fitting-the-model" title="Link to this heading">#</a></h2>
<p>Now that we have an objective, how do we obtain a good set of parameters?</p>
<p>Let’s look at the objective in 2 dimensions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">importlib</span><span class="o">.</span><span class="n">reload</span><span class="p">(</span><span class="n">util</span><span class="p">)</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">eval_optimizer</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="c1"># plt.savefig(&quot;./uci_breast_cancer/plots/objective_heatmap_2d.png&quot;, dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[iteration 7, 0.0007s]: objective: 1.141e+02, gradient l2 norm : 1.091e-07
</pre></div>
</div>
<img alt="../_images/1f8b3524c17a8f5a1148d5047eac5892563888d4790b87d58b48ff007d5b19d1.png" src="../_images/1f8b3524c17a8f5a1148d5047eac5892563888d4790b87d58b48ff007d5b19d1.png" />
</div>
</div>
<p>The image visualizes the objective <span class="math notranslate nohighlight">\(L\)</span> as a function of the two weights <span class="math notranslate nohighlight">\(w_1\)</span> and <span class="math notranslate nohighlight">\(w_2\)</span>, with objective values going from low (yellow) to high (red). The <strong>minimum</strong> is marked by a blue dot.</p>
<section id="taking-the-derivative">
<h3>Taking the derivative<a class="headerlink" href="#taking-the-derivative" title="Link to this heading">#</a></h3>
<p>The derivative of the objective with respect to a single <span class="math notranslate nohighlight">\(w_d\)</span> is defined as follows:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L}{\partial w_d} = \sum_n^{N}{x_{nd}} \cdot
 \left( \pi\left(\mathbf{x}_n\mathbf{w}_n\right)-I\left(\mathbf{y}_n== c_1\right)\right) + \lambda \cdot w_d
\]</div>
<p><span class="math notranslate nohighlight">\(I(a==b)\)</span> denotes the indicator function, which yields 1 if <span class="math notranslate nohighlight">\(a=b\)</span> and 0 otherwise.</p>
<p>The sign of the derivative indicates the direction in which the objective gets larger or smaller and the magnitude the rate.</p>
</section>
<section id="the-gradient">
<h3>The gradient<a class="headerlink" href="#the-gradient" title="Link to this heading">#</a></h3>
<p>By stacking all partial derivatives into a single vector, we obtain the gradient <span class="math notranslate nohighlight">\(\nabla_\mathbf{w} (L)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla{L}\left(\mathbf{w}^{t}\right) =
\left[\begin{matrix}
\frac{\partial L}{\partial w^t_1}\\
\vdots\\
\frac{\partial L}{\partial w^t_D}
\end{matrix}\right]
=
\underbrace{\mathbf{X}^{T}
 \left( \pi\left(\mathbf{X}\mathbf{w}^t\right)-I\left(\mathbf{y}==c_1\right)\right)}_{\nabla{\text{loss}}\left(\mathbf{w}^{t}\right)}+ \underbrace{\lambda \cdot \mathbf{w}^t}_{\nabla{\text{regularizer}}\left(\mathbf{w}^{t}\right)}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\nabla_\mathbf{w} (L)\)</span> is a <span class="math notranslate nohighlight">\(D\)</span>-dimensional vector pointing in the direction of steepest growth of the objective and in the opposite direction in which the steepest reduction.
Using the gradient, we can define a simple optimization algorithm.</p>
<section id="steepest-descent">
<h4>Steepest descent<a class="headerlink" href="#steepest-descent" title="Link to this heading">#</a></h4>
<p>The steepest descent algorithm uses the gradient by making small steps in the direction <span class="math notranslate nohighlight">\(-\nabla_{\mathbf{w}^{t}} (L)\)</span>. You can think about it as being on a hill and descending the hill in the steepest direction downwards.
Therefore the algorithm is called <strong>steepest descent</strong>.</p>
<p>given learning rate <span class="math notranslate nohighlight">\(0&lt;\alpha&lt;1.0\)</span> and current weight estimate <span class="math notranslate nohighlight">\(\mathbf{w}^{t}\)</span>.
Iterate by setting <span class="math notranslate nohighlight">\(\mathbf{w}^{t+1} = \mathbf{w}^{t} - \alpha \cdot \nabla_{\mathbf{w}^{t}} (L)\)</span>.</p>
<p>A typical value for <span class="math notranslate nohighlight">\(\alpha\)</span> is around <span class="math notranslate nohighlight">\(10^{-4}\)</span>.</p>
<p>A problem with steepest descent is that the estimate tends to oscillate and often even overshoots and diverges (leading to an increase in the objetive). Getting the learning rate right is very hard, trading off progress in learning and risk of diverging. Many tricks exit to improve learning in gradient descent, such as weight decay, where the learning rate is gradually reduced during learning.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">eval_optimizer</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">steep</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># plt.savefig(&quot;./uci_breast_cancer/plots/objective_heatmap_2d_steepest.png&quot;, dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[iteration 7, 0.0008s]: objective: 1.141e+02, gradient l2 norm : 1.091e-07
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[iteration 200000, 9.2913s]: objective: 1.142e+02, gradient l2 norm : 1.310e-01
</pre></div>
</div>
<img alt="../_images/eda167e06517284deb6f543f84fd46da32606966fbb735cd3da3f6e03a51c616.png" src="../_images/eda167e06517284deb6f543f84fd46da32606966fbb735cd3da3f6e03a51c616.png" />
</div>
</div>
<p>In the image above, where we applied weight decay, multiplying the learning rate by <span class="math notranslate nohighlight">\(1-10^{-6}\)</span> after each iteration, the learning does not converge to the minimum (the blue dot) after 200.000 iterations.</p>
</section>
<section id="using-curvature-information">
<h4>Using curvature information<a class="headerlink" href="#using-curvature-information" title="Link to this heading">#</a></h4>
<p>We can build a better learning algorithm by using second order information, utilizing a second-order Taylor-series expansion around the current weight estimate <span class="math notranslate nohighlight">\(\mathbf{w}^t\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">importlib</span><span class="o">.</span><span class="n">reload</span><span class="p">(</span><span class="n">util</span><span class="p">)</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">eval_optimizer1D</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">taylor1</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">taylor2</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># plt.savefig(&quot;./uci_breast_cancer/plots/derivative_1D.png&quot;, dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[iteration 7, 0.0007s]: objective: 1.141e+02, gradient l2 norm : 1.091e-07
</pre></div>
</div>
<img alt="../_images/e82e17482951f89ded9955733b0f414e45e017064606c85496aeec40acfdcb71.png" src="../_images/e82e17482951f89ded9955733b0f414e45e017064606c85496aeec40acfdcb71.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">importlib</span><span class="o">.</span><span class="n">reload</span><span class="p">(</span><span class="n">util</span><span class="p">)</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">eval_optimizer1D</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">taylor1</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">taylor2</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># plt.savefig(&quot;./uci_breast_cancer/plots/newton_step_1D.png&quot;, dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[iteration 7, 0.0007s]: objective: 1.141e+02, gradient l2 norm : 1.091e-07
</pre></div>
</div>
<img alt="../_images/aef24da1d22414a8c355fd8f43a2449185a54c3e442674540549d54afcbea6b7.png" src="../_images/aef24da1d22414a8c355fd8f43a2449185a54c3e442674540549d54afcbea6b7.png" />
</div>
</div>
<p>The image depicts the idea for a single weight <span class="math notranslate nohighlight">\(w_1\)</span>. At the current estimate <span class="math notranslate nohighlight">\(w_1^{t}\)</span>, we compute the first and second derivatives of the objective to form a parabolic fit to the objective <span class="math notranslate nohighlight">\(L\)</span>. Then, we obtain <span class="math notranslate nohighlight">\(w_1^{t+1}\)</span> as the minimum of that parabolic and iterate.</p>
</section>
</section>
<section id="the-hessian">
<h3>The Hessian<a class="headerlink" href="#the-hessian" title="Link to this heading">#</a></h3>
<p>In higher dimensions, the matrix second derivatives is called the Hessian <span class="math notranslate nohighlight">\(\nabla^2_{\mathbf{w}}(L)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_{\mathbf{w}^{t}}  =
\left[\begin{matrix}
{\partial^{2} L }/{\partial^2 w_1} &amp; {\partial^{2} L }/{\partial w_1 \partial w_2} &amp; \dots &amp; {\partial^{2} L }/{\partial w_1 \partial w_D}\\
\vdots &amp; &amp;\ddots  &amp;\vdots\\
{\partial^{2} L }/{\partial w_D \partial w_1} &amp; {\partial^{2} L }/{\partial w_D \partial w_2} &amp; \dots &amp; {\partial^{2} L }/{\partial^2 w_D}
\end{matrix}\right] 
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
\mathbf{H}_{\mathbf{w}^{t}}= 
\underbrace{
\left(\mathbf{X}  
\mathrm{diag}\left(
\pi\left(\mathbf{X}\mathbf{w}^{t}\right) \cdot 
\left(\mathbf{1}-\pi(\mathbf{X}\mathbf{w}^{t}\right)
\right)
\right)^{T} \mathbf{X} 
}_{\mathbf{H}_{\mathbf{w}^{t}} (\mathrm{loss})}+\underbrace{ \lambda \cdot \mathbf{I}_{D\times D}}_{\mathbf{H}_{\mathbf{w}^{t}} (\mathrm{regularizer})}
\]</div>
</section>
<section id="the-newton-raphson-algorithm">
<h3>The Newton-Raphson algorithm<a class="headerlink" href="#the-newton-raphson-algorithm" title="Link to this heading">#</a></h3>
<p>The update rule using second order information can be derived by iterating the following update until the Euclidiean (<span class="math notranslate nohighlight">\(l^2\)</span>) norm of the gradient is close to 0.</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^{t+1} = \mathbf{w}^{t} - \mathbf{H}_{\mathbf{w}^t}^{-1} \nabla{L}\left(\mathbf{w}^{t}\right)
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">eval_optimizer</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">steep</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">irls</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># plt.savefig(&quot;./uci_breast_cancer/plots/objective_heatmap_2d_steepest_irls.png&quot;, dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[iteration 7, 0.0012s]: objective: 1.141e+02, gradient l2 norm : 1.091e-07
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[iteration 200000, 9.2889s]: objective: 1.142e+02, gradient l2 norm : 1.310e-01
</pre></div>
</div>
<img alt="../_images/40a80d4cfc4090b15d0dd8c4d1074d3e314232bf8b0ae282d99358018524349d.png" src="../_images/40a80d4cfc4090b15d0dd8c4d1074d3e314232bf8b0ae282d99358018524349d.png" />
</div>
</div>
<p>The blue line corresponds to minimization using the Newton-Raphson algorithm.
We observe that the curvature information provided by <span class="math notranslate nohighlight">\(\mathbf{H}_{\mathbf{w}^{t}}\)</span> dramatically speeds up optimization, with convergence achieved after 7 iterations.</p>
</section>
<section id="the-implementation-of-logistic-regression">
<h3>The implementation of logistic regression<a class="headerlink" href="#the-implementation-of-logistic-regression" title="Link to this heading">#</a></h3>
<p>Let’s put things together and implement logistic regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements logistic regression classifier with the objective</span>
<span class="sd">    sum_{n in class_{1}} log(pi(x_n.dot(w))) + sum_{n in class_{0}} log(1-pi(x_n.dot(w))) + lambd/2 * w.T.dot(w)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Keyword arguments:</span>
<span class="sd">        lambd     -- regularization paramter for L2 norm of w (scalar or numpy 1D array with length equal to the number of dimensions) (default: 1e-5)</span>
<span class="sd">        tol       -- tolerance of the optimizer (default: 1e-5)</span>
<span class="sd">        max_iter  -- maximum number of interations of the optimizer (default: 100)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># create a placeholer for the weights w</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_labels</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># crete a placeholder for the list of class labels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambd</span> <span class="o">=</span> <span class="n">lambd</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        minimize the objective</span>
<span class="sd">        sum_{n in class_{1}}  log(pi(x_n.dot(w))) + sum_{n in class_{0}} log(1-pi(x_n.dot(w))) + lambd/2 * w.T.dot(w)</span>
<span class="sd">        &quot;&quot;&quot;</span>
            
        <span class="bp">self</span><span class="o">.</span><span class="n">class_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">class_labels</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;too many classes&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># zero-init w</span>
        <span class="n">num_iter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">gradient_last</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span> <span class="c1"># initialize the gradient to a large number</span>
        <span class="c1"># Newton-Raphson / IRLS updates</span>
        <span class="k">while</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">gradient_last</span><span class="o">*</span><span class="n">gradient_last</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">num_iter</span><span class="o">&lt;</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="n">gradient_last</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">perform_update</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
            <span class="n">num_iter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="bp">self</span>
    
    <span class="k">def</span> <span class="nf">perform_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        compute and perform a single step in the iterative optimization scheme</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># compute the Iteratively Reweighted Least Squares update (equiv. Newton-Raphson)</span>
        <span class="n">hessian</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hessian</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="n">update</span> <span class="o">=</span> <span class="o">-</span> <span class="n">la</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">hessian</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+</span> <span class="n">update</span>
        <span class="k">return</span> <span class="n">gradient</span>
    
    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        compute the probabilities of the lexicographically larger class label</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">logistic</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">))</span>
        <span class="n">z</span><span class="p">[</span><span class="n">z</span><span class="o">&lt;</span><span class="n">min_val</span><span class="p">]</span> <span class="o">=</span> <span class="n">min_val</span>
        <span class="n">z</span><span class="p">[</span><span class="n">z</span><span class="o">&gt;</span><span class="mi">1</span><span class="o">-</span><span class="n">min_val</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">min_val</span>
        <span class="k">return</span> <span class="n">z</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        predict a class label using pi(x)&gt;=threshold</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prediction</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">class_labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">prediction</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_labels</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> 
        <span class="k">return</span> <span class="n">prediction</span>
    
    <span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        L = sum_{n in class_{1}}  log(pi(x_n.dot(w))) + sum_{n in class_{0}} log(1-pi(x_n.dot(w))) + lambd/2 * w.T.dot(w)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">log_0_pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pi</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">class_labels</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">log_1_pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="n">pi</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">class_labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_0_pi</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">log_1_pi</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="c1"># this version is more stable for perfect prediction</span>
        <span class="n">regularizer</span> <span class="o">=</span>  <span class="mf">0.5</span><span class="o">*</span>  <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lambd</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>  <span class="o">+</span> <span class="n">regularizer</span>
    
    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        compute the [D x 1] gradient vector</span>
<span class="sd">        nabla w := [dL / dw_j for each j in 1..D]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">pi</span><span class="o">-</span><span class="p">(</span><span class="n">y</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">class_labels</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span><span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambd</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span>
    
    <span class="k">def</span> <span class="nf">hessian</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        compute the [D x D] Hessian matrix</span>
<span class="sd">        nabla^2 w := [d^2 L / (dw_i dw_j) for each i,j in 1..D]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pi</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span>  <span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="p">(</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="n">pi</span><span class="p">)))</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambd</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluating-the-model">
<h3>Evaluating the model<a class="headerlink" href="#evaluating-the-model" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\text{accuracy} = \frac{1}{N} \text{\# correctly classified}
\]</div>
<section id="confusion-matrix">
<h4>Confusion matrix<a class="headerlink" href="#confusion-matrix" title="Link to this heading">#</a></h4>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Predicted Positive</p></th>
<th class="head text-center"><p>Predicted Negative</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Positive</strong></p></td>
<td><p>True Positive (<span class="math notranslate nohighlight">\(TP\)</span>)</p></td>
<td class="text-center"><p>False Negative (<span class="math notranslate nohighlight">\(FN\)</span>)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Negative</strong></p></td>
<td><p>False Positive (<span class="math notranslate nohighlight">\(FP\)</span>)</p></td>
<td class="text-center"><p>True Negative (<span class="math notranslate nohighlight">\(TN\)</span>)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="trade-off-between-sensitivity-and-specificity">
<h4>Trade-off between Sensitivity and Specificity<a class="headerlink" href="#trade-off-between-sensitivity-and-specificity" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\text{sensitivity} = \frac{TP}{TP + FN}
\]</div>
<div class="math notranslate nohighlight">
\[
\text{specificity} = \frac{TP}{TP + FP}
\]</div>
</section>
</section>
<section id="train-error-vs-test-error">
<h3>Train error vs. Test error<a class="headerlink" href="#train-error-vs-test-error" title="Link to this heading">#</a></h3>
<p>Let us compute these quantities on the test data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">importlib</span><span class="o">.</span><span class="n">reload</span><span class="p">(</span><span class="n">util</span><span class="p">)</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span><span class="mf">3.8</span><span class="p">))</span>
<span class="n">util</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">x_test</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y_test</span><span class="o">=</span><span class="n">y</span><span class="p">,</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># plt.savefig(&quot;confusion_matrix_train.png&quot;,dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>number samples: 398
number M: 149
number B: 249
Accuracy   : 0.902
Sensitivity: 0.826
Specificity: 0.904
</pre></div>
</div>
<img alt="../_images/b3e6046ef8aa7efa172fc36643721e9e1770ec29705c48ef85a3ef9fcd302981.png" src="../_images/b3e6046ef8aa7efa172fc36643721e9e1770ec29705c48ef85a3ef9fcd302981.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">importlib</span><span class="o">.</span><span class="n">reload</span><span class="p">(</span><span class="n">util</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">testing_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;concavity_mean&quot;</span><span class="p">,</span> <span class="s2">&quot;texture_mean&quot;</span><span class="p">])</span>

<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">4.5</span><span class="p">,</span><span class="mf">3.8</span><span class="p">))</span>
<span class="n">util</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">x_test</span><span class="o">=</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="c1"># plt.savefig(&quot;confusion_matrix_test.png&quot;,dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>number samples: 171
number M: 63
number B: 108
Accuracy   : 0.854
Sensitivity: 0.762
Specificity: 0.828
</pre></div>
</div>
<img alt="../_images/f17d9a85257153cbcb5f3d22bd08f15b96052ca6a3832f69103b084dbdd4dc07.png" src="../_images/f17d9a85257153cbcb5f3d22bd08f15b96052ca6a3832f69103b084dbdd4dc07.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">importlib</span><span class="o">.</span><span class="n">reload</span><span class="p">(</span><span class="n">util</span><span class="p">)</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span><span class="p">,</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">plotfun2D_logreg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">X_test</span><span class="o">=</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span><span class="mf">39.9</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.45</span><span class="p">])</span>
<span class="c1"># plt.savefig(&quot;./uci_breast_cancer/plots/scatter_decision_boundary_test.png&quot;, dpi=600)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-0.01, 0.45)
</pre></div>
</div>
<img alt="../_images/666f04392f5601645cd9461d2a12f7488a2db1df5e24a05ba7e7ef50fc2e2085.png" src="../_images/666f04392f5601645cd9461d2a12f7488a2db1df5e24a05ba7e7ef50fc2e2085.png" />
</div>
</div>
</section>
</section>
<section id="conclusions">
<h2>Conclusions<a class="headerlink" href="#conclusions" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>use data to derive a program instead of relying on expert</p></li>
<li><p>need a target objective to learn and regularization to avoid overfitting</p></li>
<li><p>predictions are uncertain -&gt; probabilities</p></li>
<li><p>fitting to data</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<p>[Street et al, 1992] N. Street, W. Wolberg, O.L. Mangasarian:  Nuclear Feature Extraction For Breast Tumor Diagnosis. IS&amp;T/SPIE 1993.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./logistic_regression_uci_breast_cancer"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter_probability/probability.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Probability</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Machine Learning for Human Phenotype Classification</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diagnosing-breast-cancer-biopsies-using-logistic-regression">Diagnosing Breast cancer biopsies using Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-classificaiton">Binary Classificaiton</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#plot-the-data">plot the data</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-logistic-sigmoid">The logistic sigmoid</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objective">Objective</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularizer">Regularizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combined-objective">Combined objective</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-model">Fitting the model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#taking-the-derivative">Taking the derivative</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gradient">The gradient</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#steepest-descent">Steepest descent</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-curvature-information">Using curvature information</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hessian">The Hessian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-newton-raphson-algorithm">The Newton-Raphson algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-implementation-of-logistic-regression">The implementation of logistic regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-model">Evaluating the model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">Confusion matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#trade-off-between-sensitivity-and-specificity">Trade-off between Sensitivity and Specificity</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-error-vs-test-error">Train error vs. Test error</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusions">Conclusions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christoph Lippert
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>