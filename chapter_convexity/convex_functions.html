
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Basics of convex functions &#8212; Mathematics for Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_convexity/convex_functions';</script>
    <link rel="canonical" href="https://healthml.github.io/Math4ML/chapter_convexity/convex_functions.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Second-Order Calculus and Optimization" href="../chapter_second_order/overview_second_order.html" />
    <link rel="prev" title="Convex sets" href="convex_sets.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/hpi-logo-colored.svg" class="logo__image only-light" alt="Mathematics for Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/hpi-logo-colored.svg" class="logo__image only-dark" alt="Mathematics for Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Preface
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Mathematics for Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_ml_basics/intro.html">Machine Learning Problems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_ml_basics/classification.html">Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_ml_basics/regression.html">Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_ml_basics/clustering.html">Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_ml_basics/representation_learning.html">Representation Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_spaces/overview_spaces.html">Vector and Function Spaces</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/vector_spaces.html">Vector Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/polynomial_vector_space.html">Polynomial Vector Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/basis_functions_vector_space.html">Basis Functions Vector Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/subspaces.html">Subspaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/metric_spaces.html">Metric Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/normed_spaces.html">Normed Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/inner_product_spaces.html">Inner Product Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/transposition.html">Transposition</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_calculus/overview_calculus.html">Calculus and Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/extrema.html">Extrema</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/gradients.html">Gradients</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/gradient_descent_ridge.html">Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/matrix_calculus.html">Matrix Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/jacobian.html">Jacobian</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/chain_rule.html">Chain Rule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/mean_value_theorem.html">Mean Value Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/minima_first_order_condition.html">First Order Condition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/analytical_solution_ridge.html">Quadratic Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/line_search.html">Line Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/hessian.html">Hessian</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/taylors_theorem.html">Taylor’s Theorem</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_decompositions/overview_decompositions.html">Matrix Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/matrix_rank.html">Rank of a Matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/determinant.html">Determinant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/row_equivalence.html">Gaussian Elimination and the PLU Decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/square_matrices.html">Fundamental Equivalences for Square matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/trace.html">Trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/eigenvectors.html">Eigenvalues and Eigenvectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/orthogonal_matrices.html">Orthogonal matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/symmetric_matrices.html">Symmetric matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/Rayleigh_quotients.html">Rayleigh Quotients</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/matrix_norms.html">Matrix Norms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/psd_matrices.html">Positive (semi-)definite matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/pca.html">Principal Components Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/svd.html">Singular value decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/pseudoinverse.html">Moore-Penrose Pseudoinverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/orthogonal_projections.html">Orthogonal projections</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_decompositions/big_picture.html">Fundamental Subspaces</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="overview_convexity.html">Convexity</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="convex_sets.html">Convex sets</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Basics of convex functions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_second_order/overview_second_order.html">Second-Order Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_second_order/minima_second_order_condition.html">Second Order Condition for Minima</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_second_order/newtons_method.html">Newton’s Method</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_probability/overview_probability.html">Probability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_probability/probability_basics.html">Probability Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_probability/random_variables.html">Random variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_probability/functions_of_random_variables.html">Functions of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_probability/expectation.html">Expected Value</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_probability/variance.html">Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_probability/covariance.html">Covariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_probability/random_vectors.html">Random vectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_probability/functions_random_vectors.html">Functions of Random Vectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_probability/joint_distributions.html">Joint distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_probability/gaussian.html">The Gaussian distribution</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendix/proofs.html">Detailed Proofs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix/Cauchy%E2%80%93Schwarz_inequality.html">Cauchy-Schwarz Inequality</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/Bolzano-Weierstrass_theorem.html">Bolzano-Weierstrass Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/extreme_value_theorem.html">Extreme Value Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/Rolles_theorem.html">Rolle's Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/mean_value_theorem_proof.html">Mean Value Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/scalar-scalar_chain_rule.html">Chain Rule for Scalar-Scalar Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/squeeze_theorem.html">Squeeze Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/first_fundamental_theorem_calculus.html">First Fundamental Theorem of Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/second_fundamental_theorem_calculus.html">Second Fundamental Theorem of Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/Clairauts_theorem.html">Clairaut's Theorem</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/differentiation_rules.html">Differentiation Rules</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendix/Exercise%20Sheet%20Solutions.html">Exercise Sheet Solutions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix/Exercise%20Sheet%201%20Solutions.html">Exercise Sheet 1 Solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/Exercise%20Sheet%202%20Solutions.html">Exercise Sheet 2 Solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/Exercise%20Sheet%203%20Solutions.html">Exercise Sheet 3 Solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/Exercise%20Sheet%204%20Solutions.html">Exercise Sheet 4 Solutions</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/HealthML/Math4ML" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/HealthML/Math4ML/edit/main/book/chapter_convexity/convex_functions.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/HealthML/Math4ML/issues/new?title=Issue%20on%20page%20%2Fchapter_convexity/convex_functions.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_convexity/convex_functions.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Basics of convex functions</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#consequences-of-convexity">Consequences of convexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#effects-of-changing-the-feasible-set">Effects of Changing the Feasible Set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#showing-that-a-function-is-convex">Showing that a function is convex</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">🧠 Intuition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity-and-gradient-descent-convergence">Convexity and Gradient Descent Convergence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-algorithm">Gradient Descent Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-convexity-matters">Why Convexity Matters</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-functions">1. <strong>Convex functions</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#strongly-convex-functions">2. <strong>Strongly convex functions</strong>:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-objectives-and-convexity">Machine Learning Objectives and Convexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-with-squared-loss">✅ 1. <strong>Linear Regression with Squared Loss</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">✅ 2. <strong>Ridge Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-binary-classification">✅ 3. <strong>Logistic Regression (Binary Classification)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-layer-neural-network">❌ 4. <strong>Two-Layer Neural Network</strong></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="basics-of-convex-functions">
<h1>Basics of convex functions<a class="headerlink" href="#basics-of-convex-functions" title="Link to this heading">#</a></h1>
<p>In the remainder of this section, assume
<span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span> unless otherwise noted. We’ll start
with the definitions and then give some results.</p>
<p>A function <span class="math notranslate nohighlight">\(f\)</span> is <strong>convex</strong> if</p>
<div class="math notranslate nohighlight">
\[f(t\mathbf{x} + (1-t)\mathbf{y}) \leq t f(\mathbf{x}) + (1-t)f(\mathbf{y})\]</div>
<p>for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \operatorname{dom} f\)</span> and all <span class="math notranslate nohighlight">\(t \in [0,1]\)</span>.</p>
<p>Geometrically, convexity means that the line segment between two points
on the graph of <span class="math notranslate nohighlight">\(f\)</span> lies on or above the graph itself.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Define a convex function</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Define x values and compute y</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Choose two points on the graph</span>
<span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.0</span>
<span class="n">y1</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">f</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>

<span class="c1"># Compute the line segment between the two points</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">xt</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">x2</span>
<span class="n">yt_line</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="n">y1</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span> <span class="o">*</span> <span class="n">y2</span>

<span class="c1"># Plot the function and the line segment</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) = x^2$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">yt_line</span><span class="p">,</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Line segment&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">],</span> <span class="p">[</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">],</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span>  <span class="c1"># endpoints</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Geometric Interpretation of Convexity&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/0d047f2eca9d0507d4ddae3b984159d8ec13daa6fb1e461bf5913eccdddc8fef.png" src="../_images/0d047f2eca9d0507d4ddae3b984159d8ec13daa6fb1e461bf5913eccdddc8fef.png" />
</div>
</div>
<p>If the inequality holds strictly (i.e. <span class="math notranslate nohighlight">\(&lt;\)</span> rather than <span class="math notranslate nohighlight">\(\leq\)</span>) for all
<span class="math notranslate nohighlight">\(t \in (0,1)\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{y}\)</span>, then we say that <span class="math notranslate nohighlight">\(f\)</span> is
<strong>strictly convex</strong>.</p>
<p>Strict convexity means that the graph of <span class="math notranslate nohighlight">\(f\)</span> lies strictly above the
line segment, except at the segment endpoints.</p>
<p>A function <span class="math notranslate nohighlight">\(f\)</span> is <strong>strongly convex with parameter <span class="math notranslate nohighlight">\(m\)</span></strong> (or
<strong><span class="math notranslate nohighlight">\(m\)</span>-strongly convex</strong>) if the function</p>
<div class="math notranslate nohighlight">
\[\mathbf{x} \mapsto f(\mathbf{x}) - \frac{m}{2}\|\mathbf{x}\|_2^2\]</div>
<p>is convex.</p>
<p>These conditions are given in increasing order of strength; strong
convexity implies strict convexity which implies convexity.</p>
<hr class="docutils" />
<p>A good way to gain intuition about the distinction between convex,
strictly convex, and strongly convex functions is to consider examples
where the stronger property fails to hold.</p>
<p>Functions that are convex but not strictly convex:</p>
<p>(i) <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \mathbf{w}^{\!\top\!}\mathbf{x} + \alpha\)</span> for any
<span class="math notranslate nohighlight">\(\mathbf{w} \in \mathbb{R}^d, \alpha \in \mathbb{R}\)</span>. Such a
function is called an <strong>affine function</strong>, and it is both convex and
concave.
(In fact, a function is affine if and only if it is both convex and concave.)
Note that linear functions and constant
functions are special cases of affine functions.</p>
<p>(ii) <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \|\mathbf{x}\|_1\)</span></p>
<p>Functions that are strictly but not strongly convex:</p>
<p>(i) <span class="math notranslate nohighlight">\(f(x) = x^4\)</span>.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>This example is interesting because it is strictly
convex but you cannot show this fact via a second-order argument
(since $f&#39;&#39;(0) = 0$).
</pre></div>
</div>
<p>(ii) <span class="math notranslate nohighlight">\(f(x) = \exp(x)\)</span>.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>This example is interesting because it&#39;s bounded
below but has no local minimum.
</pre></div>
</div>
<p>(iii) <span class="math notranslate nohighlight">\(f(x) = -\log x\)</span>.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>This example is interesting because it&#39;s
strictly convex but not bounded below.
</pre></div>
</div>
<p>Functions that are strongly convex:</p>
<p>(i) <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \|\mathbf{x}\|_2^2\)</span></p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">x_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>  <span class="c1"># for -log(x)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>

<span class="c1"># 1. Convex but not strictly convex</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) = 2x + 1$ (affine)&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Convex but not strictly convex: Affine&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) = |x|$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Convex but not strictly convex: $L_1$ norm&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 2. Strictly convex but not strongly convex</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) = x^4$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Strictly convex, not strongly convex: $x^4$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) = \exp(x)$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Strictly convex, not strongly convex: $\exp(x)$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 3. Strictly convex, not strongly convex: -log(x)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pos</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x_pos</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) = -\log x$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Strictly convex, not strongly convex: $-\log x$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 4. Strongly convex</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) = x^2$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;brown&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Strongly convex: $x^2$&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/73b2837986a4347ff42c3215b385f31d9a8f67a8a752328a2e00648d5f330da2.png" src="../_images/73b2837986a4347ff42c3215b385f31d9a8f67a8a752328a2e00648d5f330da2.png" />
</div>
</div>
<section id="consequences-of-convexity">
<h2>Consequences of convexity<a class="headerlink" href="#consequences-of-convexity" title="Link to this heading">#</a></h2>
<p>Why do we care if a function is (strictly/strongly) convex?</p>
<p>Basically, our various notions of convexity have implications about the
nature of minima. It should not be surprising that the stronger
conditions tell us more about the minima.</p>
<div class="proof proposition admonition" id="prop-convex-minima">
<p class="admonition-title"><span>Proposition </span> (Minima of convex functions)</p>
<section class="proposition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> be a convex set.</p>
<p>If <span class="math notranslate nohighlight">\(f\)</span> is convex, then any local minimum of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is also a global minimum.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Suppose <span class="math notranslate nohighlight">\(f\)</span> is convex, and let <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> be a local
minimum of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>.</p>
<p>Then for some neighborhood <span class="math notranslate nohighlight">\(N \subseteq \mathcal{X}\)</span> about <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>, we have
<span class="math notranslate nohighlight">\(f(\mathbf{x}) \geq f(\mathbf{x}^*)\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x} \in N\)</span>.</p>
<p>Suppose
towards a contradiction that there exists
<span class="math notranslate nohighlight">\(\tilde{\mathbf{x}} \in \mathcal{X}\)</span> such that
<span class="math notranslate nohighlight">\(f(\tilde{\mathbf{x}}) &lt; f(\mathbf{x}^*)\)</span>.</p>
<p>Consider the line segment
<span class="math notranslate nohighlight">\(\mathbf{x}(t) = t\mathbf{x}^* + (1-t)\tilde{\mathbf{x}}, ~ t \in [0,1]\)</span>,
noting that <span class="math notranslate nohighlight">\(\mathbf{x}(t) \in \mathcal{X}\)</span> by the convexity of
<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>.</p>
<p>Then by the convexity of <span class="math notranslate nohighlight">\(f\)</span>,</p>
<div class="math notranslate nohighlight">
\[f(\mathbf{x}(t)) \leq tf(\mathbf{x}^*) + (1-t)f(\tilde{\mathbf{x}}) &lt; tf(\mathbf{x}^*) + (1-t)f(\mathbf{x}^*) = f(\mathbf{x}^*)\]</div>
<p>for all <span class="math notranslate nohighlight">\(t \in (0,1)\)</span>.</p>
<p>We can pick <span class="math notranslate nohighlight">\(t\)</span> to be sufficiently close to <span class="math notranslate nohighlight">\(1\)</span> that
<span class="math notranslate nohighlight">\(\mathbf{x}(t) \in N\)</span>;
then <span class="math notranslate nohighlight">\(f(\mathbf{x}(t)) \geq f(\mathbf{x}^*)\)</span> by
the definition of <span class="math notranslate nohighlight">\(N,\)</span> but <span class="math notranslate nohighlight">\(f(\mathbf{x}(t)) &lt; f(\mathbf{x}^*)\)</span> by the
above inequality, a contradiction.</p>
<p>It follows that <span class="math notranslate nohighlight">\(f(\mathbf{x}^*) \leq f(\mathbf{x})\)</span> for all
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}\)</span>, so <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a global minimum of
<span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. ◻</p>
</div>
<div class="proof proposition admonition" id="prop-minima-striclty-convex">
<p class="admonition-title"><span>Proposition </span> (Minima stricly convex functions)</p>
<section class="proposition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> be a convex set.</p>
<p>If <span class="math notranslate nohighlight">\(f\)</span> is strictly convex, then there
exists at most one local minimum of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. Consequently,
if it exists it is the unique global minimum of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The second sentence follows from the first, so all we must show
is that if a local minimum exists in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> then it is unique.</p>
<p>Suppose <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is a local minimum of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, and
suppose towards a contradiction that there exists a local minimum
<span class="math notranslate nohighlight">\(\tilde{\mathbf{x}} \in \mathcal{X}\)</span> such that
<span class="math notranslate nohighlight">\(\tilde{\mathbf{x}} \neq \mathbf{x}^*\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(f\)</span> is strictly convex, it is convex, so <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> and
<span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}\)</span> are both global minima of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> by
the previous result. Hence <span class="math notranslate nohighlight">\(f(\mathbf{x}^*) = f(\tilde{\mathbf{x}})\)</span>.
Consider the line segment
<span class="math notranslate nohighlight">\(\mathbf{x}(t) = t\mathbf{x}^* + (1-t)\tilde{\mathbf{x}}, ~ t \in [0,1]\)</span>,
which again must lie entirely in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. By the strict convexity
of <span class="math notranslate nohighlight">\(f\)</span>,</p>
<div class="math notranslate nohighlight">
\[f(\mathbf{x}(t)) &lt; tf(\mathbf{x}^*) + (1-t)f(\tilde{\mathbf{x}}) = tf(\mathbf{x}^*) + (1-t)f(\mathbf{x}^*) = f(\mathbf{x}^*)\]</div>
<p>for all <span class="math notranslate nohighlight">\(t \in (0,1)\)</span>.</p>
<p>But this contradicts the fact that <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>
is a global minimum. Therefore if <span class="math notranslate nohighlight">\(\tilde{\mathbf{x}}\)</span> is a local
minimum of <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, then
<span class="math notranslate nohighlight">\(\tilde{\mathbf{x}} = \mathbf{x}^*\)</span>, so <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> is the unique
minimum in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. ◻</p>
</div>
<p>It is worthwhile to examine how the feasible set affects the
optimization problem. We will see why the assumption that <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>
is convex is needed in the results above.</p>
</section>
<section id="effects-of-changing-the-feasible-set">
<h2>Effects of Changing the Feasible Set<a class="headerlink" href="#effects-of-changing-the-feasible-set" title="Link to this heading">#</a></h2>
<p>Consider the function <span class="math notranslate nohighlight">\(f(x) = x^2\)</span>, which is a strictly convex function.
The unique global minimum of this function in <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> is <span class="math notranslate nohighlight">\(x = 0\)</span>.</p>
<p>But let’s see what happens when we change the feasible set
<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">cases</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;(i) $\mathcal</span><span class="si">{X}</span><span class="s2"> = \{1\}$&quot;</span><span class="p">,</span> <span class="s2">&quot;feasible&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;orange&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;(ii) $\mathcal</span><span class="si">{X}</span><span class="s2"> = \mathbb</span><span class="si">{R}</span><span class="s2"> \setminus \{0\}$&quot;</span><span class="p">,</span> <span class="s2">&quot;feasible&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]]),</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;orange&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;(iii) $\mathcal</span><span class="si">{X}</span><span class="s2"> = (-\infty,-1] \cup [0,\infty)$&quot;</span><span class="p">,</span> <span class="s2">&quot;feasible&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">]]),</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;orange&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="sa">r</span><span class="s2">&quot;(iv) $\mathcal</span><span class="si">{X}</span><span class="s2"> = (-\infty,-1] \cup [1,\infty)$&quot;</span><span class="p">,</span> <span class="s2">&quot;feasible&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">]]),</span> <span class="s2">&quot;color&quot;</span><span class="p">:</span> <span class="s2">&quot;orange&quot;</span><span class="p">},</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">case</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">,</span> <span class="n">cases</span><span class="p">):</span>
    <span class="c1"># Plot the function</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) = x^2$&#39;</span><span class="p">)</span>
    <span class="c1"># Highlight feasible set</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">case</span><span class="p">[</span><span class="s2">&quot;feasible&quot;</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">pt</span> <span class="ow">in</span> <span class="n">case</span><span class="p">[</span><span class="s2">&quot;feasible&quot;</span><span class="p">]:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">pt</span><span class="p">,</span> <span class="n">pt</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">case</span><span class="p">[</span><span class="s2">&quot;color&quot;</span><span class="p">],</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;feasible set&quot;</span> <span class="k">if</span> <span class="n">pt</span> <span class="o">==</span> <span class="n">case</span><span class="p">[</span><span class="s2">&quot;feasible&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">case</span><span class="p">[</span><span class="s2">&quot;feasible&quot;</span><span class="p">],</span> <span class="n">case</span><span class="p">[</span><span class="s2">&quot;feasible&quot;</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">case</span><span class="p">[</span><span class="s2">&quot;color&quot;</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;feasible set&quot;</span><span class="p">)</span>
    <span class="c1"># Mark minima</span>
    <span class="k">if</span> <span class="n">case</span><span class="p">[</span><span class="s2">&quot;title&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;(i)&quot;</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;minimum&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">case</span><span class="p">[</span><span class="s2">&quot;title&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;(ii)&quot;</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;No minimum&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">case</span><span class="p">[</span><span class="s2">&quot;title&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;(iii)&quot;</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;local min&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;go&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;global min&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">case</span><span class="p">[</span><span class="s2">&quot;title&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;(iv)&quot;</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;global min&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">case</span><span class="p">[</span><span class="s2">&quot;title&quot;</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/e85eaf769f2592fb17dbfeef407bac2cce3b78cabee44e266e575ab4e7132fe9.png" src="../_images/e85eaf769f2592fb17dbfeef407bac2cce3b78cabee44e266e575ab4e7132fe9.png" />
</div>
</div>
<p>(i) <span class="math notranslate nohighlight">\(\mathcal{X} = \{1\}\)</span>: This set is actually convex, so we still have
a unique global minimum. But it is not the same as the unconstrained
minimum!</p>
<p>(ii) <span class="math notranslate nohighlight">\(\mathcal{X} = \mathbb{R} \setminus \{0\}\)</span>: This set is non-convex,
and we can see that <span class="math notranslate nohighlight">\(f\)</span> has no minima in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. For any
point <span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span>, one can find another point
<span class="math notranslate nohighlight">\(y \in \mathcal{X}\)</span> such that <span class="math notranslate nohighlight">\(f(y) &lt; f(x)\)</span>.</p>
<p>(iii) <span class="math notranslate nohighlight">\(\mathcal{X} = (-\infty,-1] \cup [0,\infty)\)</span>: This set is
non-convex, and we can see that there is a local minimum
(<span class="math notranslate nohighlight">\(x = -1\)</span>) which is distinct from the global minimum (<span class="math notranslate nohighlight">\(x = 0\)</span>).</p>
<p>(iv) <span class="math notranslate nohighlight">\(\mathcal{X} = (-\infty,-1] \cup [1,\infty)\)</span>: This set is
non-convex, and we can see that there are two global minima
(<span class="math notranslate nohighlight">\(x = \pm 1\)</span>).</p>
</section>
<section id="showing-that-a-function-is-convex">
<h2>Showing that a function is convex<a class="headerlink" href="#showing-that-a-function-is-convex" title="Link to this heading">#</a></h2>
<p>Hopefully the previous section has convinced the reader that convexity
is an important property.
Next we turn to the issue of showing that a
function is (strictly/strongly) convex.
It is of course possible (in
principle) to directly show that the condition in the definition holds,
but often this is not the most convenient approach.
Instead, we can use a variety of sufficient conditions, properties, and tools—such as the properties of norms, the behavior of the gradient, the use of second derivatives, or by demonstrating that the function is built from convex functions in ways that preserve convexity—to make it much easier to verify convexity in practice.</p>
<div class="proof proposition admonition" id="prop-norms-convex">
<p class="admonition-title"><span>Proposition </span> (Norms)</p>
<section class="proposition-content" id="proof-content">
<p>Norms are convex.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Let <span class="math notranslate nohighlight">\(\|\cdot\|\)</span> be a norm on a vector space <span class="math notranslate nohighlight">\(V\)</span>.</p>
<p>Then for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in V\)</span> and <span class="math notranslate nohighlight">\(t \in [0,1]\)</span>,</p>
<div class="math notranslate nohighlight">
\[\|t\mathbf{x} + (1-t)\mathbf{y}\| \leq \|t\mathbf{x}\| + \|(1-t)\mathbf{y}\| = |t|\|\mathbf{x}\| + |1-t|\|\mathbf{y}\| = t\|\mathbf{x}\| + (1-t)\|\mathbf{y}\|\]</div>
<p>where we have used respectively the triangle inequality, the homogeneity
of norms, and the fact that <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(1-t\)</span> are nonnegative.</p>
<p>Hence <span class="math notranslate nohighlight">\(\|\cdot\|\)</span> is convex. ◻</p>
</div>
<div class="proof theorem admonition" id="prf-first-order-convex">
<p class="admonition-title"><span>Theorem </span> (First Order Condition for Convex Functions (1D case))</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(f : \mathbb{R} \to \mathbb{R}\)</span> be differentiable on an open interval <span class="math notranslate nohighlight">\(I \subseteq \mathbb{R}\)</span>. Then:</p>
<p><span class="math notranslate nohighlight">\(f\)</span> is <strong>convex</strong> on <span class="math notranslate nohighlight">\(I\)</span> if and only if for all <span class="math notranslate nohighlight">\(x, y \in I\)</span>:</p>
<div class="math notranslate nohighlight">
\[
f(y) \geq f(x) + f'(x)(y - x)
\]</div>
<p>This means that the graph of <span class="math notranslate nohighlight">\(f\)</span> lies <strong>above its tangent lines</strong>.</p>
</section>
</div><section id="intuition">
<h3>🧠 Intuition<a class="headerlink" href="#intuition" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The inequality says: <em>the function at any point <span class="math notranslate nohighlight">\( y \)</span> is at least as large as the linear approximation at <span class="math notranslate nohighlight">\( x \)</span></em>.</p></li>
<li><p>This ensures the function is <strong>bending upwards</strong> — a hallmark of convexity.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Define the convex function and its gradient</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Choose a point x0 to draw the tangent</span>
<span class="n">x0</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">y0</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="n">slope</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>

<span class="c1"># Equation of the tangent line at x0</span>
<span class="n">tangent</span> <span class="o">=</span> <span class="n">y0</span> <span class="o">+</span> <span class="n">slope</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) = x^2$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tangent</span><span class="p">,</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Tangent at $x_0=0.5$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">x0</span><span class="p">],</span> <span class="p">[</span><span class="n">y0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="o">+</span><span class="mf">0.5</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$(x_0, f(x_0))$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tangent</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="p">(</span><span class="n">y</span><span class="o">&gt;</span><span class="n">tangent</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) \geq$ tangent&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$f(x)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Convexity: $f(x)$ lies above its tangent at $x_0$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/77f1125ccdc379b5ce2bb5fc31926d8463fba37e00908cf1f69498cf602dcef5.png" src="../_images/77f1125ccdc379b5ce2bb5fc31926d8463fba37e00908cf1f69498cf602dcef5.png" />
</div>
</div>
<div class="proof admonition" id="proof">
<p>Proof. (⇒) If <span class="math notranslate nohighlight">\( f \)</span> is convex, then the inequality holds:</p>
<p>Let <span class="math notranslate nohighlight">\( f \)</span> be convex on <span class="math notranslate nohighlight">\( I \)</span>.</p>
<p>Fix any <span class="math notranslate nohighlight">\( x, y \in I \)</span>, and define the function:</p>
<div class="math notranslate nohighlight">
\[
\phi(t) := f((1 - t)x + t y), \quad t \in [0,1].
\]</div>
<p>Since <span class="math notranslate nohighlight">\( f \)</span> is convex, <span class="math notranslate nohighlight">\( \phi \)</span> is convex as a function of <span class="math notranslate nohighlight">\( t \)</span>, and differentiable.</p>
<p>By the definition of convexity of a differentiable function on an interval, we have:</p>
<div class="math notranslate nohighlight">
\[
\phi(1) \geq \phi(0) + \phi'(0)(1)
\]</div>
<p>Compute:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \phi(0) = f(x) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \phi(1) = f(y) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \phi'(t) = f'((1 - t)x + t y) \cdot (y - x) \Rightarrow \phi'(0) = f'(x)(y - x) \)</span></p></li>
</ul>
<p>Substituting gives:</p>
<div class="math notranslate nohighlight">
\[
f(y) \geq f(x) + f'(x)(y - x)
\]</div>
<hr class="docutils" />
<h4 class="rubric" id="conversely-if-the-inequality-holds-then-f-is-convex">(⇐) Conversely, if the inequality holds, then <span class="math notranslate nohighlight">\( f \)</span> is convex:</h4>
<p>Let <span class="math notranslate nohighlight">\( x &lt; y \)</span> and <span class="math notranslate nohighlight">\( t \in [0,1] \)</span>. Define <span class="math notranslate nohighlight">\( z = (1 - t)x + t y \in [x, y] \)</span>. By the hypothesis:</p>
<ul class="simple">
<li><p>From point <span class="math notranslate nohighlight">\( x \)</span>, we have:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f(z) \geq f(x) + f'(x)(z - x)
\]</div>
<ul class="simple">
<li><p>From point <span class="math notranslate nohighlight">\( y \)</span>, we also have:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f(z) \geq f(y) + f'(y)(z - y)
\]</div>
<p>Now combine this with the <strong>mean value theorem</strong> and use the inequality on both sides to derive:</p>
<div class="math notranslate nohighlight">
\[
f((1 - t)x + t y) \leq (1 - t)f(x) + t f(y)
\]</div>
<p>This proves the convexity of <span class="math notranslate nohighlight">\( f \)</span>.</p>
</div>
<div class="proof proposition admonition" id="prop-convex-functions-graph">
<p class="admonition-title"><span>Proposition </span> (First Order Condition for Convex Functions)</p>
<section class="proposition-content" id="proof-content">
<p>Suppose <span class="math notranslate nohighlight">\(f\)</span> is differentiable.</p>
<p>Then <span class="math notranslate nohighlight">\(f\)</span> is convex if and only if</p>
<div class="math notranslate nohighlight">
\[f(\mathbf{y}) \geq f(\mathbf{x}) + \langle \nabla f(\mathbf{x}), \mathbf{y} - \mathbf{x} \rangle\]</div>
<p>for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \operatorname{dom} f\)</span>.</p>
</section>
</div><p>The proposition says that for a convex, differentiable function
<span class="math notranslate nohighlight">\(f\)</span>, the graph of <span class="math notranslate nohighlight">\(f\)</span> always lies above its tangent at any point. In other words, the tangent line at any point
<span class="math notranslate nohighlight">\(x\)</span> is a global underestimator of the function.</p>
<div class="proof admonition" id="proof">
<p>Proof. (<strong>“Only if” direction</strong>)<br />
Suppose <span class="math notranslate nohighlight">\(f\)</span> is convex and differentiable.</p>
<p>By the definition of convexity, for any <span class="math notranslate nohighlight">\(t \in [0,1]\)</span> and any <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \operatorname{dom} f\)</span>,</p>
<div class="math notranslate nohighlight">
\[
f(t\mathbf{y} + (1-t)\mathbf{x}) \leq t f(\mathbf{y}) + (1-t) f(\mathbf{x}).
\]</div>
<p>Define <span class="math notranslate nohighlight">\(\varphi(t) = f(\mathbf{x} + t(\mathbf{y} - \mathbf{x}))\)</span> for <span class="math notranslate nohighlight">\(t \in [0,1]\)</span>.</p>
<p>Then <span class="math notranslate nohighlight">\(\varphi\)</span> is convex as a function of <span class="math notranslate nohighlight">\(t\)</span>, and differentiable.</p>
<p>The convexity of <span class="math notranslate nohighlight">\(\varphi\)</span> implies</p>
<div class="math notranslate nohighlight">
\[
\varphi(1) \geq \varphi(0) + \varphi'(0)(1-0) = f(\mathbf{x}) + \langle \nabla f(\mathbf{x}), \mathbf{y} - \mathbf{x} \rangle.
\]</div>
<p>But <span class="math notranslate nohighlight">\(\varphi(1) = f(\mathbf{y})\)</span>, so</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{y}) \geq f(\mathbf{x}) + \langle \nabla f(\mathbf{x}), \mathbf{y} - \mathbf{x} \rangle.
\]</div>
<p>(<strong>“If” direction</strong>)<br />
Suppose the inequality</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{y}) \geq f(\mathbf{x}) + \langle \nabla f(\mathbf{x}), \mathbf{y} - \mathbf{x} \rangle
\]</div>
<p>holds for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \operatorname{dom} f\)</span>.</p>
<p>We want to show that <span class="math notranslate nohighlight">\(f\)</span> is convex, i.e.,</p>
<div class="math notranslate nohighlight">
\[
f(t\mathbf{y} + (1-t)\mathbf{x}) \leq t f(\mathbf{y}) + (1-t) f(\mathbf{x})
\]</div>
<p>for all <span class="math notranslate nohighlight">\(t \in [0,1]\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(t \in (0,1)\)</span> and define <span class="math notranslate nohighlight">\(\mathbf{z} = t\mathbf{y} + (1-t)\mathbf{x}\)</span>.</p>
<p>By the assumption, applied at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f(\mathbf{y}) &amp;\geq f(\mathbf{z}) + \langle \nabla f(\mathbf{z}), \mathbf{y} - \mathbf{z} \rangle \\
f(\mathbf{x}) &amp;\geq f(\mathbf{z}) + \langle \nabla f(\mathbf{z}), \mathbf{x} - \mathbf{z} \rangle
\end{aligned}
\end{split}\]</div>
<p>Multiply the first inequality by <span class="math notranslate nohighlight">\(t\)</span> and the second by <span class="math notranslate nohighlight">\(1-t\)</span>, then add:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
t f(\mathbf{y}) + (1-t) f(\mathbf{x}) &amp;\geq t f(\mathbf{z}) + t \langle \nabla f(\mathbf{z}), \mathbf{y} - \mathbf{z} \rangle \\
&amp;\quad + (1-t) f(\mathbf{z}) + (1-t) \langle \nabla f(\mathbf{z}), \mathbf{x} - \mathbf{z} \rangle \\
&amp;= f(\mathbf{z}) + \langle \nabla f(\mathbf{z}), t(\mathbf{y} - \mathbf{z}) + (1-t)(\mathbf{x} - \mathbf{z}) \rangle
\end{aligned}
\end{split}\]</div>
<p>But</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
t(\mathbf{y} - \mathbf{z}) + (1-t)(\mathbf{x} - \mathbf{z}) &amp;= t(\mathbf{y} - (t\mathbf{y} + (1-t)\mathbf{x})) + (1-t)(\mathbf{x} - (t\mathbf{y} + (1-t)\mathbf{x})) \\
&amp;= t((1-t)(\mathbf{y} - \mathbf{x})) + (1-t)(-t(\mathbf{y} - \mathbf{x})) \\
&amp;= t(1-t)(\mathbf{y} - \mathbf{x}) - t(1-t)(\mathbf{y} - \mathbf{x}) \\
&amp;= 0
\end{aligned}
\end{split}\]</div>
<p>So the inner product term vanishes, and we have</p>
<div class="math notranslate nohighlight">
\[
t f(\mathbf{y}) + (1-t) f(\mathbf{x}) \geq f(\mathbf{z}) = f(t\mathbf{y} + (1-t)\mathbf{x})
\]</div>
<p>which is the definition of convexity.</p>
<p>◻</p>
</div>
<div class="proof proposition admonition" id="prop-Hessian-convex">
<p class="admonition-title"><span>Proposition </span> (Hessian of Convex Functions)</p>
<section class="proposition-content" id="proof-content">
<p>Suppose <span class="math notranslate nohighlight">\(f\)</span> is twice differentiable.</p>
<p>Then</p>
<p>(i) <span class="math notranslate nohighlight">\(f\)</span> is convex if and only if <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}) \succeq 0\)</span> for
all <span class="math notranslate nohighlight">\(\mathbf{x} \in \operatorname{dom} f\)</span>.</p>
<p>(ii) If <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}) \succ 0\)</span> for all
<span class="math notranslate nohighlight">\(\mathbf{x} \in \operatorname{dom} f\)</span>, then <span class="math notranslate nohighlight">\(f\)</span> is strictly convex.</p>
<p>(iii) <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(m\)</span>-strongly convex if and only if
<span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}) \succeq mI\)</span> for all
<span class="math notranslate nohighlight">\(\mathbf{x} \in \operatorname{dom} f\)</span>.</p>
</section>
</div><p>This proposition provides a simple way to check convexity, strict convexity, and strong convexity for twice differentiable functions by looking at the Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x})\)</span>:</p>
<ul class="simple">
<li><p>If the Hessian is positive semidefinite everywhere (all eigenvalues are nonnegative), the function is convex.</p></li>
<li><p>If the Hessian is positive definite everywhere (all eigenvalues are strictly positive), the function is strictly convex.</p></li>
<li><p>If the Hessian is bounded below by <span class="math notranslate nohighlight">\(mI\)</span> (all eigenvalues are at least <span class="math notranslate nohighlight">\(m &gt; 0\)</span>), the function is <span class="math notranslate nohighlight">\(m\)</span>-strongly convex.</p></li>
</ul>
<p>These conditions are very useful in practice, because checking the Hessian is often easier than checking the definition of convexity directly.</p>
<div class="proof admonition" id="proof">
<p>Proof. We prove each part in turn.</p>
<p><strong>(i) <span class="math notranslate nohighlight">\(f\)</span> is convex if and only if <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}) \succeq 0\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</strong></p>
<p>Recall that for a twice differentiable function <span class="math notranslate nohighlight">\(f\)</span>, the second-order Taylor expansion at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{y}) = f(\mathbf{x}) + \langle \nabla f(\mathbf{x}), \mathbf{y} - \mathbf{x} \rangle + \frac{1}{2} (\mathbf{y} - \mathbf{x})^\top \nabla^2 f(\mathbf{z}) (\mathbf{y} - \mathbf{x})
\]</div>
<p>for some <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> on the line segment between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>.</p>
<p><strong>(<span class="math notranslate nohighlight">\(\implies\)</span>)</strong> Suppose <span class="math notranslate nohighlight">\(f\)</span> is convex.</p>
<p>Then, by the first-order condition for convexity, for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{y}) \geq f(\mathbf{x}) + \langle \nabla f(\mathbf{x}), \mathbf{y} - \mathbf{x} \rangle.
\]</div>
<p>Subtracting the right-hand side from the Taylor expansion, we get</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{y}) - f(\mathbf{x}) - \langle \nabla f(\mathbf{x}), \mathbf{y} - \mathbf{x} \rangle = \frac{1}{2} (\mathbf{y} - \mathbf{x})^\top \nabla^2 f(\mathbf{z}) (\mathbf{y} - \mathbf{x}) \geq 0
\]</div>
<p>for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y}\)</span> and some <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> between them. This is only possible if <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{z})\)</span> is positive semidefinite for all <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, and thus for all <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p><strong>(<span class="math notranslate nohighlight">\(\impliedby\)</span>)</strong> Conversely, suppose <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}) \succeq 0\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>We want to prove that this implies <span class="math notranslate nohighlight">\(f\)</span> is convex.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \operatorname{dom} f\)</span>, and define the function <span class="math notranslate nohighlight">\(\phi(t) = f(\mathbf{x} + t(\mathbf{y} - \mathbf{x}))\)</span> for <span class="math notranslate nohighlight">\(t \in [0,1]\)</span>.</p>
<p>This is the restriction of <span class="math notranslate nohighlight">\(f\)</span> to the line segment between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>, i.e., a function from <span class="math notranslate nohighlight">\(\mathbb{R} \to \mathbb{R}\)</span>.</p>
<p>Then <span class="math notranslate nohighlight">\(\phi\)</span> is twice differentiable with:</p>
<div class="math notranslate nohighlight">
\[
\phi'(t) = \nabla f(\mathbf{x}_t)^\top (\mathbf{y} - \mathbf{x}), \quad \text{and} \quad \phi''(t) = (\mathbf{y} - \mathbf{x})^\top \nabla^2 f(\mathbf{x}_t) (\mathbf{y} - \mathbf{x}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}_t = \mathbf{x} + t(\mathbf{y} - \mathbf{x})\)</span>. By assumption, <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}_t) \succeq 0\)</span>, so:</p>
<div class="math notranslate nohighlight">
\[
\phi''(t) \geq 0 \quad \text{for all } t \in [0,1].
\]</div>
<p>Hence, <span class="math notranslate nohighlight">\(\phi\)</span> is a convex function on <span class="math notranslate nohighlight">\([0,1]\)</span>, and so:</p>
<div class="math notranslate nohighlight">
\[
f(t\mathbf{y} + (1 - t)\mathbf{x}) = \phi(t) \leq (1 - t)\phi(0) + t\phi(1) = (1 - t)f(\mathbf{x}) + t f(\mathbf{y}),
\]</div>
<p>which shows that <span class="math notranslate nohighlight">\(f\)</span> is convex.</p>
<hr class="docutils" />
<p><strong>(ii) If <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}) \succ 0\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, then <span class="math notranslate nohighlight">\(f\)</span> is strictly convex.</strong></p>
<p>If <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x})\)</span> is positive definite for all <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, then for any <span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{y}\)</span>, the quadratic form <span class="math notranslate nohighlight">\((\mathbf{y} - \mathbf{x})^\top \nabla^2 f(\mathbf{z}) (\mathbf{y} - \mathbf{x}) &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>.</p>
<p>Thus, the Taylor expansion above is strictly greater than zero for <span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{y}\)</span>, so the convexity inequality is strict for <span class="math notranslate nohighlight">\(t \in (0,1)\)</span>, i.e., <span class="math notranslate nohighlight">\(f\)</span> is strictly convex.</p>
<hr class="docutils" />
<p><strong>(iii) <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(m\)</span>-strongly convex if and only if <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}) \succeq mI\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</strong></p>
<p>Recall that <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(m\)</span>-strongly convex if <span class="math notranslate nohighlight">\(f(\mathbf{x}) - \frac{m}{2}\|\mathbf{x}\|^2\)</span> is convex.</p>
<p>The Hessian of this function is <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}) - mI\)</span>. By part (i), this is convex if and only if <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}) - mI \succeq 0\)</span>, i.e., <span class="math notranslate nohighlight">\(\nabla^2 f(\mathbf{x}) \succeq mI\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>◻</p>
</div>
<p>The following propositions show how convexity is preserved under scaling and addition of functions.</p>
<div class="proof proposition admonition" id="prop-scaling-convex-functions">
<p class="admonition-title"><span>Proposition </span> (Scaling Convex Functions)</p>
<section class="proposition-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(f\)</span> is convex and <span class="math notranslate nohighlight">\(\alpha \geq 0\)</span>, then <span class="math notranslate nohighlight">\(\alpha f\)</span> is convex.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Suppose <span class="math notranslate nohighlight">\(f\)</span> is convex and <span class="math notranslate nohighlight">\(\alpha \geq 0\)</span>.</p>
<p>Then for all
<span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \operatorname{dom}(\alpha f) = \operatorname{dom} f\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
(\alpha f)(t\mathbf{x} + (1-t)\mathbf{y}) &amp;= \alpha f(t\mathbf{x} + (1-t)\mathbf{y}) \\
&amp;\leq \alpha\left(tf(\mathbf{x}) + (1-t)f(\mathbf{y})\right) \\
&amp;= t(\alpha f(\mathbf{x})) + (1-t)(\alpha f(\mathbf{y})) \\
&amp;= t(\alpha f)(\mathbf{x}) + (1-t)(\alpha f)(\mathbf{y})
\end{aligned}\end{split}\]</div>
<p>so <span class="math notranslate nohighlight">\(\alpha f\)</span> is convex. ◻</p>
</div>
<div class="proof proposition admonition" id="prop-sum-convex-functions">
<p class="admonition-title"><span>Proposition </span> (Sum of Convex Functions)</p>
<section class="proposition-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are convex, then <span class="math notranslate nohighlight">\(f+g\)</span> is convex.</p>
<p>Furthermore, if <span class="math notranslate nohighlight">\(g\)</span> is
strictly convex, then <span class="math notranslate nohighlight">\(f+g\)</span> is strictly convex.</p>
<p>If <span class="math notranslate nohighlight">\(g\)</span> is <span class="math notranslate nohighlight">\(m\)</span>-strongly convex, then <span class="math notranslate nohighlight">\(f+g\)</span> is <span class="math notranslate nohighlight">\(m\)</span>-strongly convex.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Suppose <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are convex.</p>
<p>Then for all
<span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \operatorname{dom} (f+g) = \operatorname{dom} f \cap \operatorname{dom} g\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
(f+g)(t\mathbf{x} + (1-t)\mathbf{y}) &amp;= f(t\mathbf{x} + (1-t)\mathbf{y}) + g(t\mathbf{x} + (1-t)\mathbf{y}) \\
&amp;\leq tf(\mathbf{x}) + (1-t)f(\mathbf{y}) + g(t\mathbf{x} + (1-t)\mathbf{y}) &amp; \text{convexity of $f$} \\
&amp;\leq tf(\mathbf{x}) + (1-t)f(\mathbf{y}) + tg(\mathbf{x}) + (1-t)g(\mathbf{y}) &amp; \text{convexity of $g$} \\
&amp;= t(f(\mathbf{x}) + g(\mathbf{x})) + (1-t)(f(\mathbf{y}) + g(\mathbf{y})) \\
&amp;= t(f+g)(\mathbf{x}) + (1-t)(f+g)(\mathbf{y})
\end{aligned}\end{split}\]</div>
<p>so <span class="math notranslate nohighlight">\(f + g\)</span> is convex.</p>
<p>If <span class="math notranslate nohighlight">\(g\)</span> is strictly convex, the second inequality above holds strictly
for <span class="math notranslate nohighlight">\(\mathbf{x} \neq \mathbf{y}\)</span> and <span class="math notranslate nohighlight">\(t \in (0,1)\)</span>, so <span class="math notranslate nohighlight">\(f+g\)</span> is strictly
convex.</p>
<p>If <span class="math notranslate nohighlight">\(g\)</span> is <span class="math notranslate nohighlight">\(m\)</span>-strongly convex, then the function
<span class="math notranslate nohighlight">\(h(\mathbf{x}) \equiv g(\mathbf{x}) - \frac{m}{2}\|\mathbf{x}\|_2^2\)</span> is
convex, so <span class="math notranslate nohighlight">\(f+h\)</span> is convex.</p>
<p>But</p>
<div class="math notranslate nohighlight">
\[(f+h)(\mathbf{x}) \equiv f(\mathbf{x}) + h(\mathbf{x}) \equiv f(\mathbf{x}) + g(\mathbf{x}) - \frac{m}{2}\|\mathbf{x}\|_2^2 \equiv (f+g)(\mathbf{x}) - \frac{m}{2}\|\mathbf{x}\|_2^2\]</div>
<p>so <span class="math notranslate nohighlight">\(f+g\)</span> is <span class="math notranslate nohighlight">\(m\)</span>-strongly convex. ◻</p>
</div>
<div class="proof proposition admonition" id="prop-convex-functions-weighted-sum">
<p class="admonition-title"><span>Proposition </span> (Weighted Sum of Convex Functions)</p>
<section class="proposition-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(f_1, \dots, f_n\)</span> are convex and <span class="math notranslate nohighlight">\(\alpha_1, \dots, \alpha_n \geq 0\)</span>,
then</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n \alpha_i f_i\]</div>
<p>is convex.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Follows from the previous two propositions by induction. ◻</p>
</div>
<p>A common way to generate new convex functions is by composing a convex function with an affine transformation, as stated in the following proposition:</p>
<div class="proof proposition admonition" id="prop-linear-convex">
<p class="admonition-title"><span>Proposition </span> (Combination of Affine and Convex Functions)</p>
<section class="proposition-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(f\)</span> is convex, then
<span class="math notranslate nohighlight">\(g(\mathbf{x}) \equiv f(\mathbf{A}\mathbf{x} + \mathbf{b})\)</span> is convex
for any appropriately-sized <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>.</p>
</section>
</div><p>The following plot demonstrates how composing a convex function with an affine transformation preserves convexity: the left plot shows the original convex function, and the right plot shows its affine transformation.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Convex function in 2D</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">z1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">z2</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Affine transformation parameters</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Grid for z-space (for f)</span>
<span class="n">z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="p">)</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span><span class="p">)</span>

<span class="c1"># Grid for x-space (for g)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
<span class="c1"># Apply affine transformation</span>
<span class="n">Z_affine</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij,jkl-&gt;ikl&#39;</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">]))</span> <span class="o">+</span> <span class="n">b</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">Z_affine</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Z_affine</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Plot f(z1, z2)</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">Z1</span><span class="p">,</span> <span class="n">Z2</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Original $f(\mathbf</span><span class="si">{z}</span><span class="s1">) = z_1^2 + 2z_2^2$&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$z_1$&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$z_2$&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$f(\mathbf</span><span class="si">{z}</span><span class="s1">)$&#39;</span><span class="p">)</span>

<span class="c1"># Plot g(x1, x2)</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;plasma&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Affine Transform $g(\mathbf</span><span class="si">{x}</span><span class="s1">) = f(A\mathbf</span><span class="si">{x}</span><span class="s1"> + \mathbf</span><span class="si">{b}</span><span class="s1">)$&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$g(\mathbf</span><span class="si">{x}</span><span class="s1">)$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/7f5ee29966a1ce90a5a74196d7eca24d5ab06b165f6800a71640363f78cdb068.png" src="../_images/7f5ee29966a1ce90a5a74196d7eca24d5ab06b165f6800a71640363f78cdb068.png" />
</div>
</div>
<div class="proof admonition" id="proof">
<p>Proof. Suppose <span class="math notranslate nohighlight">\(f\)</span> is convex and <span class="math notranslate nohighlight">\(g\)</span> is defined like so.</p>
<p>Then for all
<span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \operatorname{dom} g\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
g(t\mathbf{x} + (1-t)\mathbf{y}) &amp;= f(\mathbf{A}(t\mathbf{x} + (1-t)\mathbf{y}) + \mathbf{b}) \\
&amp;= f(t\mathbf{A}\mathbf{x} + (1-t)\mathbf{A}\mathbf{y} + \mathbf{b}) \\
&amp;= f(t\mathbf{A}\mathbf{x} + (1-t)\mathbf{A}\mathbf{y} + t\mathbf{b} + (1-t)\mathbf{b}) \\
&amp;= f(t(\mathbf{A}\mathbf{x} + \mathbf{b}) + (1-t)(\mathbf{A}\mathbf{y} + \mathbf{b})) \\
&amp;\leq tf(\mathbf{A}\mathbf{x} + \mathbf{b}) + (1-t)f(\mathbf{A}\mathbf{y} + \mathbf{b}) &amp; \text{convexity of $f$} \\
&amp;= tg(\mathbf{x}) + (1-t)g(\mathbf{y})
\end{aligned}\end{split}\]</div>
<p>Thus <span class="math notranslate nohighlight">\(g\)</span> is convex. ◻</p>
</div>
<div class="proof proposition admonition" id="prop-max-convex-functions">
<p class="admonition-title"><span>Proposition </span> (Maximum of Convex Functions)</p>
<section class="proposition-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are convex, then
<span class="math notranslate nohighlight">\(h(\mathbf{x}) \equiv \max\{f(\mathbf{x}), g(\mathbf{x})\}\)</span> is convex.</p>
</section>
</div><p>Let’s look at two convex functions and their pointwise maximum.
The resulting function will be convex, but may not be smooth where the two functions cross.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Define two convex functions</span>
<span class="k">def</span><span class="w"> </span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>

<span class="k">def</span><span class="w"> </span><span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x)$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$g(x)$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$h(x) = \max\{f(x), g(x)\}$&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Maximum of Two Convex Functions is Convex&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$x$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/74c35c292df4be8b1d822ca65e515482999b1926d6f61071fb3b11672a33c1d8.png" src="../_images/74c35c292df4be8b1d822ca65e515482999b1926d6f61071fb3b11672a33c1d8.png" />
</div>
</div>
<ul class="simple">
<li><p>The blue and orange curves are two convex functions.</p></li>
<li><p>The green curve is their pointwise maximum, which is also convex (but not necessarily smooth everywhere).</p></li>
<li><p>The shaded region highlights the “upper envelope” formed by the maximum.</p></li>
</ul>
<div class="proof admonition" id="proof">
<p>Proof. Suppose <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are convex and <span class="math notranslate nohighlight">\(h\)</span> is defined like so.
Then for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \operatorname{dom} h\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
h(t\mathbf{x} + (1-t)\mathbf{y}) &amp;= \max\{f(t\mathbf{x} + (1-t)\mathbf{y}), g(t\mathbf{x} + (1-t)\mathbf{y})\} \\
&amp;\leq \max\{tf(\mathbf{x}) + (1-t)f(\mathbf{y}), tg(\mathbf{x}) + (1-t)g(\mathbf{y})\} \\
&amp;\leq \max\{tf(\mathbf{x}), tg(\mathbf{x})\} + \max\{(1-t)f(\mathbf{y}), (1-t)g(\mathbf{y})\} \\
&amp;= t\max\{f(\mathbf{x}), g(\mathbf{x})\} + (1-t)\max\{f(\mathbf{y}), g(\mathbf{y})\} \\
&amp;= th(\mathbf{x}) + (1-t)h(\mathbf{y})
\end{aligned}\end{split}\]</div>
<p>Note that in the first inequality we have used convexity
of <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> plus the fact that</p>
<div class="math notranslate nohighlight">
\[a \leq c, \; b \leq d \quad \implies
 \quad \max\{a,b\} \leq \max\{c,d\}.\]</div>
<p>In the second inequality we have used
the fact that <span class="math notranslate nohighlight">\(\max\{a+b, c+d\} \leq \max\{a,c\} + \max\{b,d\}\)</span>.</p>
<p>Thus <span class="math notranslate nohighlight">\(h\)</span> is convex. ◻</p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="convexity-and-gradient-descent-convergence">
<h2>Convexity and Gradient Descent Convergence<a class="headerlink" href="#convexity-and-gradient-descent-convergence" title="Link to this heading">#</a></h2>
<p>Gradient descent is one of the most widely used optimization methods in machine learning. Its behavior is closely tied to the <strong>convexity</strong> of the objective function.</p>
<section id="gradient-descent-algorithm">
<h3>Gradient Descent Algorithm<a class="headerlink" href="#gradient-descent-algorithm" title="Link to this heading">#</a></h3>
<p>Given a differentiable function <span class="math notranslate nohighlight">\(f: \mathbb{R}^d \to \mathbb{R}\)</span>, the <strong>gradient descent</strong> update rule is:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} - \eta \nabla f(\mathbf{x}^{(k)})
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\eta &gt; 0\)</span> is the learning rate (step size),</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla f(\mathbf{x})\)</span> is the gradient at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="why-convexity-matters">
<h3>Why Convexity Matters<a class="headerlink" href="#why-convexity-matters" title="Link to this heading">#</a></h3>
<section id="convex-functions">
<h4>1. <strong>Convex functions</strong>:<a class="headerlink" href="#convex-functions" title="Link to this heading">#</a></h4>
<p>If <span class="math notranslate nohighlight">\(f\)</span> is <strong>convex</strong> and differentiable, then any <strong>local minimum is a global minimum</strong>.
Gradient descent will eventually reach a minimizer — but convergence may be slow and can depend on the conditioning of the problem.</p>
</section>
<section id="strongly-convex-functions">
<h4>2. <strong>Strongly convex functions</strong>:<a class="headerlink" href="#strongly-convex-functions" title="Link to this heading">#</a></h4>
<p>If <span class="math notranslate nohighlight">\(f\)</span> is <strong><span class="math notranslate nohighlight">\(\mu\)</span>-strongly convex</strong> and has <strong>L-Lipschitz continuous gradients</strong>, then gradient descent <strong>converges linearly</strong> to the global minimizer <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>, meaning:</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}^{(k)}) - f(\mathbf{x}^*) \leq \left(1 - \eta \mu\right)^k \left(f(\mathbf{x}^{(0)}) - f(\mathbf{x}^*)\right)
\]</div>
<p>for <span class="math notranslate nohighlight">\(\eta \in \left(0, \frac{2}{L} \right)\)</span>.</p>
<p>→ The stronger the curvature, the faster the convergence.</p>
<hr class="docutils" />
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># A strongly convex function</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">grad_f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Gradient descent parameters</span>
<span class="n">x_vals</span> <span class="o">=</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">]</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">x_vals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">x_vals</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

<span class="c1"># Plot</span>
<span class="n">x_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x_plot</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) = (x - 1)^2 + 1$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)),</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Gradient Descent Steps&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Gradient Descent on a Strongly Convex Function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/04e41a3771982c891de4f46aeede783d31e9932a52aa23ee72304f78fa83c5fa.png" src="../_images/04e41a3771982c891de4f46aeede783d31e9932a52aa23ee72304f78fa83c5fa.png" />
</div>
</div>
<p>However, non-convex functions may have multiple local minima.
Thus, gradient descent may get stuck in a local minimum.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Updated non-convex function with a linear term</span>
<span class="n">f_nc</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
<span class="n">grad_f_nc</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">6</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.5</span>

<span class="c1"># Gradient descent with the new function</span>
<span class="n">x_vals_nc</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">eta_nc</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">x_vals_nc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_vals_nc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">eta_nc</span> <span class="o">*</span> <span class="n">grad_f_nc</span><span class="p">(</span><span class="n">x_vals_nc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

<span class="c1"># Plot the function and descent steps</span>
<span class="n">x_plot_nc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot_nc</span><span class="p">,</span> <span class="n">f_nc</span><span class="p">(</span><span class="n">x_plot_nc</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$f(x) = x^4 - 3x^2 + 0.5x + 2$ (Non-Convex)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals_nc</span><span class="p">,</span> <span class="n">f_nc</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_vals_nc</span><span class="p">)),</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Gradient Descent Steps&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Gradient Descent on a Non-Convex Function (with Linear Term)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span><span class="mi">8</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/e03392b4dcbb7d43863f7f0465170a2d7a0d231004dfa15557874fdd45aabe89.png" src="../_images/e03392b4dcbb7d43863f7f0465170a2d7a0d231004dfa15557874fdd45aabe89.png" />
</div>
</div>
<div class="math notranslate nohighlight">
\[
f(x) = x^4 - 3x^2 + 0.5x + 2
\]</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-objectives-and-convexity">
<h2>Machine Learning Objectives and Convexity<a class="headerlink" href="#machine-learning-objectives-and-convexity" title="Link to this heading">#</a></h2>
</section>
<section id="linear-regression-with-squared-loss">
<h2>✅ 1. <strong>Linear Regression with Squared Loss</strong><a class="headerlink" href="#linear-regression-with-squared-loss" title="Link to this heading">#</a></h2>
<p><strong>Problem:</strong>
Fit a linear model <span class="math notranslate nohighlight">\(y = X\beta + \varepsilon\)</span> using ordinary least squares (OLS).</p>
<p><strong>Loss function:</strong></p>
<div class="math notranslate nohighlight">
\[
L(\beta) = \frac{1}{2} \|X\beta - y\|^2
\]</div>
<ul class="simple">
<li><p>This loss function is <strong>strongly convex</strong> when <span class="math notranslate nohighlight">\(X^\top X\)</span> is full rank.</p></li>
<li><p>Guarantees a <strong>unique global minimum</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="ridge-regression">
<h2>✅ 2. <strong>Ridge Regression</strong><a class="headerlink" href="#ridge-regression" title="Link to this heading">#</a></h2>
<p><strong>Loss function:</strong></p>
<div class="math notranslate nohighlight">
\[
L(\beta) = \frac{1}{2} \|X\beta - y\|^2 + \lambda \|\beta\|^2
\]</div>
<ul class="simple">
<li><p>The regularization term <span class="math notranslate nohighlight">\(\lambda \|\beta\|^2\)</span> makes the loss <strong>strongly convex</strong> even if <span class="math notranslate nohighlight">\(X^\top X\)</span> is not full rank.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="logistic-regression-binary-classification">
<h2>✅ 3. <strong>Logistic Regression (Binary Classification)</strong><a class="headerlink" href="#logistic-regression-binary-classification" title="Link to this heading">#</a></h2>
<p><strong>Loss function:</strong></p>
<div class="math notranslate nohighlight">
\[
L(\beta) = \sum_{i=1}^n \log\left(1 + \exp(-y_i X_i^\top \beta)\right)
\]</div>
<ul class="simple">
<li><p>Convex but <strong>not strongly convex</strong> unless regularized.</p></li>
</ul>
</section>
<section id="two-layer-neural-network">
<h2>❌ 4. <strong>Two-Layer Neural Network</strong><a class="headerlink" href="#two-layer-neural-network" title="Link to this heading">#</a></h2>
<p><strong>Loss function:</strong></p>
<div class="math notranslate nohighlight">
\[
L(W_1, W_2, b_1, b_2) = \frac{1}{2n} \sum_{i=1}^n \left( W_2 \cdot \tanh(W_1 x_i + b_1) + b_2 - y_i \right)^2
\]</div>
<ul class="simple">
<li><p><strong>Not convex</strong> in the weights due to nonlinear activation.</p></li>
<li><p>Shows multiple minima and complex curvature.</p></li>
<li><p>Requires <strong>careful initialization</strong>, <strong>learning rate tuning</strong>, and often benefits from <strong>regularization</strong>.</p></li>
</ul>
<p>Here is a visualization of the <strong>non-convex loss surface</strong> of a simple 2-layer neural network with architecture:</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = W_2 \cdot \tanh(W_1 x + b_1) + b_2
\]</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span>

<span class="c1"># Simulate simple 1D regression data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Define a 2-layer neural network model manually</span>
<span class="k">def</span><span class="w"> </span><span class="nf">two_layer_nn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">W1</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">W2</span> <span class="o">*</span> <span class="n">h</span> <span class="o">+</span> <span class="n">b2</span>

<span class="c1"># Loss function for a grid of parameters (varying W1 and W2, fixing biases)</span>
<span class="n">w1_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">w2_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">loss_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">w1_range</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">w2_range</span><span class="p">)))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">w1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">w1_range</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">w2</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">w2_range</span><span class="p">):</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">two_layer_nn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
        <span class="n">loss_grid</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">preds</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Plot the non-convex loss landscape</span>
<span class="n">W1</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">w1_range</span><span class="p">,</span> <span class="n">w2_range</span><span class="p">)</span>

<span class="c1"># Initialize gradient descent at a random point</span>
<span class="n">w1_gd</span><span class="p">,</span> <span class="n">w2_gd</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.9</span><span class="p">]</span>
<span class="n">eta</span> <span class="o">=</span> <span class="mf">0.05</span>  <span class="c1"># learning rate</span>

<span class="c1"># Approximate gradient via finite differences</span>
<span class="k">def</span><span class="w"> </span><span class="nf">compute_loss</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">two_layer_nn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">preds</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">=</span> <span class="n">w1_gd</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">w2_gd</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-4</span>
    <span class="n">grad_w1</span> <span class="o">=</span> <span class="p">(</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">w1</span> <span class="o">+</span> <span class="n">eps</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span> <span class="o">-</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">w1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">,</span> <span class="n">w2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">grad_w2</span> <span class="o">=</span> <span class="p">(</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">-</span> <span class="n">eps</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">w1_gd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w1</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">grad_w1</span><span class="p">)</span>
    <span class="n">w2_gd</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w2</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">grad_w2</span><span class="p">)</span>

<span class="c1"># Compute loss values along the path</span>
<span class="n">loss_gd</span> <span class="o">=</span> <span class="p">[</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span> <span class="k">for</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">w1_gd</span><span class="p">,</span> <span class="n">w2_gd</span><span class="p">)]</span>

<span class="c1"># Overlay gradient descent path on 3D surface</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">loss_grid</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">w1_gd</span><span class="p">,</span> <span class="n">w2_gd</span><span class="p">,</span> <span class="n">loss_gd</span><span class="p">,</span> <span class="s1">&#39;ro-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Gradient Descent Path&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Gradient Descent Path on 3D Loss Surface&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Weight W1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Weight W2&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="c1"># ax.set_zlim([0,12])</span>
<span class="c1"># ax.set_xlim([-2,4.6])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/5afaf86ad741d2b5c1a468b19f5cff72f0463ebd08f3a5219b64d1c76a4e93f2.png" src="../_images/5afaf86ad741d2b5c1a468b19f5cff72f0463ebd08f3a5219b64d1c76a4e93f2.png" />
</div>
</div>
<p>We fixed biases <span class="math notranslate nohighlight">\(b_1 = b_2 = 0\)</span> and varied the weights <span class="math notranslate nohighlight">\(W_1\)</span> and <span class="math notranslate nohighlight">\(W_2\)</span>.</p>
<p>The <strong>loss landscape</strong> shows:</p>
<ul class="simple">
<li><p>Multiple <strong>local minima</strong> and <strong>saddle points</strong>.</p></li>
<li><p><strong>Wiggly, non-convex behavior</strong> characteristic of neural networks, even with a single hidden unit.</p></li>
</ul>
<p>This 3D plot shows the <strong>gradient descent trajectory</strong> over the <strong>non-convex loss surface</strong> of the 2-layer neural network:</p>
<ul class="simple">
<li><p>The red path illustrates how optimization progresses from the initial point <span class="math notranslate nohighlight">\((W_1, W_2) = (-1.0, 2.9)\)</span>.</p></li>
<li><p>The descent gets <strong>pulled into a local valley</strong>, which may not be the global minimum.</p></li>
<li><p>This visual underscores the <strong>complexity of optimizing neural networks</strong>, especially compared to convex problems.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_convexity"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="convex_sets.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Convex sets</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter_second_order/overview_second_order.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Second-Order Calculus and Optimization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#consequences-of-convexity">Consequences of convexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#effects-of-changing-the-feasible-set">Effects of Changing the Feasible Set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#showing-that-a-function-is-convex">Showing that a function is convex</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">🧠 Intuition</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity-and-gradient-descent-convergence">Convexity and Gradient Descent Convergence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-algorithm">Gradient Descent Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-convexity-matters">Why Convexity Matters</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-functions">1. <strong>Convex functions</strong>:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#strongly-convex-functions">2. <strong>Strongly convex functions</strong>:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-objectives-and-convexity">Machine Learning Objectives and Convexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-with-squared-loss">✅ 1. <strong>Linear Regression with Squared Loss</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">✅ 2. <strong>Ridge Regression</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-binary-classification">✅ 3. <strong>Logistic Regression (Binary Classification)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-layer-neural-network">❌ 4. <strong>Two-Layer Neural Network</strong></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christoph Lippert
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <script src="https://giscus.app/client.js"
        data-repo="HealthML/Math4ML"
        data-repo-id="R_kgDON-O79w"
        data-category="Comments"
        data-category-id="DIC_kwDON-O7984Co2qc"
        data-mapping="pathname"
        data-strict="1"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>