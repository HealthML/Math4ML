{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 - BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sklearn.datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def sigmoid(a):\n",
    "    \"\"\"\n",
    "    returns the logistic sigmoid \\pi(a)\n",
    "    Keyword arguments:\n",
    "    a -- scalar or numpy array\n",
    "    \"\"\"\n",
    "    expa = np.exp(a)\n",
    "    res = expa / (1.0 + expa)\n",
    "    if hasattr(a, \"__iter__\"):\n",
    "        res[a>709.7] = 1.0 # np.exp will overflow and return inf for values larger 709.7.\n",
    "    elif a>709.7:\n",
    "        res = 1.0\n",
    "    return res\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, l2=0.01, num_iter = 100, method='gd', lr=0.001, tol=0.001) -> None:\n",
    "        self.w = None\n",
    "        self.num_iter = num_iter\n",
    "        self.method = method\n",
    "        self.lr = lr\n",
    "        self.tol = tol\n",
    "        self.l2 = l2\n",
    "        self.iteration = 0\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.class_labels = np.unique(y)\n",
    "        self.w = np.zeros((X.shape[1], 1)) \n",
    "        if len(self.class_labels)>2:\n",
    "            raise Exception(\"too many classes. This logistic regression class only implements binary classification.\")\n",
    "\n",
    "        objective_values = [self.objective(X,y)]\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "\n",
    "            gradient = self.perform_update(X, y)\n",
    "            objective = self.objective(X,y)\n",
    "            objective_values.append(objective)\n",
    "\n",
    "            # if np.abs(objective_values[-1]-objective_values[-2]) < self.tol:\n",
    "            if np.max(np.abs(gradient)) < self.tol:\n",
    "            # if  np.linalg.norm(gradient)< self.tol:\n",
    "                print(f'Method: {self.method} Number of iterations: {i}')\n",
    "                # print(objective_values)\n",
    "                print(f'Objective function value: {objective_values[-1]}')\n",
    "                break\n",
    "        else:\n",
    "            print(f'Maximum number of iterations reached, objective function value {objective_values[-1]}')\n",
    "        self.training_length = i\n",
    "        \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return sigmoid(np.dot(X, self.w))\n",
    "\n",
    "    def predict_proba_w(self, X, w):\n",
    "        return sigmoid(np.dot(X, w))\n",
    "\n",
    "    def perform_update(self, X, y):\n",
    "        pi = self.predict_proba(X)\n",
    "\n",
    "\n",
    "\n",
    "        if self.method == 'backtracking':\n",
    "            #perform gradient descent update\n",
    "\n",
    "            t=1\n",
    "            alpha = 0.1\n",
    "            beta = 0.3\n",
    "\n",
    "            gradient = self.gradient(X,y, pi)\n",
    "            i = 0\n",
    "            while True: \n",
    "                # equation from the lecture https://github.com/HealthML/Math4ML-Lecture/blob/master/math4ml_2_Calculus_05_Unconstrained_Optimization_Convexity_handout.pdf\n",
    "                left_side = self.objective_w(X,y,self.w-t*gradient)\n",
    "                right_side = self.objective_w(X,y,self.w) - alpha*t*np.dot(gradient.T, gradient)\n",
    "                # if (left_side < right_side) or t<0.001:\n",
    "                if (left_side < right_side) or t<0.001:\n",
    "                    if t<0.01:\n",
    "                        print('Small t reached')\n",
    "                    break\n",
    "                t = t*beta\n",
    "                i +=1\n",
    "\n",
    "            # print(f'{left_side} {t} {i}')\n",
    "            update = - gradient * t\n",
    "\n",
    "\n",
    "        if self.method == 'gd':\n",
    "            #perform gradient descent update\n",
    "            gradient = self.gradient(X,y, pi)\n",
    "            update = - gradient * self.lr\n",
    "        \n",
    "        if self.method=='hessian':\n",
    "\n",
    "            gradient = self.gradient(X,y, pi)\n",
    "            hessian = self.hessian(X,y, pi)\n",
    "            hessian_inv = np.linalg.inv(hessian)\n",
    "            update = - np.dot(hessian_inv, gradient) * self.lr\n",
    "\n",
    "        if self.method=='diagonal_hessian':\n",
    "\n",
    "            eps = np.finfo(X.dtype).eps\n",
    "            gradient = self.gradient(X,y, pi)\n",
    "            hessian_diag = self.hessian_diag(X,y, pi)\n",
    "            hessian_inv = np.diag(1/(hessian_diag + eps))\n",
    "            update = - np.dot(hessian_inv, gradient) * self.lr\n",
    "\n",
    "        if self.method=='efficient_diagonal_hessian':\n",
    "\n",
    "            eps = np.finfo(X.dtype).eps\n",
    "            gradient = self.gradient(X,y, pi)\n",
    "            hessian_diag = self.hessian_diag(X,y, pi)\n",
    "            update = - 1/(hessian_diag[:,None]+eps) *gradient * self.lr\n",
    "\n",
    "        if self.method=='bfgs':\n",
    "\n",
    "            gradient = self.gradient(X,y, pi)\n",
    "            if self.iteration==0:\n",
    "                # self.hessian_inv = np.eye(X.shape[1])\n",
    "                self.hessian_inv = np.linalg.inv(self.hessian(X,y,pi))\n",
    "                self.gradient_previous = gradient  # previous gradient\n",
    "                self.w_previous = self.w  # previous w\n",
    "            if self.iteration > 1:\n",
    "                y_grad = gradient - self.gradient_previous\n",
    "                x = self.w - self.w_previous\n",
    "                denominator = (y_grad.T@x)\n",
    "\n",
    "                self.hessian_inv = (np.eye(X.shape[1]) - (x@y_grad.T)/denominator) @ self.hessian_inv @  (np.eye(X.shape[1]) - (y_grad@x.T)/denominator) + (x@x.T)/denominator\n",
    "            # self.hessian_inv = self.hessian_inv - (self.hessian_inv@y@y.T@self.hessian_inv)/(y.T@self.hessian_inv@y) + (x@x.T)/(y.T@x)\n",
    "\n",
    "            update = - self.hessian_inv @ gradient \n",
    "\n",
    "            t=1\n",
    "            alpha = 0.1\n",
    "            beta = 0.5\n",
    "\n",
    "            i = 0\n",
    "            fx = self.objective_w(X,y,self.w)\n",
    "            while True: \n",
    "                # equation from the lecture https://github.com/HealthML/Math4ML-Lecture/blob/master/math4ml_2_Calculus_05_Unconstrained_Optimization_Convexity_handout.pdf\n",
    "                left_side = self.objective_w(X,y,self.w+t*update)\n",
    "                right_side = fx + alpha*t*np.dot(gradient.T, update)\n",
    "                # if (left_side < right_side) or t<0.001:\n",
    "                if (left_side < right_side) or t<0.01:\n",
    "                    # if t<0.01:\n",
    "                    #     print('Small t reached')\n",
    "                    break\n",
    "                t = t*beta\n",
    "                i +=1\n",
    "\n",
    "            update = - self.hessian_inv @ gradient * t \n",
    "\n",
    "            self.gradient_previous = gradient\n",
    "            self.w_previous = self.w\n",
    "\n",
    "        self.w = self.w +  update\n",
    "        self.iteration += 1\n",
    "        return gradient\n",
    "\n",
    "\n",
    "    \n",
    "    def objective(self, X, y):\n",
    "        pi = self.predict_proba(X)\n",
    "\n",
    "        eps = np.finfo(pi.dtype).eps\n",
    "        pi = np.clip(pi, eps, 1-eps) # to avoid (log(0))\n",
    "\n",
    "        log_0_pi = np.log(pi[y==self.class_labels[1]])\n",
    "        log_1_pi = np.log(1.0-pi[y==self.class_labels[0]])\n",
    "        loss = -log_0_pi.mean() - log_1_pi.mean() # this version is more stable for perfect prediction\n",
    "\n",
    "        regularizer = 0.5 * (self.l2 * self.w * self.w).sum()\n",
    "\n",
    "        return loss + regularizer\n",
    "        \n",
    "    def objective_w(self, X, y,w):\n",
    "        pi = self.predict_proba_w(X,w)\n",
    "\n",
    "        eps = np.finfo(pi.dtype).eps\n",
    "        pi = np.clip(pi, eps, 1-eps) # to avoid (log(0))\n",
    "\n",
    "        log_0_pi = np.log(pi[y==self.class_labels[1]])\n",
    "        log_1_pi = np.log(1.0-pi[y==self.class_labels[0]])\n",
    "        loss = -log_0_pi.mean() - log_1_pi.mean() # this version is more stable for perfect prediction\n",
    "\n",
    "        regularizer = 0.5 * (self.l2 * w * w).sum()\n",
    "\n",
    "        return loss + regularizer\n",
    "    \n",
    "    def gradient(self, X, y, pi):\n",
    "        gradient = np.dot(X.T, pi - (y==self.class_labels[1])[:,None] )/X.shape[0] +  self.l2 * self.w\n",
    "        return gradient\n",
    "\n",
    "    def hessian(self, X, y, pi):\n",
    "        hessian = (X * (pi * (1.0-pi))).T.dot(X)/X.shape[0] + self.l2 * np.eye(X.shape[1])\n",
    "        return hessian \n",
    "\n",
    "    def hessian_diag(self, X, y, pi):\n",
    "        hessian_diag = np.sum((pi*(1-pi))*X**2, axis=0)/X.shape[0] + self.l2\n",
    "        return hessian_diag\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "repeat = 10\n",
    "n_samples = 1000\n",
    "n_features = 400\n",
    "n_informative = 400\n",
    "tol = 0.01\n",
    "\n",
    "t_start = time.time()\n",
    "np.random.seed(10)\n",
    "iterations = []\n",
    "for i in range(repeat):\n",
    "    X, y = sklearn.datasets.make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, class_sep=1, n_clusters_per_class=2)\n",
    "    log = LogisticRegression(l2=0.1, lr=1, num_iter=1000, method='bfgs', tol=tol  )\n",
    "    log.fit(X,y)\n",
    "    iterations.append(log.training_length)\n",
    "t_end = time.time()\n",
    "print(f'Average time: {(t_end- t_start)/repeat}, average iterations {np.mean(iterations)}')\n",
    "print('\\n')\n",
    "\n",
    "# t_start = time.time()\n",
    "# np.random.seed(10)\n",
    "# iterations = []\n",
    "# for i in range(repeat):\n",
    "#     X, y = sklearn.datasets.make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, class_sep=1, n_clusters_per_class=2)\n",
    "#     log = LogisticRegression(l2=0.5, lr=1, num_iter=1000, method='backtracking', tol=tol  )\n",
    "#     log.fit(X,y)\n",
    "#     iterations.append(log.training_length)\n",
    "# t_end = time.time()\n",
    "# print(f'Average time: {(t_end- t_start)/repeat}, average iterations {np.mean(iterations)}')\n",
    "\n",
    "print('\\n')\n",
    "t_start = time.time()\n",
    "np.random.seed(10)\n",
    "iterations = []\n",
    "for i in range(repeat):\n",
    "    X, y = sklearn.datasets.make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, class_sep=1, n_clusters_per_class=3)\n",
    "    log = LogisticRegression(l2=0.1, lr=1, num_iter=1000, method='hessian', tol=tol  )\n",
    "    log.fit(X,y)\n",
    "    iterations.append(log.training_length)\n",
    "t_end = time.time()\n",
    "print(f'Average time: {(t_end- t_start)/repeat}, average iterations {np.mean(iterations)}')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "t_start = time.time()\n",
    "np.random.seed(10)\n",
    "iterations = []\n",
    "for i in range(repeat):\n",
    "    X, y = sklearn.datasets.make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, class_sep=2, n_clusters_per_class=2)\n",
    "    log = LogisticRegression(l2=0.1, lr=0.2, num_iter=10000, method='gd', tol=tol)\n",
    "    log.fit(X,y)\n",
    "    iterations.append(log.training_length)\n",
    "t_end = time.time()\n",
    "print(f'Average time: {(t_end- t_start)/repeat}, average iterations {np.mean(iterations)}')\n",
    "\n",
    "# print('\\n')\n",
    "# t_start = time.time()\n",
    "# np.random.seed(10)\n",
    "# iterations = []\n",
    "# for i in range(repeat):\n",
    "#     X, y = sklearn.datasets.make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, class_sep=2, n_clusters_per_class=2)\n",
    "#     log = LogisticRegression(l2=0.1, lr=0.25 ,num_iter=1001, method='diagonal_hessian', tol=tol)\n",
    "#     log.fit(X,y)\n",
    "#     iterations.append(log.training_length)\n",
    "# t_end = time.time()\n",
    "# print(f'Average time: {(t_end- t_start)/repeat}, average iterations {np.mean(iterations)}')\n",
    "\n",
    "# print('\\n')\n",
    "# t_start = time.time()\n",
    "# np.random.seed(10)\n",
    "# iterations = []\n",
    "# for i in range(repeat):\n",
    "#     X, y = sklearn.datasets.make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, class_sep=2, n_clusters_per_class=2)\n",
    "#     log = LogisticRegression(l2=0.1, lr=0.25 ,num_iter=1000, method='efficient_diagonal_hessian', tol=tol)\n",
    "#     log.fit(X,y)\n",
    "#     iterations.append(log.training_length)\n",
    "# t_end = time.time()\n",
    "# print(f'Average time: {(t_end- t_start)/repeat}, average iterations {np.mean(iterations)}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
