{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 - BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sklearn.datasets\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the BFGS algorithm for unconstrained optimization. The algorithm is implemented in the `bfgs` function, which takes the following arguments.  \n",
    "\n",
    "The BFGS algorithm approximates the Hessian matrix without needing to compute the second derivative of the function.  \n",
    "Additionally, it avoids matrix inversion, which is computationally expensive.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(a):\n",
    "    \"\"\"\n",
    "    returns the logistic sigmoid \\pi(a)\n",
    "    Keyword arguments:\n",
    "    a -- scalar or numpy array\n",
    "    \"\"\"\n",
    "    expa = np.exp(a)\n",
    "    res = expa / (1.0 + expa)\n",
    "    if hasattr(a, \"__iter__\"):\n",
    "        res[a>709.7] = 1.0 # np.exp will overflow and return inf for values larger 709.7.\n",
    "    elif a>709.7:\n",
    "        res = 1.0\n",
    "    return res\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, l2=0.01, num_iter = 100, method='gd', lr=0.001, tol=0.001) -> None:\n",
    "        self.w = None\n",
    "        self.num_iter = num_iter\n",
    "        self.method = method\n",
    "        self.lr = lr\n",
    "        self.tol = tol\n",
    "        self.l2 = l2\n",
    "        self.iteration = 0\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.class_labels = np.unique(y)\n",
    "        self.w = np.zeros((X.shape[1], 1)) \n",
    "        if len(self.class_labels)>2:\n",
    "            raise Exception(\"too many classes. This logistic regression class only implements binary classification.\")\n",
    "\n",
    "        objective_values = [self.objective(X,y)]\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "\n",
    "            gradient = self.perform_update(X, y)\n",
    "            objective = self.objective(X,y)\n",
    "            objective_values.append(objective)\n",
    "\n",
    "            # if np.abs(objective_values[-1]-objective_values[-2]) < self.tol:\n",
    "            if np.max(np.abs(gradient)) < self.tol:\n",
    "            # if  np.linalg.norm(gradient)< self.tol:\n",
    "                print(f'Method: {self.method} Number of iterations: {i}')\n",
    "                # print(objective_values)\n",
    "                print(f'Objective function value: {objective_values[-1]}')\n",
    "                break\n",
    "        else:\n",
    "            print(f'Maximum number of iterations reached, objective function value {objective_values[-1]}')\n",
    "        self.training_length = i\n",
    "        \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return sigmoid(np.dot(X, self.w))\n",
    "\n",
    "    def predict_proba_w(self, X, w):\n",
    "        return sigmoid(np.dot(X, w))\n",
    "\n",
    "    def perform_update(self, X, y):\n",
    "        pi = self.predict_proba(X)\n",
    "\n",
    "\n",
    "\n",
    "        if self.method == 'backtracking':\n",
    "            #perform gradient descent update\n",
    "\n",
    "            t=1\n",
    "            alpha = 0.1\n",
    "            beta = 0.3\n",
    "\n",
    "            gradient = self.gradient(X,y, pi)\n",
    "            i = 0\n",
    "            while True: \n",
    "                # equation from the lecture https://github.com/HealthML/Math4ML-Lecture/blob/master/math4ml_2_Calculus_05_Unconstrained_Optimization_Convexity_handout.pdf\n",
    "                left_side = self.objective_w(X,y,self.w-t*gradient)\n",
    "                right_side = self.objective_w(X,y,self.w) - alpha*t*np.dot(gradient.T, gradient)\n",
    "                # if (left_side < right_side) or t<0.001:\n",
    "                if (left_side < right_side) or t<0.001:\n",
    "                    if t<0.01:\n",
    "                        print('Small t reached')\n",
    "                    break\n",
    "                t = t*beta\n",
    "                i +=1\n",
    "\n",
    "            # print(f'{left_side} {t} {i}')\n",
    "            update = - gradient * t\n",
    "\n",
    "\n",
    "        if self.method == 'gd':\n",
    "            #perform gradient descent update\n",
    "            gradient = self.gradient(X,y, pi)\n",
    "            update = - gradient * self.lr\n",
    "        \n",
    "        if self.method=='hessian':\n",
    "\n",
    "            gradient = self.gradient(X,y, pi)\n",
    "            hessian = self.hessian(X,y, pi)\n",
    "            hessian_inv = np.linalg.inv(hessian)\n",
    "            update = - np.dot(hessian_inv, gradient) * self.lr\n",
    "\n",
    "        if self.method=='diagonal_hessian':\n",
    "\n",
    "            eps = np.finfo(X.dtype).eps\n",
    "            gradient = self.gradient(X,y, pi)\n",
    "            hessian_diag = self.hessian_diag(X,y, pi)\n",
    "            hessian_inv = np.diag(1/(hessian_diag + eps))\n",
    "            update = - np.dot(hessian_inv, gradient) * self.lr\n",
    "\n",
    "        if self.method=='efficient_diagonal_hessian':\n",
    "\n",
    "            eps = np.finfo(X.dtype).eps\n",
    "            gradient = self.gradient(X,y, pi)\n",
    "            hessian_diag = self.hessian_diag(X,y, pi)\n",
    "            update = - 1/(hessian_diag[:,None]+eps) *gradient * self.lr\n",
    "\n",
    "        if self.method=='bfgs':\n",
    "\n",
    "            gradient = self.gradient(X,y, pi)\n",
    "            if self.iteration==0:\n",
    "                # self.hessian_inv = np.eye(X.shape[1])\n",
    "                self.hessian_inv = np.linalg.inv(self.hessian(X,y,pi))\n",
    "                self.gradient_previous = gradient  # previous gradient\n",
    "                self.w_previous = self.w  # previous w\n",
    "            if self.iteration > 1:\n",
    "                y_grad = gradient - self.gradient_previous\n",
    "                x = self.w - self.w_previous\n",
    "                denominator = (y_grad.T@x)\n",
    "\n",
    "                self.hessian_inv = (np.eye(X.shape[1]) - (x@y_grad.T)/denominator) @ self.hessian_inv @  (np.eye(X.shape[1]) - (y_grad@x.T)/denominator) + (x@x.T)/denominator\n",
    "\n",
    "            update = - self.hessian_inv @ gradient \n",
    "\n",
    "            t=1\n",
    "            alpha = 0.1\n",
    "            beta = 0.5\n",
    "\n",
    "            i = 0\n",
    "            fx = self.objective_w(X,y,self.w)\n",
    "            while True: \n",
    "                # equation from the lecture https://github.com/HealthML/Math4ML-Lecture/blob/master/math4ml_2_Calculus_05_Unconstrained_Optimization_Convexity_handout.pdf\n",
    "                left_side = self.objective_w(X,y,self.w+t*update)\n",
    "                right_side = fx + alpha*t*np.dot(gradient.T, update)\n",
    "                if (left_side < right_side) or t<0.01:\n",
    "                    # if t<0.01:\n",
    "                    #     print('Small t reached')\n",
    "                    break\n",
    "                t = t*beta\n",
    "                i +=1\n",
    "\n",
    "            update = - self.hessian_inv @ gradient * t \n",
    "\n",
    "            self.gradient_previous = gradient\n",
    "            self.w_previous = self.w\n",
    "\n",
    "        self.w = self.w +  update\n",
    "        self.iteration += 1\n",
    "        return gradient\n",
    "\n",
    "\n",
    "    \n",
    "    def objective(self, X, y):\n",
    "        pi = self.predict_proba(X)\n",
    "\n",
    "        eps = np.finfo(pi.dtype).eps\n",
    "        pi = np.clip(pi, eps, 1-eps) # to avoid (log(0))\n",
    "\n",
    "        log_0_pi = np.log(pi[y==self.class_labels[1]])\n",
    "        log_1_pi = np.log(1.0-pi[y==self.class_labels[0]])\n",
    "        loss = -log_0_pi.mean() - log_1_pi.mean() # this version is more stable for perfect prediction\n",
    "\n",
    "        regularizer = 0.5 * (self.l2 * self.w * self.w).sum()\n",
    "\n",
    "        return loss + regularizer\n",
    "        \n",
    "    def objective_w(self, X, y,w):\n",
    "        pi = self.predict_proba_w(X,w)\n",
    "\n",
    "        eps = np.finfo(pi.dtype).eps\n",
    "        pi = np.clip(pi, eps, 1-eps) # to avoid (log(0))\n",
    "\n",
    "        log_0_pi = np.log(pi[y==self.class_labels[1]])\n",
    "        log_1_pi = np.log(1.0-pi[y==self.class_labels[0]])\n",
    "        loss = -log_0_pi.mean() - log_1_pi.mean() # this version is more stable for perfect prediction\n",
    "\n",
    "        regularizer = 0.5 * (self.l2 * w * w).sum()\n",
    "\n",
    "        return loss + regularizer\n",
    "    \n",
    "    def gradient(self, X, y, pi):\n",
    "        gradient = np.dot(X.T, pi - (y==self.class_labels[1])[:,None] )/X.shape[0] +  self.l2 * self.w\n",
    "        return gradient\n",
    "\n",
    "    def hessian(self, X, y, pi):\n",
    "        hessian = (X * (pi * (1.0-pi))).T.dot(X)/X.shape[0] + self.l2 * np.eye(X.shape[1])\n",
    "        return hessian \n",
    "\n",
    "    def hessian_diag(self, X, y, pi):\n",
    "        hessian_diag = np.sum((pi*(1-pi))*X**2, axis=0)/X.shape[0] + self.l2\n",
    "        return hessian_diag\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "repeat = 10\n",
    "n_samples = 1000\n",
    "n_features = 400\n",
    "n_informative = 400\n",
    "tol = 0.01\n",
    "\n",
    "t_start = time.time()\n",
    "np.random.seed(10)\n",
    "iterations = []\n",
    "for i in range(repeat):\n",
    "    X, y = sklearn.datasets.make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, class_sep=1, n_clusters_per_class=2)\n",
    "    log = LogisticRegression(l2=0.1, lr=1, num_iter=1000, method='bfgs', tol=tol  )\n",
    "    log.fit(X,y)\n",
    "    iterations.append(log.training_length)\n",
    "t_end = time.time()\n",
    "print(f'Average time: {(t_end- t_start)/repeat}, average iterations {np.mean(iterations)}')\n",
    "print('\\n')\n",
    "\n",
    "# t_start = time.time()\n",
    "# np.random.seed(10)\n",
    "# iterations = []\n",
    "# for i in range(repeat):\n",
    "#     X, y = sklearn.datasets.make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, class_sep=1, n_clusters_per_class=2)\n",
    "#     log = LogisticRegression(l2=0.5, lr=1, num_iter=1000, method='backtracking', tol=tol  )\n",
    "#     log.fit(X,y)\n",
    "#     iterations.append(log.training_length)\n",
    "# t_end = time.time()\n",
    "# print(f'Average time: {(t_end- t_start)/repeat}, average iterations {np.mean(iterations)}')\n",
    "\n",
    "print('\\n')\n",
    "t_start = time.time()\n",
    "np.random.seed(10)\n",
    "iterations = []\n",
    "for i in range(repeat):\n",
    "    X, y = sklearn.datasets.make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, class_sep=1, n_clusters_per_class=3)\n",
    "    log = LogisticRegression(l2=0.1, lr=1, num_iter=1000, method='hessian', tol=tol  )\n",
    "    log.fit(X,y)\n",
    "    iterations.append(log.training_length)\n",
    "t_end = time.time()\n",
    "print(f'Average time: {(t_end- t_start)/repeat}, average iterations {np.mean(iterations)}')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "t_start = time.time()\n",
    "np.random.seed(10)\n",
    "iterations = []\n",
    "for i in range(repeat):\n",
    "    X, y = sklearn.datasets.make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, class_sep=2, n_clusters_per_class=2)\n",
    "    log = LogisticRegression(l2=0.1, lr=0.2, num_iter=10000, method='gd', tol=tol)\n",
    "    log.fit(X,y)\n",
    "    iterations.append(log.training_length)\n",
    "t_end = time.time()\n",
    "print(f'Average time: {(t_end- t_start)/repeat}, average iterations {np.mean(iterations)}')\n",
    "\n",
    "# print('\\n')\n",
    "# t_start = time.time()\n",
    "# np.random.seed(10)\n",
    "# iterations = []\n",
    "# for i in range(repeat):\n",
    "#     X, y = sklearn.datasets.make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, class_sep=2, n_clusters_per_class=2)\n",
    "#     log = LogisticRegression(l2=0.1, lr=0.25 ,num_iter=1001, method='diagonal_hessian', tol=tol)\n",
    "#     log.fit(X,y)\n",
    "#     iterations.append(log.training_length)\n",
    "# t_end = time.time()\n",
    "# print(f'Average time: {(t_end- t_start)/repeat}, average iterations {np.mean(iterations)}')\n",
    "\n",
    "# print('\\n')\n",
    "# t_start = time.time()\n",
    "# np.random.seed(10)\n",
    "# iterations = []\n",
    "# for i in range(repeat):\n",
    "#     X, y = sklearn.datasets.make_classification(n_samples=n_samples, n_classes=2, n_features=n_features, n_informative=n_informative, n_redundant=0, class_sep=2, n_clusters_per_class=2)\n",
    "#     log = LogisticRegression(l2=0.1, lr=0.25 ,num_iter=1000, method='efficient_diagonal_hessian', tol=tol)\n",
    "#     log.fit(X,y)\n",
    "#     iterations.append(log.training_length)\n",
    "# t_end = time.time()\n",
    "# print(f'Average time: {(t_end- t_start)/repeat}, average iterations {np.mean(iterations)}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
