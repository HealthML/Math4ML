## Vector Space of Multivariate Polynomials

More generally, also multivariate polynomials form a vector space:

:::{prf:theorem} Multivariate polynomials form a vector space
:label: thm-multivariate-polynomial-vector-space
:nonumber:

Let $\mathbf{x} = [x_1, x_2, \dots, x_d] \in \mathbb{R}^d$ be a Euclidean vector.  
The set $P_n^d$ of all real-valued multivariate polynomials in $\mathbf{x}$ of total degree at most $n$ is a vector space.

That is,

$$
P_n^d = \left\{ p(\mathbf{x}) = \sum_{|\alpha| \leq n} a_\alpha \mathbf{x}^\alpha \;\middle|\; a_\alpha \in \mathbb{R},\ \alpha \in \mathbb{N}_0^d \right\},
$$

where $\alpha = [\alpha_1, \dots, \alpha_d]$ is a multi-index,  
$|\alpha| = \alpha_1 + \dots + \alpha_d$, and  
$\mathbf{x}^\alpha = x_1^{\alpha_1} \cdots x_d^{\alpha_d}$.
:::

We verify that $P_n^d$ satisfies all vector space axioms in the Appendix.

## Polynomial Features in Machine Learning

Using polynomial vector spaces, we can enhance simple machine learning algorithms by explicitly representing complex, nonlinear relationships.

### Example: Polynomial Features in Linear Regression and Nearest Centroid Classifier

Consider a simple ML taskâ€”fitting or classifying data that's clearly nonlinear:

- **Original Data**: $\mathbf{x} \in \mathbb{R}$, one-dimensional feature.
- **Polynomial Feature Map**: Transform the input into a polynomial vector space, e.g.:
  
$$\phi(x) = [1, x, x^2, \dots, x^n]^\top.$$

#### Linear Regression with Polynomial Features:
Instead of fitting a line $y = w_0 + w_1 x$, we fit:

$$y = w_0 + w_1 x + w_2 x^2 + \dots + w_n x^n = \mathbf{w}^\top \phi(x)$$

This can model more complex curves while still being linear in the parameters $\mathbf{w}$.

#### Nearest Centroid Classifier with Polynomial Features:
Instead of measuring Euclidean distance in the original feature space, we measure it in the polynomial feature space:

- Centroids become averages of polynomial features:

$$\mathbf{c}_k = \frac{1}{N_k}\sum_{i:y_i=k} \phi(x_i).$$

- Classification uses distances in this polynomial space:

$$\hat{y} = \arg\min_k \|\phi(x)-\mathbf{c}_k\|.$$

This simple feature mapping enables the classifier to separate nonlinear boundaries (circles, ellipses, curves) easily and effectively.


### Insights for Students:

- Recognizing polynomials as vector spaces gives a clear mathematical justification for methods using polynomial features.
- Polynomial vector spaces allow linear methods to handle nonlinear patterns by embedding data into higher-dimensional spaces.
- Even simple classifiers or regressors gain expressive power through such explicit polynomial feature expansions.
