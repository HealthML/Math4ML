
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Representer Theorem for Linear Functions &#8212; Mathematics for Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter_decompositions/representer_theorem';</script>
    <link rel="canonical" href="https://healthml.github.io/Math4ML/chapter_decompositions/representer_theorem.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/hpi-logo-colored.svg" class="logo__image only-light" alt="Mathematics for Machine Learning - Home"/>
    <script>document.write(`<img src="../_static/hpi-logo-colored.svg" class="logo__image only-dark" alt="Mathematics for Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Preface
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Mathematics for Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_ml_basics/intro.html">Machine Learning Problems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_ml_basics/classification.html">Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_ml_basics/regression.html">Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_ml_basics/clustering.html">Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_ml_basics/representation_learning.html">Representation Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_spaces/overview_spaces.html">Vector and Function Spaces</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/vector_spaces.html">Vector Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/polynomial_vector_space.html">Polynomial Vector Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/basis_functions_vector_space.html">Basis Functions Vector Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/subspaces.html">Subspaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/metric_spaces.html">Metric Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/normed_spaces.html">Normed Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/inner_product_spaces.html">Inner Product Spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_spaces/transposition.html">Transposition</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_calculus/overview_calculus.html">Calculus and Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/extrema.html">Extrema</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/gradients.html">Gradients</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/gradient_descent_ridge.html">Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/matrix_calculus.html">Matrix Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/jacobian.html">Jacobian</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/chain_rule.html">Chain Rule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/mean_value_theorem.html">Mean Value Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/minima_first_order_condition.html">First Order Condition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/analytical_solution_ridge.html">Quadratic Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/line_search.html">Line Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/hessian.html">Hessian</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_calculus/taylors_theorem.html">Taylor’s Theorem</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="overview_decompositions.html">Matrix Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="matrix_rank.html">Rank of a Matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="determinant.html">Determinant</a></li>
<li class="toctree-l2"><a class="reference internal" href="row_equivalence.html">Gaussian Elimination and the PLU Decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="square_matrices.html">Fundamental Equivalences for Square matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="trace.html">Trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="eigenvectors.html">Eigenvalues and Eigenvectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="orthogonal_matrices.html">Orthogonal matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="symmetric_matrices.html">Symmetric matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="Rayleigh_quotients.html">Rayleigh Quotients</a></li>
<li class="toctree-l2"><a class="reference internal" href="matrix_norms.html">Matrix Norms</a></li>
<li class="toctree-l2"><a class="reference internal" href="psd_matrices.html">Positive (semi-)definite matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="pca.html">Principal Components Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="svd.html">Singular value decomposition</a></li>
<li class="toctree-l2"><a class="reference internal" href="pseudoinverse.html">Moore-Penrose Pseudoinverse</a></li>
<li class="toctree-l2"><a class="reference internal" href="orthogonal_projections.html">Orthogonal projections</a></li>
<li class="toctree-l2"><a class="reference internal" href="big_picture.html">Fundamental Subspaces</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_convexity/overview_convexity.html">Convexity</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convexity/convex_sets.html">Convex sets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convexity/convex_functions.html">Basics of convex functions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../chapter_second_order/overview_second_order.html">Second-Order Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_second_order/minima_second_order_condition.html">Second Order Condition for Minima</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendix/proofs.html">Detailed Proofs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../appendix/Cauchy%E2%80%93Schwarz_inequality.html">Cauchy-Schwarz Inequality</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/Bolzano-Weierstrass_theorem.html">Bolzano-Weierstrass Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/extreme_value_theorem.html">Extreme Value Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/Rolles_theorem.html">Rolle's Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/mean_value_theorem_proof.html">Mean Value Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/scalar-scalar_chain_rule.html">Chain Rule for Scalar-Scalar Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/squeeze_theorem.html">Squeeze Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/first_fundamental_theorem_calculus.html">First Fundamental Theorem of Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/second_fundamental_theorem_calculus.html">Second Fundamental Theorem of Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendix/Clairauts_theorem.html">Clairaut's Theorem</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/differentiation_rules.html">Differentiation Rules</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/HealthML/Math4ML" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/HealthML/Math4ML/edit/main/book/chapter_decompositions/representer_theorem.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/HealthML/Math4ML/issues/new?title=Issue%20on%20page%20%2Fchapter_decompositions/representer_theorem.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter_decompositions/representer_theorem.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Representer Theorem for Linear Functions</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representer-theorem-for-linear-classifiers-and-linear-regression">Representer Theorem for Linear Classifiers and Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">Explanation:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#application-in-kernel-methods">Application in Kernel Methods</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="representer-theorem-for-linear-functions">
<h1>Representer Theorem for Linear Functions<a class="headerlink" href="#representer-theorem-for-linear-functions" title="Link to this heading">#</a></h1>
<p>In many machine learning problems, especially those involving <span class="math notranslate nohighlight">\(\ell_2\)</span>-regularized risk minimization over linear functions, the <strong>representer theorem</strong> guarantees that the solution to the optimization problem can be written as a finite linear combination of the training samples.</p>
<p>Consider the following regularized formulation:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{argmin}_{\mathbf{w}, b} \; \sum_{i=1}^n L(y_i, \mathbf{w}^\top \mathbf{x}_i + b) + \lambda\, \|\mathbf{w}\|^2,
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\)</span> are the training examples with <span class="math notranslate nohighlight">\(\mathbf{x}_i \in \mathbb{R}^d\)</span> and corresponding labels <span class="math notranslate nohighlight">\(y_i\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(L\)</span> is a loss function (such as the logistic loss for logistic regression, or the squared loss for linear regression),</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span> is a regularization parameter,</p></li>
<li><p><span class="math notranslate nohighlight">\(\|\mathbf{w}\|^2\)</span> represents the squared Euclidean norm, which penalizes the complexity of the classifier.</p></li>
</ul>
<p>The representer theorem asserts that there exists a solution <span class="math notranslate nohighlight">\((\mathbf{w}^*, b^*)\)</span> where the optimal weight vector <span class="math notranslate nohighlight">\(\mathbf{w}^*\)</span> lies in the span of the training data. That is, one can express</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^* = \sum_{i=1}^n \alpha_i \mathbf{x}_i,
\]</div>
<p>for some coefficients <span class="math notranslate nohighlight">\(\alpha_1, \alpha_2, \dots, \alpha_n \in \mathbb{R}\)</span>. Consequently, the decision function for a new example <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span> becomes</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}^*) = \mathbf{w}^{*\top} \mathbf{x} + b^* 
= \left(\sum_{i=1}^n \alpha_i \mathbf{x}_i^\top\right) \mathbf{x} + b^*
= \sum_{i=1}^n \alpha_i\, \langle \mathbf{x}_i, \mathbf{x}^* \rangle + b^*.
\]</div>
<p>This representation shows that the decision function is a linear combination of the inner products between the training examples and the new input <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>, plus a bias term <span class="math notranslate nohighlight">\(b^*\)</span>.
This is particularly useful in kernelized settings, where the inner products can be computed using a kernel function <span class="math notranslate nohighlight">\(k(\mathbf{x}_i, \mathbf{x}^*)\)</span> that implicitly maps the data into a higher-dimensional feature space without explicitly computing the mapping.</p>
<p>This representation has profound implications:</p>
<ul class="simple">
<li><p><strong>Finite Representation:</strong> Even if the underlying hypothesis space (e.g., an RKHS in the kernelized setting) is infinite-dimensional, regularization forces the solution to lie in the finite-dimensional span of the training examples.</p></li>
<li><p><strong>Computational Efficiency:</strong> The optimization problem is effectively reduced to finding the finite set of coefficients <span class="math notranslate nohighlight">\(\alpha_i\)</span> (and <span class="math notranslate nohighlight">\(b\)</span>), which can be computed efficiently using kernel methods or convex optimization techniques.</p></li>
<li><p><strong>Interpretability:</strong> In linear classifiers, this representation reveals that the learned decision boundary is entirely determined by the training data. In the case of support vector machines, for instance, only a subset of the training points (the support vectors) will have nonzero coefficients <span class="math notranslate nohighlight">\(\alpha_i\)</span>, directly indicating which examples are critical for classification.</p></li>
</ul>
<p>Thus, for linear classifiers with decision boundaries of the form <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x} + b\)</span>, the representer theorem not only ensures that the problem has a solution in the span of the data but also provides practical and theoretical benefits by reducing the complexity of the learning task.</p>
<p>Similarly, for linear regression with regularization, the representer theorem guarantees that the optimal solution is in the span of the training inputs and can be written entirely in terms of inner products between these inputs, providing both theoretical insight and practical advantages in computation and model interpretation.</p>
<hr class="docutils" />
<section id="representer-theorem-for-linear-classifiers-and-linear-regression">
<h2>Representer Theorem for Linear Classifiers and Linear Regression<a class="headerlink" href="#representer-theorem-for-linear-classifiers-and-linear-regression" title="Link to this heading">#</a></h2>
<div class="proof theorem admonition" id="thm-representer">
<p class="admonition-title"><span>Theorem </span> (Representer Theorem)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\)</span> be a set of training examples in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(L\)</span> be a loss function. Consider the following optimization problem:</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{w}, b} \; \sum_{i=1}^n L(y_i, \mathbf{w}^\top \mathbf{x}_i + b) + \lambda\, \|\mathbf{w}\|^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span> is a regularization parameter. Then, there exists an optimal solution <span class="math notranslate nohighlight">\((\mathbf{w}^*, b^*)\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^* = \sum_{i=1}^n \alpha_i \mathbf{x}_i,
\]</div>
<p>for some coefficients <span class="math notranslate nohighlight">\(\alpha_i \in \mathbb{R}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> are the training examples.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. Assume that we are solving a regularized risk minimization problem over <span class="math notranslate nohighlight">\(\mathbf{w} \in \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(b \in \mathbb{R}\)</span> of the form</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{w}, b} \; \sum_{i=1}^n L\bigl(y_i, \mathbf{w}^\top \mathbf{x}_i + b\bigr) + \lambda\, \|\mathbf{w}\|^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span> is a loss function, and <span class="math notranslate nohighlight">\(\lambda&gt;0\)</span> is the regularization parameter.
Let</p>
<div class="math notranslate nohighlight">
\[
V = \operatorname{span}\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\}
\quad\text{and}\quad
V^\perp = \{\mathbf{u}\in\mathbb{R}^d : \mathbf{u}^\top \mathbf{v}=0\ \forall\,\mathbf{v}\in V\}.
\]</div>
<p>be the subspace spanned by the training data and its orthogonal complement, respectively.</p>
<p>The loss term <span class="math notranslate nohighlight">\(L(y_i, \mathbf{w}^\top \mathbf{x}_i + b)\)</span> depends only on the inner product <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x}_i\)</span> and the bias <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>The regularization term <span class="math notranslate nohighlight">\(\|\mathbf{w}\|^2\)</span> penalizes the overall norm of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>.</p>
<p>The key observation is that the loss term does not depend on the component of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> that lies in <span class="math notranslate nohighlight">\(V^\perp\)</span>.</p>
<p>Thus, we can decompose <span class="math notranslate nohighlight">\(\mathbf{w}\in \mathbb{R}^d\)</span> into two orthogonal components:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w} = \mathbf{w}_0 + \mathbf{w}_1,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w}_0 \in V\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_1 \in V^\perp\)</span> (the orthogonal complement of <span class="math notranslate nohighlight">\(V\)</span>).</p>
<p>Notice that by construction, for every training example <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> (which lies in <span class="math notranslate nohighlight">\(V\)</span>), we have</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}_1^\top\mathbf{x}_i = 0.
\]</div>
<p>Thus, the prediction for each training example is</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^\top \mathbf{x}_i + b = (\mathbf{w}_0 + \mathbf{w}_1)^\top \mathbf{x}_i + b = \mathbf{w}_0^\top \mathbf{x}_i + \mathbf{w}_1^\top \mathbf{x}_i + b = \mathbf{w}_0^\top \mathbf{x}_i + b.
\]</div>
<p>This shows that the component <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> (lying in <span class="math notranslate nohighlight">\(V^\perp\)</span>) does not affect the predictions on the training data.</p>
<p>Now, consider the regularization term <span class="math notranslate nohighlight">\(\|\mathbf{w}\|^2\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{w}\|^2 = \mathbf{w}^\top\mathbf{w} = (\mathbf{w}_0 + \mathbf{w}_1)^\top(\mathbf{w}_0 + \mathbf{w}_1) = \mathbf{w}_0^\top\mathbf{w}_0 + \mathbf{w}_1^\top\mathbf{w}_1 + 2\mathbf{w}_0^\top\mathbf{w}_1
\]</div>
<p>Because <span class="math notranslate nohighlight">\(\mathbf{w}_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> are orthogonal, <span class="math notranslate nohighlight">\(\mathbf{w}_0^\top\mathbf{w}_1=0\)</span> and we have</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{w}\|^2 = \|\mathbf{w}_0\|^2 + \|\mathbf{w}_1\|^2.
\]</div>
<p>If <span class="math notranslate nohighlight">\(\mathbf{w}_1 \neq \mathbf{0}\)</span>, then <span class="math notranslate nohighlight">\(\|\mathbf{w}\|^2 &gt; \|\mathbf{w}_0\|^2\)</span> but the predictions remain the same.
Since the objective includes the regularization term <span class="math notranslate nohighlight">\(\lambda\,\|\mathbf{w}\|^2\)</span>, any solution can be improved by setting <span class="math notranslate nohighlight">\(\mathbf{w}_1 = \mathbf{0}\)</span>.
In other words, there is no benefit to having a component in <span class="math notranslate nohighlight">\(V^\perp\)</span>.</p>
<p>Thus, we can always find an optimal weight vector <span class="math notranslate nohighlight">\(\mathbf{w}^*\)</span> that lies entirely in <span class="math notranslate nohighlight">\(V\)</span>; that is,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^* = \mathbf{w}_0 \quad \text{with} \quad \mathbf{w}^* \in V.
\]</div>
<p>Because <span class="math notranslate nohighlight">\(V\)</span> is the span of <span class="math notranslate nohighlight">\(\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\}\)</span>, there exist scalars <span class="math notranslate nohighlight">\(\alpha_1, \alpha_2, \dots, \alpha_n\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^* = \sum_{i=1}^n \alpha_i \mathbf{x}_i.
\]</div>
<p>This completes the proof that the optimal solution can be expressed as a linear combination of the training data.</p>
</div>
<hr class="docutils" />
<section id="explanation">
<h3>Explanation:<a class="headerlink" href="#explanation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Key Observation:</strong> The loss term depends only on <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x}_i\)</span>. Since any component of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> perpendicular to all training examples (i.e. in <span class="math notranslate nohighlight">\(V^\perp\)</span>) does not affect the loss, including it only increases the regularization penalty.</p></li>
<li><p><strong>Conclusion:</strong> We can always set the <span class="math notranslate nohighlight">\(V^\perp\)</span> component to zero without changing the predictions on the training data, thereby arriving at an optimal solution that lies in the span of the training data. Consequently, the optimal weight vector can be written as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^* = \sum_{i=1}^n \alpha_i \mathbf{x}_i.
\]</div>
<p>This representer theorem is central in kernel methods and many linear models because it reduces an infinite-dimensional search (if one considers a feature space mapping) to a finite-dimensional problem based solely on the training samples.</p>
</section>
</section>
<section id="application-in-kernel-methods">
<h2>Application in Kernel Methods<a class="headerlink" href="#application-in-kernel-methods" title="Link to this heading">#</a></h2>
<p>The Representer Theorem is the foundation for <strong>kernel methods</strong>, since it shows that—even when our hypothesis space is a high‑ or infinite‑dimensional Reproducing Kernel Hilbert Space (RKHS)—the solution can be expressed in terms of the finite training set.</p>
<ol class="arabic">
<li><p><strong>Feature maps and kernels</strong><br />
We begin by choosing a feature mapping</p>
<div class="math notranslate nohighlight">
\[
     \phi: \mathcal{X}\to\mathcal{H},
   \]</div>
<p>into an (often infinite‑dimensional) inner‐product space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>.  A kernel function</p>
<div class="math notranslate nohighlight">
\[
     k(\mathbf{x},\mathbf{x}') 
     = \langle \phi(\mathbf{x}),\,\phi(\mathbf{x}')\rangle_{\mathcal{H}}
   \]</div>
<p>allows us to compute inner products in <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> without ever constructing <span class="math notranslate nohighlight">\(\phi(\mathbf{x})\)</span> explicitly.</p>
</li>
<li><p><strong>Kernelized predictive form</strong><br />
By the Representer Theorem, the optimal weight vector in <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> satisfies</p>
<div class="math notranslate nohighlight">
\[
     \mathbf{w}^* \;=\;\sum_{i=1}^n \alpha_i\,\phi(\mathbf{x}_i).
   \]</div>
<p>Hence the learned decision (or regression) function takes the form</p>
<div class="math notranslate nohighlight">
\[
     f(\mathbf{x})
     = \mathbf{w}^{*\top}\phi(\mathbf{x}) + b
     = \sum_{i=1}^n \alpha_i\,k(\mathbf{x}_i,\mathbf{x}) \;+\; b.
   \]</div>
<p>All dependence on the (possibly infinite) feature space is captured through the <span class="math notranslate nohighlight">\(n\times n\)</span> <strong>kernel Gram matrix</strong>$<span class="math notranslate nohighlight">\(K\)</span><span class="math notranslate nohighlight">\( with entries \)</span>K_{ij}=k(\mathbf{x}_i,\mathbf{x}_j)$.</p>
</li>
<li><p><strong>Training via kernels</strong></p>
<ul>
<li><p><strong>Kernel ridge regression</strong><br />
Minimizes<br />
<span class="math notranslate nohighlight">\(\tfrac1n\|K\boldsymbol\alpha + b\mathbf{1} - \mathbf{y}\|^2 + \lambda\,\boldsymbol\alpha^\top K\,\boldsymbol\alpha\)</span>.<br />
One can solve for <span class="math notranslate nohighlight">\(\boldsymbol\alpha\)</span> in closed form:</p>
<div class="math notranslate nohighlight">
\[
       [K + \lambda n\,I]\,\boldsymbol\alpha = \mathbf{y} - b\,\mathbf{1}.
     \]</div>
</li>
<li><p><strong>Support Vector Machines</strong><br />
Solves the convex dual<br />
<span class="math notranslate nohighlight">\(\max_{\boldsymbol\alpha}\;\sum_i\alpha_i - \tfrac12\sum_{ij}\alpha_i\alpha_j y_i y_j\,K_{ij}\)</span><br />
subject to <span class="math notranslate nohighlight">\(\sum_i\alpha_i y_i=0\)</span>, <span class="math notranslate nohighlight">\(0\le\alpha_i\le C\)</span>.</p></li>
<li><p><strong>Kernel Logistic Regression, Gaussian Processes, Kernel PCA, …</strong><br />
All follow the same pattern: inference and training reduce to operations on the Gram matrix.</p></li>
</ul>
</li>
<li><p><strong>Practical consequences</strong></p>
<ul class="simple">
<li><p><strong>Computational complexity:</strong><br />
Training requires <span class="math notranslate nohighlight">\(\mathcal{O}(n^3)\)</span> in general (e.g.\ inverting <span class="math notranslate nohighlight">\(K\)</span>), and each new prediction costs <span class="math notranslate nohighlight">\(\mathcal{O}(n)\)</span>.</p></li>
<li><p><strong>Expressivity:</strong><br />
Kernels let us fit very flexible decision boundaries (polynomial, radial basis, string kernels, etc.) without ever leaving our linear‐model framework.</p></li>
<li><p><strong>Regularization:</strong><br />
The <span class="math notranslate nohighlight">\(\lambda\|\mathbf{w}\|^2_{\mathcal{H}}\)</span> penalty translates into a smoothness control on the function <span class="math notranslate nohighlight">\(f\)</span>, balancing data fit vs.\ model complexity.</p></li>
</ul>
</li>
</ol>
<hr class="docutils" />
<p>By leveraging the Representer Theorem, kernel methods transform a potentially intractable infinite‑dimensional problem into a finite one over the training set, enabling powerful, nonparametric learning with rigorous regularization.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter_decompositions"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representer-theorem-for-linear-classifiers-and-linear-regression">Representer Theorem for Linear Classifiers and Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">Explanation:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#application-in-kernel-methods">Application in Kernel Methods</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christoph Lippert
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <script src="https://giscus.app/client.js"
        data-repo="HealthML/Math4ML"
        data-repo-id="R_kgDON-O79w"
        data-category="Comments"
        data-category-id="DIC_kwDON-O7984Co2qc"
        data-mapping="pathname"
        data-strict="1"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>