# Exercise Sheet 4 Solutions

### 1.
 
$$
A = \begin{pmatrix} 3 & 2 \\ -2 & -1 \end{pmatrix}
$$

**Characteristic Polynomial:**

We compute the characteristic polynomial:

$$
p(\lambda) = \det(A - \lambda I) 
= \det \begin{pmatrix} 3 - \lambda & 2 \\ -2 & -1 - \lambda \end{pmatrix}
$$

$$
= (3 - \lambda)(-1 - \lambda) - (-2)(2)
= \lambda^2 - 2\lambda + 1 = (\lambda - 1)^2
$$

**Eigenvalues:**

Solving the characteristic polynomial:

$$
(\lambda - 1)^2 = 0 \Rightarrow \lambda_1 = \lambda_2 = 1
$$

**Eigenvectors:**

We solve:

$$
(A - \lambda I)\vec{x} = 0 \Rightarrow
\begin{pmatrix} 2 & 2 \\ -2 & -2 \end{pmatrix} \vec{x} = \vec{0}
$$

This reduces to:

$$
x_1 + x_2 = 0 \Rightarrow \vec{x} = \alpha \begin{pmatrix} 1 \\ -1 \end{pmatrix}
$$

So, the eigenvector is any scalar multiple of:

$$
\vec{x} = \begin{pmatrix} 1 \\ -1 \end{pmatrix}
$$


**Now, we solve them for B:**

$$
B = \begin{pmatrix}
0 & 1 & 0 \\
1 & 0 & 1 \\
1 & 1 & 0
\end{pmatrix}
$$

**Characteristic Polynomial:**

We compute the characteristic polynomial:

$$
p(\lambda) = \det(B - \lambda I) = 
\det \begin{pmatrix}
-\lambda & 1 & 0 \\
1 & -\lambda & 1 \\
1 & 1 & -\lambda
\end{pmatrix}
$$

Expanding along the first row:

```math
= -\lambda \cdot \det \begin{pmatrix}
-\lambda & 1 \\
1 & -\lambda
\end{pmatrix}
- 1 \cdot \det \begin{pmatrix}
1 & 1 \\
1 & -\lambda
\end{pmatrix}
+ 0
```

$$
= -\lambda(\lambda^2 - 1) - (-\lambda - 1)
= -\lambda^3 + \lambda + \lambda + 1
= -\lambda^3 + 2\lambda + 1
$$

So the characteristic polynomial is:

$$
p(\lambda) = -\lambda^3 + 2\lambda + 1
$$

**Eigenvalues:**

Solving the characteristic polynomial:

$$
-\lambda^3 + 2\lambda + 1 = 0
$$

```math
-\left( \lambda + 1 \right)\left( \lambda - \frac{1 + \sqrt{5}}{2} \right)\left( \lambda - \frac{1 - \sqrt{5}}{2} \right) = 0
```

so:

$$
\lambda_1 \approx -0.62, \quad
\lambda_2 = -1, \quad
\lambda_3 \approx 1.62
$$

**Eigenvectors:**

To find the eigenvectors, we solve:

$$
(B - \lambda I)\vec{x} = 0
$$

**For** $$ \lambda_1 \approx -0.62 $$:

$$
\vec{v}_1 = \begin{pmatrix}
1 \\
-0.62 \\
-0.62
\end{pmatrix}
$$

**For** $$ \lambda_2 = -1 $$:

$$
\vec{v}_2 = \begin{pmatrix}
-1 \\
1 \\
0
\end{pmatrix}
$$

**For** $$ \lambda_3 \approx 1.62 $$:

$$
\vec{v}_3 = \begin{pmatrix}
1 \\
1.62 \\
1.62
\end{pmatrix}
$$

$$\blacksquare$$

### 2.
We start by expressing the trace of $$ ABC $$:

```math
\mathrm{tr}(ABC) = \sum_{i=1}^{n} (ABC)_{ii}
```

**Computing $$ (ABC)_{ii} $$:**

We want to compute the entry in the $$ i $$-th row and $$ i $$-th column of the matrix product $$ ABC $$.

1. First, compute the product $$ AB $$, which gives:

```math
(AB)_{il} = \sum_{j=1}^{m} A_{ij} B_{jl}
```

2. Then compute $$ ABC = (AB)C $$. The $$ (i, i) $$-th element of $$ ABC $$ is:

```math
(ABC)_{ii} = \sum_{l=1}^{k} (AB)_{il} C_{li}
```

3. Substitute the expression for $$ (AB)_{il} $$:

```math
(ABC)_{ii} = \sum_{l=1}^{k} \left( \sum_{j=1}^{m} A_{ij} B_{jl} \right) C_{li}
```

4. Rearranging the summation order:

```math
(ABC)_{ii} = \sum_{j=1}^{m} \sum_{l=1}^{k} A_{ij} B_{jl} C_{li}
```

So, the trace becomes:

```math
\mathrm{tr}(ABC) = \sum_{i=1}^{n} \sum_{j=1}^{m} \sum_{k=1}^{k} A_{ij} B_{jk} C_{ki}
```


**Computing the trace of $$ BCA $$:**

```math
\mathrm{tr}(BCA) = \sum_{j=1}^{m} (BCA)_{jj}
```

Expand the product:

1. First compute $$ BC $$:

```math
(BC)_{ji} = \sum_{k=1}^{k} B_{jk} C_{ki}
```

2. Then compute:

```math
(BCA)_{jj} = \sum_{i=1}^{n} (BC)_{ji} A_{ij} = \sum_{i=1}^{n} \sum_{k=1}^{k} B_{jk} C_{ki} A_{ij}
```

3. So the trace is:

```math
\mathrm{tr}(BCA) = \sum_{j=1}^{m} \sum_{k=1}^{k} \sum_{i=1}^{n} B_{jk} C_{ki} A_{ij}
```


**Final Step:**

Since scalar multiplication is commutative:

```math
\sum_{i=1}^{n} \sum_{j=1}^{m} \sum_{k=1}^{k} A_{ij} B_{jk} C_{ki}
= \sum_{j=1}^{m} \sum_{k=1}^{k} \sum_{i=1}^{n} B_{jk} C_{ki} A_{ij}
```

Therefore:

```math
\mathrm{tr}(ABC) = \mathrm{tr}(BCA)
```

$$\blacksquare$$

### 3.

**Eigenvalues are the same**

Let $$ p(\lambda) $$ be the characteristic polynomial of $$ A $$. By definition:

```math
p(\lambda) = \det(A - \lambda I)
```

Now consider the transpose:

```math
\det(A^\top - \lambda I)
= \det((A - \lambda I)^\top)
= \det(A - \lambda I)
= p(\lambda)
```

So both $$ A $$ and $$ A^\top $$ have the **same characteristic polynomial**, which means they have the **same eigenvalues**, including their algebraic multiplicities.


**Eigenvectors may differ**
Although $$ A $$ and $$ A^\top $$ have the same eigenvalues, their eigenvectors **need not** be the same.

To see this, consider a concrete example:

Let

```math
A = \begin{pmatrix}
1 & 1 \\
0 & 1
\end{pmatrix}
```

Then:

```math
A^\top = \begin{pmatrix}
1 & 0 \\
1 & 1
\end{pmatrix}
```

The characteristic polynomial of both is:

```math
\det(A - \lambda I) = (1 - \lambda)^2
```

So they both have a **repeated eigenvalue** $$ \lambda = 1 $$.

Now compute eigenvectors:

- For $$ A $$, we solve:

```math
(A - I)\vec{x} = \begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix} \vec{x} = 0
\Rightarrow x_2 = 0 \Rightarrow \vec{x} = \begin{pmatrix}
1 \\
0
\end{pmatrix}
```

- For $$ A^\top $$, we solve:

```math
(A^\top - I)\vec{x} = \begin{pmatrix}
0 & 0 \\
1 & 0
\end{pmatrix} \vec{x} = 0
\Rightarrow x_1 = 0 \Rightarrow \vec{x} = \begin{pmatrix}
0 \\
1
\end{pmatrix}
```

Thus, the **eigenvectors are different**, even though the eigenvalues are the same.

**So:**

- $$ A $$ and $$ A^\top $$ always have the **same eigenvalues**, including multiplicities.
- However, they may have **different eigenvectors**, especially when the matrix is **not symmetric**.

$$\blacksquare$$

### 4.
#### (a) The rank of $$ B $$

We observe that each row (and column) of $$ B $$ is a linear combination of the others:

- The second row is:  
  ```math
  \text{Row}_2 = 2 \cdot \text{Row}_1
  ```
- The third row is:  
  ```math
  \text{Row}_3 = 3 \cdot \text{Row}_1
  ```

So all rows lie in the span of the first row.

Letâ€™s do row reduction to confirm:

```math
\begin{pmatrix}
1 & 2 & 3 \\
2 & 4 & 6 \\
3 & 6 & 9
\end{pmatrix}
\rightarrow
\begin{pmatrix}
1 & 2 & 3 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{pmatrix}
```

Only one non-zero row remains after Gaussian elimination.

Therefore, the rank of $$ B $$ is:

```math
\mathrm{rank}(B) = 1
```

#### (b) Are the columns of $$ B $$ linearly independent?

Recall that the number of linearly independent columns is equal to the rank of the matrix.

Since:

```math
\mathrm{rank}(B) = 1 < 3
```

This means that the columns are **linearly dependent**.

In fact:

```math
\text{Col}_2 = 2 \cdot \text{Col}_1 \\
\text{Col}_3 = 3 \cdot \text{Col}_1
```
$$\blacksquare$$
